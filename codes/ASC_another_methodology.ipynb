{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlZmsA_ww0a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b9bf67e-08c0-4448-bb85-6a9bc4bf656b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE36g9DJ2-ZI"
      },
      "source": [
        "# Download glove file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGdfHs61wlbj"
      },
      "outputs": [],
      "source": [
        "# !pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIbVa91rw2Bx"
      },
      "outputs": [],
      "source": [
        "# import gdown\n",
        "\n",
        "# file_id = '1-4E3dyjIhg_UxUWkrTlGLd5AfOKxeMuN'  # Replace with your file ID\n",
        "# output_path = '/content/drive/MyDrive/ASC/glove.txt'\n",
        "\n",
        "# url = f'https://drive.google.com/uc?id={file_id}'\n",
        "# gdown.download(url, output_path, quiet=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t75JIPl8vouV"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psFCFMNQvouc",
        "outputId": "3ef029f3-e586-4d97-fe76-8d1424c87e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVzVpUDqvouh",
        "outputId": "65741bad-faae-45d3-bb43-7b0ce09beaa9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMiSPtmL5K1k"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import csv\n",
        "import math\n",
        "import torch\n",
        "import keras\n",
        "import os\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from keras.models import Sequential , Model\n",
        "from keras.layers import LSTM, Bidirectional, Dense, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from scipy.special import softmax\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHjFRdoXvouj"
      },
      "source": [
        "# Read dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "0xI6SXzDP4Bz",
        "outputId": "f29f0b89-0c88-4fc2-b02d-7491ef560bdb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6c80c72-ec73-4464-be9e-de20b70e862d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>processed_review</th>\n",
              "      <th>aspect</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The instructor did not really do a good enough...</td>\n",
              "      <td>teacher really good enough job setting stage e...</td>\n",
              "      <td>the teacher</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Thank you. Instructor Mis Summer and Coursera</td>\n",
              "      <td>thank teacher mi summer coursera</td>\n",
              "      <td>the teacher</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When instructor was speaking or showing exampl...</td>\n",
              "      <td>teacher speaking showing example screen always...</td>\n",
              "      <td>the teacher</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>With the instructors accent, I had difficulty ...</td>\n",
              "      <td>teacher accent difficulty understanding everyt...</td>\n",
              "      <td>the teacher</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>This is a very good course for complete beginn...</td>\n",
              "      <td>good course complete beginner provides basic k...</td>\n",
              "      <td>the teacher</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6c80c72-ec73-4464-be9e-de20b70e862d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6c80c72-ec73-4464-be9e-de20b70e862d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6c80c72-ec73-4464-be9e-de20b70e862d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bbc61131-cae7-428e-a6be-b7f0ea712ee4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bbc61131-cae7-428e-a6be-b7f0ea712ee4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bbc61131-cae7-428e-a6be-b7f0ea712ee4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                              review  \\\n",
              "0  The instructor did not really do a good enough...   \n",
              "1      Thank you. Instructor Mis Summer and Coursera   \n",
              "2  When instructor was speaking or showing exampl...   \n",
              "3  With the instructors accent, I had difficulty ...   \n",
              "4  This is a very good course for complete beginn...   \n",
              "\n",
              "                                    processed_review       aspect  label  \n",
              "0  teacher really good enough job setting stage e...  the teacher      0  \n",
              "1                   thank teacher mi summer coursera  the teacher      0  \n",
              "2  teacher speaking showing example screen always...  the teacher      0  \n",
              "3  teacher accent difficulty understanding everyt...  the teacher      0  \n",
              "4  good course complete beginner provides basic k...  the teacher      0  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "csv_file_path = \"/content/drive/MyDrive/ASC/reviews ADS Courses.csv\"\n",
        "df_data = pd.read_csv(csv_file_path)\n",
        "df_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jljXifsvouk"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JrsUVkPvoul",
        "outputId": "1d9fbd09-4c8f-4ef4-bb57-417afa8d32ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 742 samples\n",
            "Value 1: 742 samples\n",
            "Value 2: 742 samples\n"
          ]
        }
      ],
      "source": [
        "temp = df_data.loc[df_data['aspect'] == 'the teacher', 'label']\n",
        "unique_values, counts = np.unique(temp, return_counts=True)\n",
        "# Print unique values and their respective counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Value {value}: {count} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dmHgEJUvoum",
        "outputId": "975a24af-78f8-47da-d887-7c613dd4147a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 15437 samples\n",
            "Value 1: 15829 samples\n",
            "Value 2: 15829 samples\n"
          ]
        }
      ],
      "source": [
        "temp = df_data.loc[df_data['aspect'] == 'the course', 'label']\n",
        "unique_values, counts = np.unique(temp, return_counts=True)\n",
        "\n",
        "# Print unique values and their respective counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Value {value}: {count} samples')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDXsC9dRvoun"
      },
      "source": [
        "# number of unique words in dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kclgY5PIVuH",
        "outputId": "3243de9f-9d50-4e8d-e0d7-d627e4e71955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in the dataset: 686161\n"
          ]
        }
      ],
      "source": [
        "df_data['processed_review'] = df_data['processed_review'].fillna('')\n",
        "\n",
        "# Convert all entries to strings\n",
        "df_data['processed_review'] = df_data['processed_review'].astype(str)\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df_data['processed_review'].tolist())\n",
        "\n",
        "# Calculate the total number of tokens (words)\n",
        "total_words = sum(len(sentence.split()) for sentence in df_data['processed_review'].tolist())\n",
        "\n",
        "print(\"Total number of words in the dataset:\", total_words)\n",
        "vocab_size = total_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI2PA-32voup"
      },
      "source": [
        "# load glove embeddings vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXDhDOCp6XVz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def load_glove_vectors(file_path):\n",
        "#     word_vectors = {}\n",
        "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
        "#         # Get the total number of lines in the file for tqdm\n",
        "#         total_lines = sum(1 for line in file)\n",
        "#         file.seek(0)  # Reset file pointer to the beginning\n",
        "\n",
        "#         # Initialize tqdm with the total number of lines\n",
        "#         with tqdm(total=total_lines, desc=\"Loading GloVe Vectors\", unit=\"line\") as pbar:\n",
        "#             for line in file:\n",
        "#                 values = line.split()\n",
        "#                 word = values[0]\n",
        "#                 vector = np.asarray(values[1:], dtype='float32')\n",
        "#                 word_vectors[word] = vector\n",
        "#                 pbar.update(1)  # Update the progress bar\n",
        "\n",
        "#     return word_vectors\n",
        "\n",
        "# glove_file = '/content/drive/MyDrive/ASC/glove.txt'\n",
        "# glove_vectors = load_glove_vectors(glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHha340k6XV0"
      },
      "outputs": [],
      "source": [
        "# len(glove_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZNF2CLOvour"
      },
      "source": [
        "# calculate the longest review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZWiX1s4vous"
      },
      "outputs": [],
      "source": [
        "sentences = df_data['processed_review'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTOmD9w16XV2",
        "outputId": "a8050291-9a98-43e8-acfb-006689d5868a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "466"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "max_len = max(len(word_tokenize(sent.lower())) for sent in sentences)\n",
        "max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TUWAcSGvouu"
      },
      "source": [
        "# using glove for embeddings for words in all reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpe35Wzg6XV3"
      },
      "outputs": [],
      "source": [
        "# sentences_embedding = []\n",
        "\n",
        "# for sen in sentences:\n",
        "#     words_sentence_embedding = []\n",
        "#     words = word_tokenize(sen.lower())\n",
        "#     for word in words:\n",
        "#         embedding_vector = glove_vectors.get(word)\n",
        "#         if embedding_vector is not None and len(embedding_vector) > 0:\n",
        "#             words_sentence_embedding.append(embedding_vector)\n",
        "#     sentences_embedding.append(words_sentence_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYXgD_Nj6XV4"
      },
      "outputs": [],
      "source": [
        "# len(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knYVJrRe6XV4"
      },
      "outputs": [],
      "source": [
        "# sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLCeqnay6XV5"
      },
      "outputs": [],
      "source": [
        "# words = word_tokenize(sentences[0].lower())\n",
        "# len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS7LwOws6XV6"
      },
      "outputs": [],
      "source": [
        "# (len(sentences_embedding[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtrfoonHvouz"
      },
      "source": [
        "# save embedding glove in pikle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Z4Ki2sC6XWI"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/ASC/glove_embedding.pkl', 'wb') as f:\n",
        "#     pickle.dump(sentences_embedding, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtUFbp4Evouz"
      },
      "source": [
        "# read embedding glove from pikle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkHf_iKd6XWI"
      },
      "outputs": [],
      "source": [
        "# objects_glove_position = []\n",
        "# with (open(\"/content/drive/MyDrive/ASC/glove_embedding.pkl\", \"rb\")) as openfile:\n",
        "#     while True:\n",
        "#         try:\n",
        "#             objects_glove_position.append(pickle.load(openfile))\n",
        "#         except EOFError:\n",
        "#             break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5G9aI356XWJ"
      },
      "outputs": [],
      "source": [
        "# len(objects_glove_position)\n",
        "# for i in objects_glove_position :\n",
        "#     print(len(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSqqlrND6XWK"
      },
      "outputs": [],
      "source": [
        "# len(objects_glove_position[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBuhPUZJvou1"
      },
      "source": [
        "# relative position attention embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BB7DbXRFfpj"
      },
      "outputs": [],
      "source": [
        "# class RelativeGlobalAttention(nn.Module):\n",
        "#     def __init__(self, d_model, num_heads, max_len=1024, dropout=0.1):\n",
        "#         super().__init__()\n",
        "#         d_head, remainder = divmod(d_model, num_heads)\n",
        "#         if remainder:\n",
        "#             raise ValueError(\n",
        "#                 \"incompatible `d_model` and `num_heads`\"\n",
        "#             )\n",
        "#         self.max_len = max_len\n",
        "#         self.d_model = d_model\n",
        "#         self.num_heads = num_heads\n",
        "#         self.key = nn.Linear(d_model, d_model)\n",
        "#         self.value = nn.Linear(d_model, d_model)\n",
        "#         self.query = nn.Linear(d_model, d_model)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.Er = nn.Parameter(torch.randn(max_len, d_head))\n",
        "#         self.register_buffer(\n",
        "#             \"mask\",\n",
        "#             torch.tril(torch.ones(max_len, max_len))\n",
        "#             .unsqueeze(0).unsqueeze(0)\n",
        "#         )\n",
        "#         # self.mask.shape = (1, 1, max_len, max_len)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # x.shape == (batch_size, seq_len, d_model)\n",
        "#         batch_size, seq_len, _ = x.shape\n",
        "\n",
        "#         if seq_len > self.max_len:\n",
        "#             raise ValueError(\n",
        "#                 \"sequence length exceeds model capacity\"\n",
        "#             )\n",
        "\n",
        "#         k_t = self.key(x).reshape(batch_size, seq_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
        "#         # k_t.shape = (batch_size, num_heads, d_head, seq_len)\n",
        "#         v = self.value(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "#         q = self.query(x).reshape(batch_size, seq_len, self.num_heads, -1).transpose(1, 2)\n",
        "#         # shape = (batch_size, num_heads, seq_len, d_head)\n",
        "\n",
        "#         start = self.max_len - seq_len\n",
        "#         Er_t = self.Er[start:, :].transpose(0, 1)\n",
        "#         # Er_t.shape = (d_head, seq_len)\n",
        "#         QEr = torch.matmul(q, Er_t)\n",
        "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
        "#         Srel = self.skew(QEr)\n",
        "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
        "\n",
        "#         QK_t = torch.matmul(q, k_t)\n",
        "#         # QK_t.shape = (batch_size, num_heads, seq_len, seq_len)\n",
        "#         attn = (QK_t + Srel) / math.sqrt(q.size(-1))\n",
        "#         mask = self.mask[:, :, :seq_len, :seq_len]\n",
        "#         # mask.shape = (1, 1, seq_len, seq_len)\n",
        "#         attn = attn.masked_fill(mask == 0, float(\"-inf\"))\n",
        "#         # attn.shape = (batch_size, num_heads, seq_len, seq_len)\n",
        "#         attn = F.softmax(attn, dim=-1)\n",
        "#         out = torch.matmul(attn, v)\n",
        "#         # out.shape = (batch_size, num_heads, seq_len, d_head)\n",
        "#         out = out.transpose(1, 2)\n",
        "#         # out.shape == (batch_size, seq_len, num_heads, d_head)\n",
        "#         out = out.reshape(batch_size, seq_len, -1)\n",
        "#         # out.shape == (batch_size, seq_len, d_model)\n",
        "#         return self.dropout(out)\n",
        "\n",
        "\n",
        "#     def skew(self, QEr):\n",
        "#         # QEr.shape = (batch_size, num_heads, seq_len, seq_len)\n",
        "#         padded = F.pad(QEr, (1, 0))\n",
        "#         # padded.shape = (batch_size, num_heads, seq_len, 1 + seq_len)\n",
        "#         batch_size, num_heads, num_rows, num_cols = padded.shape\n",
        "#         reshaped = padded.reshape(batch_size, num_heads, num_cols, num_rows)\n",
        "#         # reshaped.size = (batch_size, num_heads, 1 + seq_len, seq_len)\n",
        "#         Srel = reshaped[:, :, 1:, :]\n",
        "#         # Srel.shape = (batch_size, num_heads, seq_len, seq_len)\n",
        "#         return Srel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py1CcNFlvou4"
      },
      "source": [
        "# test for relative position attention embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4asGdPD6XWQ"
      },
      "outputs": [],
      "source": [
        "# sentence = \"i like to eat pizza\"\n",
        "\n",
        "# # Tokenize the sentence (you may need to use a tokenizer based on your use case)\n",
        "# tokens = sentence.split()\n",
        "\n",
        "# # Suppose you have word embeddings for each word in the sentence\n",
        "# # For example, create a random tensor as embeddings (for demonstration purposes)\n",
        "# embedding_size = 300  # Same as the d_model used in RelativeGlobalAttention\n",
        "# word_embeddings = torch.randn(len(tokens), embedding_size)\n",
        "\n",
        "# # Create an instance of the RelativeGlobalAttention class\n",
        "# num_heads = 12\n",
        "# attention_layer = RelativeGlobalAttention(embedding_size, num_heads)\n",
        "\n",
        "# # List to store relative positional embeddings for each word\n",
        "# relative_position_embeddings = []\n",
        "\n",
        "# # Iterate through each word's embedding and apply the attention mechanism\n",
        "# for i in range(len(tokens)):\n",
        "#     # Get the embedding for the current word\n",
        "#     word_embedding = word_embeddings[i].unsqueeze(0).unsqueeze(0)  # Reshape to (1, 1, d_model)\n",
        "\n",
        "#     # Apply the attention mechanism to get relative positional embeddings\n",
        "#     attention_output = attention_layer(word_embedding)\n",
        "\n",
        "#     # Append the relative positional embedding for the current word to the list\n",
        "#     relative_position_embeddings.append(attention_output.squeeze(0).squeeze(0))\n",
        "\n",
        "# # Convert the list of relative positional embeddings to a torch tensor\n",
        "# relative_position_embeddings = torch.stack(relative_position_embeddings)\n",
        "\n",
        "# # The relative_position_embeddings tensor now contains the embeddings of relative positions\n",
        "# print(relative_position_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtAByLudvou6"
      },
      "source": [
        "# apply relative position embedding on all the words of the all reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYgqPXfE6XWS"
      },
      "outputs": [],
      "source": [
        "# num_heads = 12\n",
        "# embedding_size = 300\n",
        "\n",
        "# attention_layer = RelativeGlobalAttention(embedding_size, num_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtiACFsi6XWT"
      },
      "outputs": [],
      "source": [
        "# sentences_relative_positional_embedding=[]\n",
        "# for embedding in objects_glove_position :\n",
        "#     for sen_emb in embedding:\n",
        "#         words_relative_positional_sentence_embedding=[]\n",
        "#         for word_emb in sen_emb :\n",
        "# #             print(len(word_emb))\n",
        "#             embedding_vector = torch.tensor(word_emb).unsqueeze(0).unsqueeze(0)\n",
        "# #             print(embedding_vector)\n",
        "#             attention_output = attention_layer(embedding_vector)\n",
        "#             words_relative_positional_sentence_embedding.append(attention_output.squeeze(0).squeeze(0).detach().numpy())\n",
        "#         sentences_relative_positional_embedding.append(words_relative_positional_sentence_embedding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHarbhxu6XWU"
      },
      "outputs": [],
      "source": [
        "# type(sentences_relative_positional_embedding[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBhfzAmz6XWV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# (sentences_relative_positional_embedding[0][0]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIkVgpjBvovZ"
      },
      "source": [
        "# save embedding in pikle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0PJsE6R6XWV"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/ASC/relative_position_embedding.pkl', 'wb') as f:\n",
        "#     pickle.dump(sentences_relative_positional_embedding, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnQCASj0vova"
      },
      "source": [
        "# read embedding from pikle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3fAns0y6XWW"
      },
      "outputs": [],
      "source": [
        "# objects_realtive_position = []\n",
        "# with (open(\"/content/drive/MyDrive/ASC/relative_position_embedding.pkl\", \"rb\")) as openfile:\n",
        "#     while True:\n",
        "#         try:\n",
        "#             objects_realtive_position.append(pickle.load(openfile))\n",
        "#         except EOFError:\n",
        "#             break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INVxdgIw6XWX"
      },
      "outputs": [],
      "source": [
        "# for relative in objects_realtive_position :\n",
        "#     print(len(relative))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YffqR3nsvove"
      },
      "source": [
        "# concatenate glove embedding and relative embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjyfP-HJ6XWX"
      },
      "outputs": [],
      "source": [
        "# for item1, item2 in zip(objects_glove_position, objects_realtive_position):\n",
        "#     sentences_new_embedding=[]\n",
        "#     for sen1,sen2 in zip(item1, item2):\n",
        "#         sen_new_embedding=[]\n",
        "#         if len(sen1)==len(sen2):\n",
        "#               for word1,word2 in zip(sen1,sen2):\n",
        "#                 combined_vector = np.concatenate((word1, word2))\n",
        "#                 sen_new_embedding.append(combined_vector)\n",
        "#         if len(sen1)>len(sen2):\n",
        "#             for i in range(len(sen1)-len(sen2)):\n",
        "#                 sen2.append(np.zeros(300))\n",
        "#             for word1,word2 in zip(sen1,sen2):\n",
        "#                 combined_vector = np.concatenate((word1, word2))\n",
        "#                 sen_new_embedding.append(combined_vector)\n",
        "\n",
        "#         if len(sen2)>len(sen1):\n",
        "#             for i in range(len(sen2)-len(sen1)):\n",
        "#                 sen1.append(np.zeros(300))\n",
        "#             for word1,word2 in zip(sen1,sen2):\n",
        "#                 combined_vector = np.concatenate((word1, word2))\n",
        "#                 sen_new_embedding.append(combined_vector)\n",
        "\n",
        "#         sentences_new_embedding.append(sen_new_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqcnVOViMVQQ"
      },
      "outputs": [],
      "source": [
        "# objects_glove_position = 0\n",
        "# objects_realtive_position = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slQLIOp3W2r7"
      },
      "outputs": [],
      "source": [
        "# np.array(sentences_new_embedding).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWiVAbvLFfpk"
      },
      "outputs": [],
      "source": [
        "# np.array(sentences_new_embedding[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE92K7XWFfpl"
      },
      "outputs": [],
      "source": [
        "# np.array(sentences_new_embedding[0]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC4Edr9SqQkH"
      },
      "outputs": [],
      "source": [
        "# embedings_all_words = []\n",
        "# number_of_words = 0\n",
        "# for sent in (sentences_new_embedding):\n",
        "#     number_of_words = number_of_words + len(sent)\n",
        "#     for word in sent :\n",
        "#          embedings_all_words.append(word)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toSsDQfFvovj"
      },
      "outputs": [],
      "source": [
        "# np.array(embedings_all_words).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6swaT9VprF89",
        "outputId": "e6bbe6d7-210c-4fec-a5c2-01e01d32827f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(672316, 600)"
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# embedings_all_words = np.array(embedings_all_words).reshape(672316, 600)\n",
        "# embedings_all_words.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAhiHqxhq2qA"
      },
      "outputs": [],
      "source": [
        "# number_of_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-OGWJQvovm"
      },
      "source": [
        "# bi-lstm for the reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmxpBOTYFfpl",
        "outputId": "cd7adaa5-628c-4cb5-8088-8b406ef4b83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 466, 600)          403389600 \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 466, 600)          2162400   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 466, 1)            601       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 405552601 (1.51 GB)\n",
            "Trainable params: 405552601 (1.51 GB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# # Define your vocabulary size and embedding dimensions\n",
        "# vocab_size = number_of_words  # Define your vocabulary size\n",
        "# embedding_dim = 600  # Define your embedding dimensions\n",
        "\n",
        "# # from keras.models import Model\n",
        "\n",
        "# # Create the Bi-LSTM model\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(input_dim=vocab_size, weights=[embedings_all_words], output_dim=embedding_dim, input_length=max_len, mask_zero=True))\n",
        "# model.add(Bidirectional(LSTM(300, return_sequences=True)))  # Set return_sequences=True to get sequences instead of a single output\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# # Compile the model (you can use the appropriate loss and optimizer for your task)\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # Print the model summary\n",
        "# model.summary()\n",
        "\n",
        "# embedding_layer = model.layers[1]  # Get the Embedding layer (the first layer)\n",
        "# embedding_model = Model(inputs=model.input, outputs=embedding_layer.output)\n",
        "\n",
        "# # Now, you can use embedding_model to get the hidden states\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neZiLWpKvovo"
      },
      "source": [
        "# number of aspects words from aspect 1 (teacher) in each review  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDjgLUbEHM0e"
      },
      "outputs": [],
      "source": [
        "# aspect_1 = \"the teacher\"\n",
        "# aspect_2 = \"the course\"\n",
        "# aspects = [\"the teacher\",\"the course\"]\n",
        "# aspect1_for_all_sentence =[]\n",
        "# aspect2_for_all_sentence =[]\n",
        "# label_aspect_1 = []\n",
        "# label_aspect_2 = []\n",
        "\n",
        "\n",
        "# label1 = df_data[\"labeles_aspect1\"].tolist()\n",
        "# label2 = df_data[\"labeles_aspect2\"].tolist()\n",
        "# for index,sen in enumerate(sentences):\n",
        "#   if 'teacher' in sen:\n",
        "#     aspect1_for_all_sentence.append(sen)\n",
        "#     label_aspect_1.append(label1[index])\n",
        "#   if 'course' in sen:\n",
        "#     aspect2_for_all_sentence.append(sen)\n",
        "#     label_aspect_2.append(label2[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSelcIWnGypg"
      },
      "outputs": [],
      "source": [
        "# aspects_words_1 = [\"the teacher\"]*len(label_aspect_1)\n",
        "# aspects_words_2 = [\"the course\"]*len(label_aspect_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2OHZVe1G1RP"
      },
      "outputs": [],
      "source": [
        "# len(aspects_words_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qpfPmETI74g"
      },
      "outputs": [],
      "source": [
        "# print(len(sentences))\n",
        "# print(len(aspect1_for_all_sentence))\n",
        "# print(len(aspect2_for_all_sentence))\n",
        "# print(len(sentences) - (len(aspect1_for_all_sentence)+len(aspect2_for_all_sentence)))\n",
        "# print(aspect1_for_all_sentence[0])\n",
        "# print(aspect2_for_all_sentence[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWNusIMovovq"
      },
      "source": [
        "# embedding for aspect1 set words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoUaY-rZMpb3"
      },
      "outputs": [],
      "source": [
        "# words =  word_tokenize(aspect_1.lower())\n",
        "# embedding_words = []\n",
        "# for index,word in enumerate(words):\n",
        "#     glove_emb = glove_vectors.get(word)\n",
        "#     word_embedding = torch.tensor(glove_emb).unsqueeze(0).unsqueeze(0)\n",
        "#     attention_output = attention_layer(word_embedding)\n",
        "#     att_embedding = attention_output.squeeze(0).squeeze(0).detach().numpy()\n",
        "#     con = np.concatenate([glove_emb, att_embedding], axis=0)\n",
        "#     embedding_words.append(con)\n",
        "\n",
        "# embedding_aspect_1 = embedding_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFeHRrk4QHIS"
      },
      "outputs": [],
      "source": [
        "# print(np.array(embedding_aspect_1).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi6jRZzvovr"
      },
      "source": [
        "# embedding for aspect2 set words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXwpB2lRPUkp"
      },
      "outputs": [],
      "source": [
        "# words =  word_tokenize(aspect_2.lower())\n",
        "# embedding_words = []\n",
        "# for index,word in enumerate(words):\n",
        "#     glove_emb = glove_vectors.get(word)\n",
        "#     word_embedding = torch.tensor(glove_emb).unsqueeze(0).unsqueeze(0)  # Reshape to (1, 1, d_model)\n",
        "#     attention_output = attention_layer(word_embedding)\n",
        "#     att_embedding = attention_output.squeeze(0).squeeze(0).detach().numpy()\n",
        "#     con = np.concatenate([glove_emb, att_embedding], axis=0)\n",
        "#     embedding_words.append(con)\n",
        "\n",
        "# embedding_aspect_2 = embedding_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K1kA64vQcQz"
      },
      "outputs": [],
      "source": [
        "# print(np.array(embedding_aspect_2).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvNcM5fIvovu"
      },
      "source": [
        "# embedding for all aspects words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eABP2rxqQ1wd"
      },
      "outputs": [],
      "source": [
        "# embedding_for_all_aspects = []\n",
        "# embedding_for_all_aspects.extend(embedding_aspect_1)\n",
        "# embedding_for_all_aspects.extend(embedding_aspect_2)\n",
        "# np.array(embedding_for_all_aspects).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abYWZCrSvov0"
      },
      "source": [
        "# saving embedding words aspects in pkl file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "820wudsZvov0"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('/content/drive/MyDrive/ASC/embedding_for_all_aspects.pkl', 'wb') as f:\n",
        "#     pickle.dump(embedding_for_all_aspects, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV2chA-wvov1"
      },
      "source": [
        "# read embedding words aspects in pkl file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD7MY99_vov1"
      },
      "outputs": [],
      "source": [
        "# embedings_all_words_in_aspects = []\n",
        "# with (open('/content/drive/MyDrive/ASC/embedding_for_all_aspects.pkl', \"rb\")) as openfile:\n",
        "#     while True:\n",
        "#         try:\n",
        "#             embedings_all_words_in_aspects.append(pickle.load(openfile))\n",
        "#         except EOFError:\n",
        "#             break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw29umpCZDDB"
      },
      "outputs": [],
      "source": [
        "# embedings_all_words_in_aspects = np.array(embedings_all_words_in_aspects).reshape(4, 600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjQXAJ23vov2"
      },
      "source": [
        "# bi-lstm for aspects words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnZKJKivvov2",
        "outputId": "923a582b-483a-47de-e4f4-1ea6474e3979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 2, 600)            2400      \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 2, 600)            2162400   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2164800 (8.26 MB)\n",
            "Trainable params: 2164800 (8.26 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# vocab_size_for_aspects = 4  # Define your vocabulary size\n",
        "# embedding_dim = 600  # Define your embedding dimensions\n",
        "# max_len_aspect = 2  # Length of each input sequence (since we're inputting one aspect word)\n",
        "\n",
        "# # Assuming you have pre-trained word embeddings stored in 'embeddings_all_words'\n",
        "\n",
        "# model = Sequential([\n",
        "#     Embedding(input_dim=vocab_size_for_aspects, weights=[embedings_all_words_in_aspects], output_dim=embedding_dim, input_length=max_len_aspect, mask_zero=True),  # Embedding layer\n",
        "#     Bidirectional(LSTM(units=300, return_sequences=True)),           # Bi-LSTM layer\n",
        "# ])\n",
        "# # Compile the model\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# # Print the model summary\n",
        "# model.summary()\n",
        "\n",
        "# embedding_layer = model.layers[1]  # Get the Embedding layer (the first layer)\n",
        "# embedding_model_for_aspect_word = Model(inputs=model.input, outputs=embedding_layer.output)\n",
        "# # 'embedding' will contain the embedding vector for the aspect word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjacuc8cqGMD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# ooooooooooooooooooooo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_U_GpsOvov3"
      },
      "source": [
        "# train test split for data which belong to aspect2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW1CNbso8H5J"
      },
      "outputs": [],
      "source": [
        "# df_aspect_1 = pd.DataFrame({'Review Text': aspect1_for_all_sentence ,'label': label_aspect_1 ,'aspect word' : aspects_words_1})\n",
        "# df_aspect_2 = pd.DataFrame({'Review Text': aspect2_for_all_sentence ,'label': label_aspect_2 ,'aspect word' : aspects_words_2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_87WYT4ZDGG"
      },
      "outputs": [],
      "source": [
        "# X = df_data\n",
        "# y = df_data['label']\n",
        "\n",
        "# # Splitting the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVCIsBSlvov5"
      },
      "outputs": [],
      "source": [
        "# df_data.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNY1DhcOvov6"
      },
      "outputs": [],
      "source": [
        "# X_train.shape , X_test.shape , y_train.shape , y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pjeVxBGKOgD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Save each variable to a pickle file\n",
        "# def save_to_pickle(data, file_path):\n",
        "#     with open(file_path, 'wb') as f:\n",
        "#         pickle.dump(data, f)\n",
        "\n",
        "# # Save X_train\n",
        "# save_to_pickle(X_train, '/content/drive/MyDrive/ASC/dataset/X_train.pkl')\n",
        "\n",
        "# # Save X_test\n",
        "# save_to_pickle(X_test, '/content/drive/MyDrive/ASC/dataset/X_test.pkl')\n",
        "\n",
        "# # Save y_train\n",
        "# save_to_pickle(y_train, '/content/drive/MyDrive/ASC/dataset/y_train.pkl')\n",
        "\n",
        "# # Save y_test\n",
        "# save_to_pickle(y_test, '/content/drive/MyDrive/ASC/dataset/y_test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMTo1lZqKz2U"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Function to load data from pickle file\n",
        "def load_from_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "# Load X_train\n",
        "X_train = load_from_pickle('/content/drive/MyDrive/ASC/dataset/X_train.pkl')\n",
        "\n",
        "# Load X_test\n",
        "X_test = load_from_pickle('/content/drive/MyDrive/ASC/dataset/X_test.pkl')\n",
        "\n",
        "# Load y_train\n",
        "y_train = load_from_pickle('/content/drive/MyDrive/ASC/dataset/y_train.pkl')\n",
        "\n",
        "# Load y_test\n",
        "y_test = load_from_pickle('/content/drive/MyDrive/ASC/dataset/y_test.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiq18xqfrma_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DasaN-fkLGf1",
        "outputId": "e32d7de8-8c43-45ac-a78d-0461518d6b66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((39456, 4), (9865, 4), (39456,), (9865,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "X_train.shape , X_test.shape , y_train.shape , y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji61KWGdvov8"
      },
      "source": [
        "# make sequence tokenizing vector , and padding equal to the length of the longest review , for train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DibRWPTb2WYC",
        "outputId": "a189bd6c-cb56-447e-8b41-98cf310fc7fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(39456, 466)\n"
          ]
        }
      ],
      "source": [
        "# tokenizer1 = Tokenizer(num_words=number_of_words, char_level=False)\n",
        "# tokenizer1.fit_on_texts(X_train['processed_review'])\n",
        "# training_sequences = tokenizer1.texts_to_sequences(X_train['processed_review'])\n",
        "\n",
        "# # Apply padding to sequences\n",
        "# training_padded = pad_sequences(training_sequences, maxlen= max_len, padding='post', truncating='post')\n",
        "\n",
        "# # Now the shape of training_padded should be (num_samples, 254)\n",
        "# print(training_padded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZPvRJ4vov9"
      },
      "source": [
        "# make sequence tokenizing vector , and padding equal to the length of the longest review , for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZCegM4Hvov-",
        "outputId": "ccf98e8b-7710-4ef1-ad48-6c3bbdfd9307"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9865, 466)\n"
          ]
        }
      ],
      "source": [
        "# tokenizer1 = Tokenizer(num_words=number_of_words, char_level=False)\n",
        "# tokenizer1.fit_on_texts(X_test['processed_review'])\n",
        "# testing_sequences = tokenizer1.texts_to_sequences(X_test['processed_review'])\n",
        "\n",
        "# # Apply padding to sequences\n",
        "# testing_padded = pad_sequences(testing_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# # Now the shape of training_padded should be (num_samples, 254)\n",
        "# print(testing_padded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlOJugMS06J_"
      },
      "source": [
        "<!--  -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR7H4MUFhNHn"
      },
      "source": [
        "# make sequence tokenizing vector , for the aspects , for train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxmQD3-0vowD",
        "outputId": "2e7fbae0-9e10-4442-a2a7-b54eae25c810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(39456, 2)\n"
          ]
        }
      ],
      "source": [
        "# tokenizer1 = Tokenizer(num_words=4, char_level=False)\n",
        "# tokenizer1.fit_on_texts(X_train['aspect'])\n",
        "# training_aspects_sequences = tokenizer1.texts_to_sequences(X_train['aspect'])\n",
        "\n",
        "# # Apply padding to sequences\n",
        "# training_aspects_padded = pad_sequences(training_aspects_sequences, maxlen=2, padding='post', truncating='post')\n",
        "\n",
        "# # Now the shape of training_padded should be (num_samples, 254)\n",
        "# print(training_aspects_padded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyqiU-AWhROP"
      },
      "source": [
        "# make sequence tokenizing vector , for the aspects , for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAK2vTY0I2eT",
        "outputId": "13c022a8-50bc-4071-d0ba-643f05b7fc49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9865, 2)\n"
          ]
        }
      ],
      "source": [
        "# tokenizer1 = Tokenizer(num_words=4, char_level=False)\n",
        "# tokenizer1.fit_on_texts(X_test['aspect'])\n",
        "# testing_aspects_sequences = tokenizer1.texts_to_sequences(X_test['aspect'])\n",
        "\n",
        "# # Apply padding to sequences\n",
        "# testing_aspects_padded = pad_sequences(testing_aspects_sequences, maxlen=2, padding='post', truncating='post')\n",
        "\n",
        "# # Now the shape of training_padded should be (num_samples, 254)\n",
        "# print(testing_aspects_padded.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3tc25CqvowB"
      },
      "source": [
        "# use the bi-lstm model on the training reviews data , to get the hidden state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cbByZR8J_wl"
      },
      "outputs": [],
      "source": [
        "# def algorithm(data_padded, aspects_padded, title):\n",
        "#     # Wrap the outer loop with tqdm\n",
        "#     for i in tqdm(range(100, len(data_padded), 100)):\n",
        "#         hidden_states = []\n",
        "#         hidden_states_aspect = []\n",
        "\n",
        "#         input_data = np.array(data_padded[i-100:i])\n",
        "#         hidden_states.append(embedding_model.predict(input_data))\n",
        "#         hidden_states = np.array(hidden_states[0])\n",
        "\n",
        "#         input_data = np.array(aspects_padded[i-100:i])\n",
        "#         hidden_states_aspect.append(embedding_model_for_aspect_word.predict(input_data))\n",
        "#         hidden_states_aspect = np.array(hidden_states_aspect[0])\n",
        "\n",
        "#         for j in range(hidden_states.shape[0]):\n",
        "#             interaction = np.dot(hidden_states[j], hidden_states_aspect[j].T)\n",
        "\n",
        "#             softmaxed_matrix_columns = softmax(interaction, axis=0)\n",
        "#             softmaxed_matrix_rows = softmax(interaction, axis=1)\n",
        "#             column_avg_matrix = np.mean(softmaxed_matrix_rows, axis=0)\n",
        "#             upsilon = np.dot(softmaxed_matrix_columns, column_avg_matrix)\n",
        "#             r = np.dot(upsilon, hidden_states[j])\n",
        "\n",
        "#             with open(f'/content/drive/MyDrive/ASC/dataset/{title}/{i-100+j}.pkl', 'wb') as f:\n",
        "#                 pickle.dump(r, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "de39776067424779a8506b8ef4a41882",
            "5ae5b1158ba24d8180741fb75e55f10c",
            "67e0c30580a34fa189e417b4b99f8b75",
            "94653bf3562b4c7fb1c0dd3d884e20ab",
            "d360f5f3e6b64d8a8c4bd26102b61403",
            "be5dd67a17f5484ba227a40de0724b5f",
            "5352f2ec00204538873634d0028c9e40",
            "ee3a02a6f40640989eadde32979b8c55",
            "2d5e1277d6d9485fa458de8d9591c7b8",
            "506dff407b2c4c85a1d45e98a3375878",
            "f6aa6069303a4b1893ac089a89be24d4"
          ]
        },
        "id": "xkzMiCDu1hos",
        "outputId": "0aeea5fb-bed7-420c-b4cf-3876f77b8220"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de39776067424779a8506b8ef4a41882",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/394 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 2s 7ms/step\n",
            "4/4 [==============================] - 3s 4ms/step\n",
            "4/4 [==============================] - 1s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 13ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 61ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 57ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 53ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 68ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 58ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 57ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 57ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 61ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 58ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 61ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 62ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 59ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 59ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 60ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 60ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 10ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 62ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 59ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 59ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 57ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 11ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 12ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 62ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 59ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "# algorithm(training_padded , training_aspects_padded , 'train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e8217b37ecc0489a85cfd63adaea902a",
            "ae230456060c4e07a557d71827ad11da",
            "4e79b7f4e5c8457ab5585136ef7f370f",
            "427f6367733848b98d2f7cd71bd9207e",
            "03785cd2135945348d9bfb000eead73e",
            "bc033c0b573c4c6b91d8634b263d99b0",
            "71a2817fdf954b46973eacd20f702ef4",
            "7478d977c8de4ac9a1ce10a329bb718f",
            "c6e7186153ce4c6f90c32ee7308a0964",
            "4897088a258a490a81b62c43b3f9822b",
            "fa9a0c457d774dcf824e7e566624e06b"
          ]
        },
        "id": "x_54lBHa1hkR",
        "outputId": "8b562e64-209c-457f-8ea0-3ada142184f2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8217b37ecc0489a85cfd63adaea902a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/98 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 60ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 9ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 66ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 68ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 5ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 6ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n",
            "4/4 [==============================] - 0s 8ms/step\n",
            "4/4 [==============================] - 0s 3ms/step\n",
            "4/4 [==============================] - 0s 7ms/step\n",
            "4/4 [==============================] - 0s 4ms/step\n"
          ]
        }
      ],
      "source": [
        "# algorithm(testing_padded , testing_aspects_padded , 'test')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s6jUIEparV2"
      },
      "outputs": [],
      "source": [
        "# len(training_padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmvkovfA1hgV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfrGbahl1hVK"
      },
      "outputs": [],
      "source": [
        "# input_data = np.array(testing_padded[:2])\n",
        "# hidden_states_testing= embedding_model.predict(input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GhdAKdt1hN1"
      },
      "outputs": [],
      "source": [
        "# del hidden_states_testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POqMcpiPTEcc"
      },
      "outputs": [],
      "source": [
        "# hidden_states_testing = []\n",
        "# for i in range(500,len(testing_padded),500):\n",
        "#   input_data = np.array(testing_padded[i-500:i])\n",
        "#   hidden_states_testing.append(embedding_model.predict(input_data))\n",
        "# input_data = np.array(testing_padded[9500:9865])\n",
        "# hidden_states_testing.append(embedding_model.predict(input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRZQs41fD1MB"
      },
      "outputs": [],
      "source": [
        "# input_data = np.array(training_padded[9500:9865])\n",
        "# hidden_states_training.append(embedding_model.predict(input_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgqZX7Ji_iDW"
      },
      "outputs": [],
      "source": [
        "# np.array(hidden_states_training[0]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLwd0QxwGME5"
      },
      "outputs": [],
      "source": [
        "# hidden_states_training = np.concatenate(hidden_states_training, axis=0)\n",
        "# print(hidden_states_training.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdyP9PzW_Zhm"
      },
      "outputs": [],
      "source": [
        "# hidden_states_training.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXO_UjZZvowC"
      },
      "source": [
        "# use the bi-lstm model on the testing reviews data , to get the hidden state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l69uuKZeJOSm"
      },
      "outputs": [],
      "source": [
        "# training_padded.shape_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg0PToyYJCPi"
      },
      "outputs": [],
      "source": [
        "# testing_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wx_xgbPwvowC"
      },
      "outputs": [],
      "source": [
        "# # Convert input_data into a numpy array\n",
        "# input_data = np.array(testing_padded)\n",
        "# hidden_states_tasting = embedding_model.predict(input_data)\n",
        "# hidden_states_tasting.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlH6BuvyvowD"
      },
      "source": [
        "# make sequence tokenizing vector , for the aspects , for train data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_x8CKRxvowF"
      },
      "source": [
        "# use the bi-lstm model on the training aspects data , to get the hidden state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0zlGt3kvowF"
      },
      "outputs": [],
      "source": [
        "# # Convert input_data into a numpy array\n",
        "# input_data = np.array(training_aspects_padded)\n",
        "# hidden_states_aspects_training = embedding_model_for_aspect_word.predict(input_data)\n",
        "# hidden_states_aspects_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5WFhqfOYmRr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JrDUEuHNJqE"
      },
      "outputs": [],
      "source": [
        "# # Convert input_data into a numpy array\n",
        "# input_data = np.array(testing_aspects_padded)\n",
        "# hidden_states_aspects_testing = embedding_model_for_aspect_word.predict(input_data)\n",
        "# hidden_states_aspects_testing.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2moKrVZvowG"
      },
      "source": [
        "# save hidden states for all previous steps to pkl file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9YzhTPAvowH"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('/content/drive/MyDrive/ASC/hidden_states_training.pkl', 'wb') as f:\n",
        "#     pickle.dump(hidden_states_training, f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/ASC/hidden_states_aspects_training.pkl', 'wb') as f:\n",
        "#     pickle.dump(hidden_states_aspects_training, f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/ASC/hidden_states_tasting.pkl', 'wb') as f:\n",
        "#     pickle.dump(hidden_states_tasting, f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/ASC/hidden_states_aspects_testing.pkl', 'wb') as f:\n",
        "#     pickle.dump(hidden_states_aspects_testing, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiqmPrEpvowH"
      },
      "source": [
        "# read hidden states from pkl file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKknQmzVvowH"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_testing = []\n",
        "# with (open(\"/content/drive/MyDrive/ASC/hidden_states_aspects_testing.pkl\", \"rb\")) as openfile:\n",
        "#     while True:\n",
        "#         try:\n",
        "#             hidden_states_aspects_testing.append(pickle.load(openfile))\n",
        "#         except EOFError:\n",
        "#             break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-etL6fWM44qV"
      },
      "outputs": [],
      "source": [
        "# hidden_states_training = []\n",
        "# with (open(\"/content/drive/MyDrive/ASC/hidden_states_training.pkl\", \"rb\")) as openfile:\n",
        "#     while True:\n",
        "#         try:\n",
        "#             hidden_states_training.append(pickle.load(openfile))\n",
        "#         except EOFError:\n",
        "#             break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vwXyRRFvowI"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_training = []\n",
        "# with (open(\"/content/drive/MyDrive/ASC/hidden_states_aspects_training.pkl\", \"rb\")) as openfile:\n",
        "#     while True:\n",
        "#         try:\n",
        "#             hidden_states_aspects_training.append(pickle.load(openfile))\n",
        "#         except EOFError:\n",
        "#             break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh2s_kPJvowJ"
      },
      "outputs": [],
      "source": [
        "# hidden_states_tasting = []\n",
        "# with (open(\"/content/drive/MyDrive/ASC/hidden_states_tasting.pkl\", \"rb\")) as openfile:\n",
        "#     while True:\n",
        "#         try:\n",
        "#             hidden_states_tasting.append(pickle.load(openfile))\n",
        "#         except EOFError:\n",
        "#             break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAyg8_ahvowJ"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_testing = np.array(hidden_states_aspects_testing[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WrXeZngvowK"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_testing.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8MByz2mvowK"
      },
      "outputs": [],
      "source": [
        "# hidden_states_tasting = np.array(hidden_states_tasting[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVDl7NwivowL"
      },
      "outputs": [],
      "source": [
        "# hidden_states_tasting.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDpvQdFovowL"
      },
      "outputs": [],
      "source": [
        "# hidden_states_training  = np.array(hidden_states_training[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqm4t50uvowM"
      },
      "outputs": [],
      "source": [
        "# hidden_states_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwSbHZL9vowN"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_training = np.array(hidden_states_aspects_training[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI872NKyvowN"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqVTNgjvvowO"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_training.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdMWqNDpvowO"
      },
      "source": [
        "# applying all the steps of the approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb-l2K7GE32I"
      },
      "outputs": [],
      "source": [
        "# hidden_states_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-8e5Ey0E7eJ"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8aSKyUbFGrM"
      },
      "outputs": [],
      "source": [
        "# hidden_states_aspects_training.T.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXtRTwqnvowQ"
      },
      "outputs": [],
      "source": [
        "# result  = []\n",
        "# for i in range(hidden_states_aspects_training.shape[0]):\n",
        "#     result.append(np.dot(hidden_states_training[i], hidden_states_aspects_training[i].T))\n",
        "\n",
        "\n",
        "\n",
        "# print(np.array(result).shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OCOUkSYvowR"
      },
      "outputs": [],
      "source": [
        "# result2  = []\n",
        "# for i in range(hidden_states_aspects_testing.shape[0]):\n",
        "#     result2.append(np.dot(hidden_states_tasting[i], hidden_states_aspects_testing[i].T)  )\n",
        "\n",
        "# print(np.array(result2).shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLb7AzchvowS"
      },
      "outputs": [],
      "source": [
        "# result2[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2F9_OipvowT"
      },
      "outputs": [],
      "source": [
        "# result[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfNU9YQJvowT"
      },
      "outputs": [],
      "source": [
        "# softmaxed_matrix_columns = []\n",
        "\n",
        "# for i in range(hidden_states_aspects_training.shape[0]):\n",
        "#     softmaxed_matrix_columns.append(softmax(result[i], axis=0))\n",
        "\n",
        "# print(np.array(softmaxed_matrix_columns).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jJn6mLSvowU"
      },
      "outputs": [],
      "source": [
        "# softmaxed_matrix_columns_testing = []\n",
        "\n",
        "# for i in range(hidden_states_aspects_testing.shape[0]):\n",
        "#     softmaxed_matrix_columns_testing.append(softmax(result2[i], axis=0))\n",
        "\n",
        "# print(np.array(softmaxed_matrix_columns_testing).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qC6U2IyvowV"
      },
      "outputs": [],
      "source": [
        "# softmaxed_matrix_columns_testing[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9Xp4umhvowV"
      },
      "outputs": [],
      "source": [
        "# softmaxed_matrix_columns[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wV4D8JLYvowW"
      },
      "outputs": [],
      "source": [
        "# softmaxed_matrix_rows = []\n",
        "\n",
        "# for i in range(hidden_states_aspects_training.shape[0]):\n",
        "#     softmaxed_matrix_rows.append(softmax(result[i], axis=1))\n",
        "\n",
        "# print(np.array(softmaxed_matrix_rows).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67kY7C_9vowX"
      },
      "outputs": [],
      "source": [
        "# softmaxed_matrix_rows_testing = []\n",
        "\n",
        "# for i in range(hidden_states_aspects_testing.shape[0]):\n",
        "#     softmaxed_matrix_rows_testing.append(softmax(result2[i], axis=1))\n",
        "\n",
        "# print(np.array(softmaxed_matrix_rows_testing).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKhKFAWqvowX"
      },
      "outputs": [],
      "source": [
        "# softmaxed_matrix_rows_testing[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EivD_xMOvowY"
      },
      "outputs": [],
      "source": [
        "# column_avg_matrix = []\n",
        "# for i in range(hidden_states_aspects_training.shape[0]):\n",
        "#     column_avg_matrix.append(np.mean(softmaxed_matrix_rows[i], axis=0))\n",
        "\n",
        "# print(np.array(column_avg_matrix).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1NJfBhpvowZ"
      },
      "outputs": [],
      "source": [
        "# column_avg_matrix_testing = []\n",
        "# for i in range(hidden_states_aspects_testing.shape[0]):\n",
        "#     column_avg_matrix_testing.append(np.mean(softmaxed_matrix_rows_testing[i], axis=0))\n",
        "\n",
        "# print(np.array(column_avg_matrix_testing).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV9mPFFfvowZ"
      },
      "outputs": [],
      "source": [
        "# column_avg_matrix_testing[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbfMuDE9vowa"
      },
      "outputs": [],
      "source": [
        "# column_avg_matrix[0].shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6nz__7Tvowb"
      },
      "outputs": [],
      "source": [
        "# #softmaxed_matrix_columns , column_avg_matrix\n",
        "# vector_result = []\n",
        "# for i in range(hidden_states_aspects_training.shape[0]):\n",
        "#     vector_result.append(np.dot(softmaxed_matrix_columns[i], column_avg_matrix[i]))\n",
        "\n",
        "# print(np.array(vector_result).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itIEw0vPvowc"
      },
      "outputs": [],
      "source": [
        "# vector_result_testing = []\n",
        "# for i in range(hidden_states_aspects_testing.shape[0]):\n",
        "#     vector_result_testing.append(np.dot(softmaxed_matrix_columns_testing[i], column_avg_matrix_testing[i]))\n",
        "\n",
        "# print(np.array(vector_result_testing).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdUXMwwtvowc"
      },
      "outputs": [],
      "source": [
        "# np.array(vector_result_testing[0]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdevFFarvowd"
      },
      "outputs": [],
      "source": [
        "# np.array(vector_result[0]).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un97fXMxvowd"
      },
      "outputs": [],
      "source": [
        "# hidden_states_training.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T9EQv4dvowe"
      },
      "outputs": [],
      "source": [
        "# np.array(vector_result).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df9icfmDvowe"
      },
      "outputs": [],
      "source": [
        "# hidden_state_with_vector = []\n",
        "\n",
        "# for i in range(hidden_states_aspects_training.shape[0]):\n",
        "#     hidden_state_with_vector.append(np.dot( vector_result[i] , hidden_states_training[i]))\n",
        "# print(np.array(hidden_state_with_vector).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2_SPg5xvowf"
      },
      "outputs": [],
      "source": [
        "# hidden_state_with_vector_testing = []\n",
        "\n",
        "# for i in range(hidden_states_aspects_testing.shape[0]):\n",
        "#     hidden_state_with_vector_testing.append(np.dot( vector_result_testing[i] , hidden_states_tasting[i]))\n",
        "# print(np.array(hidden_state_with_vector_testing).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zbj-62UXvowh"
      },
      "outputs": [],
      "source": [
        "# np.array(hidden_state_with_vector[0]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crp2YOhJvowi"
      },
      "outputs": [],
      "source": [
        "# hidden_state_with_vector_testing  = np.array(hidden_state_with_vector_testing)\n",
        "# # hidden_state_with_vector_testing  = np.reshape(hidden_state_with_vector_testing,len(hidden_state_with_vector_testing),600)\n",
        "# hidden_state_with_vector_testing.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUFjp0q7vowr"
      },
      "outputs": [],
      "source": [
        "# hidden_state_with_vector  =  np.array(hidden_state_with_vector)\n",
        "# hidden_state_with_vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6uwJhRHvowr"
      },
      "outputs": [],
      "source": [
        "# hidden_state_with_vector = np.reshape(hidden_state_with_vector, (len(hidden_state_with_vector), 600))\n",
        "# hidden_state_with_vector.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhHjhCDpvows"
      },
      "outputs": [],
      "source": [
        "# y_train  =  np.array(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAiz_zOrvowt"
      },
      "outputs": [],
      "source": [
        "# y_test =  np.array(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yGahac_vowt"
      },
      "source": [
        "# train the last fully conected layer for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqEFuT9pP7p9",
        "outputId": "50e43188-63a3-4092-b155-cbfc318f561d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading hidden state batches: 100%|██████████| 39400/39400 [00:36<00:00, 1091.48it/s]\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import multiprocessing\n",
        "\n",
        "# def load_hidden_state(file_path):\n",
        "#     t = []\n",
        "#     if os.path.exists(file_path):\n",
        "#         with open(file_path, \"rb\") as openfile:\n",
        "#             while True:\n",
        "#                 try:\n",
        "#                     t.append(pickle.load(openfile))\n",
        "#                 except EOFError:\n",
        "#                     break\n",
        "#     else:\n",
        "#         print(f\"File does not exist: {file_path}\")\n",
        "\n",
        "#     return np.array(t)\n",
        "\n",
        "# def load_hidden_state_batch_parallel(filepaths):\n",
        "#     with multiprocessing.Pool(processes=(multiprocessing.cpu_count())) as pool:\n",
        "#         results = list(tqdm(pool.imap(load_hidden_state, filepaths), total=len(filepaths), desc=\"Loading hidden state batches\"))\n",
        "\n",
        "#     return np.concatenate(results)\n",
        "\n",
        "# # Generate file paths\n",
        "# filepaths = [f\"/content/drive/MyDrive/ASC/dataset/train/{i}.pkl\" for i in range(39400)]\n",
        "\n",
        "# # Load hidden state batches in parallel\n",
        "# X_parallel = load_hidden_state_batch_parallel(filepaths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp-LI9ZBifiW",
        "outputId": "fa8794e1-4e40-4dc0-b5c8-c428cf10182d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading hidden state batches: 100%|██████████| 9800/9800 [00:09<00:00, 1069.44it/s]\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import multiprocessing\n",
        "\n",
        "# def load_hidden_state(file_path):\n",
        "#     t = []\n",
        "#     if os.path.exists(file_path):\n",
        "#         with open(file_path, \"rb\") as openfile:\n",
        "#             while True:\n",
        "#                 try:\n",
        "#                     t.append(pickle.load(openfile))\n",
        "#                 except EOFError:\n",
        "#                     break\n",
        "#     else:\n",
        "#         print(f\"File does not exist: {file_path}\")\n",
        "\n",
        "#     return np.array(t)\n",
        "\n",
        "# def load_hidden_state_batch_parallel(filepaths):\n",
        "#     with multiprocessing.Pool(processes=(multiprocessing.cpu_count())) as pool:\n",
        "#         results = list(tqdm(pool.imap(load_hidden_state, filepaths), total=len(filepaths), desc=\"Loading hidden state batches\"))\n",
        "\n",
        "#     return np.concatenate(results)\n",
        "\n",
        "# # Generate file paths\n",
        "# filepaths = [f\"/content/drive/MyDrive/ASC/dataset/test/{i}.pkl\" for i in range(9800)]\n",
        "\n",
        "# # Load hidden state batches in parallel\n",
        "# X_t = load_hidden_state_batch_parallel(filepaths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evfyS5TyV_Er"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open(\"/content/drive/MyDrive/ASC/dataset/test.pkl\", 'wb') as f:\n",
        "#     pickle.dump(X_t , f)\n",
        "# with open(\"/content/drive/MyDrive/ASC/dataset/train.pkl\", 'wb') as f:\n",
        "#     pickle.dump(X_parallel , f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTUmtCHbOl3Y",
        "outputId": "9b2d9d47-2cb5-45e3-f374-dbfce44ca2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 597 samples\n",
            "Value 1: 592 samples\n",
            "Value 2: 596 samples\n"
          ]
        }
      ],
      "source": [
        "temp = X_train.loc[X_train['aspect'] == 'the teacher', 'label']\n",
        "unique_values, counts = np.unique(temp, return_counts=True)\n",
        "# Print unique values and their respective counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Value {value}: {count} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG81m5uHOlm0",
        "outputId": "193df979-5c3b-4f5d-82e8-948abeed5ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 12378 samples\n",
            "Value 1: 12668 samples\n",
            "Value 2: 12625 samples\n"
          ]
        }
      ],
      "source": [
        "temp = X_train.loc[X_train['aspect'] == 'the course', 'label']\n",
        "unique_values, counts = np.unique(temp, return_counts=True)\n",
        "\n",
        "# Print unique values and their respective counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Value {value}: {count} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRH3rURROB8z",
        "outputId": "a98fcac2-0767-4052-98ef-7cb7312e5dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 145 samples\n",
            "Value 1: 150 samples\n",
            "Value 2: 146 samples\n"
          ]
        }
      ],
      "source": [
        "temp = X_test.loc[X_test['aspect'] == 'the teacher', 'label']\n",
        "unique_values, counts = np.unique(temp, return_counts=True)\n",
        "# Print unique values and their respective counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Value {value}: {count} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3zJyhpCN_N8",
        "outputId": "90876986-6440-4fbd-8ea7-7ac797ff1df0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Value 0: 3059 samples\n",
            "Value 1: 3161 samples\n",
            "Value 2: 3204 samples\n"
          ]
        }
      ],
      "source": [
        "temp = X_test.loc[X_test['aspect'] == 'the course', 'label']\n",
        "unique_values, counts = np.unique(temp, return_counts=True)\n",
        "\n",
        "# Print unique values and their respective counts\n",
        "for value, count in zip(unique_values, counts):\n",
        "    print(f'Value {value}: {count} samples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "my5IVRJNSr07",
        "outputId": "1fc8f19c-48ec-4eac-ddb4-57e7ef26bcd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Values and Counts for 'the teacher' aspect:\n",
            "+-----------+------------------+-----------------+\n",
            "| Sentiment | Count in X_train | Count in X_test |\n",
            "+-----------+------------------+-----------------+\n",
            "|  Negative |       597        |       145       |\n",
            "|  Neutral  |       592        |       150       |\n",
            "|  Positive |       596        |       146       |\n",
            "+-----------+------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Function to print values, counts in X_train, and counts in X_test as a table\n",
        "def print_table(unique_values, counts_train, counts_test):\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Sentiment', 'Count in X_train', 'Count in X_test']\n",
        "\n",
        "    sentiment_mapping = {1: 'Neutral', 0: 'Negative', 2: 'Positive'}\n",
        "\n",
        "    # Add rows\n",
        "    for value, count_train, count_test in zip(unique_values, counts_train, counts_test):\n",
        "        sentiment_label = sentiment_mapping[value]\n",
        "        table.add_row([sentiment_label, count_train, count_test])\n",
        "\n",
        "    print(table)\n",
        "\n",
        "# For X_train\n",
        "temp_train = X_train.loc[X_train['aspect'] == 'the teacher', 'label']\n",
        "unique_values_train, counts_train = np.unique(temp_train, return_counts=True)\n",
        "\n",
        "# For X_test\n",
        "temp_test = X_test.loc[X_test['aspect'] == 'the teacher', 'label']\n",
        "unique_values_test, counts_test = np.unique(temp_test, return_counts=True)\n",
        "\n",
        "# Print combined table\n",
        "print(\"Sentiment Values and Counts for 'the teacher' aspect:\")\n",
        "print_table(unique_values_train, counts_train, counts_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q2nqQqWUC1s",
        "outputId": "4f466e58-52df-49f5-97a4-4aaccf2dc8bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment Values and Counts for 'the course' aspect:\n",
            "+-----------+------------------+-----------------+\n",
            "| Sentiment | Count in X_train | Count in X_test |\n",
            "+-----------+------------------+-----------------+\n",
            "|  Negative |      12378       |       3059      |\n",
            "|  Neutral  |      12668       |       3161      |\n",
            "|  Positive |      12625       |       3204      |\n",
            "+-----------+------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Function to print values, counts in X_train, and counts in X_test as a table\n",
        "def print_table(unique_values_train, counts_train, unique_values_test, counts_test):\n",
        "    table = PrettyTable()\n",
        "    table.field_names = ['Sentiment', 'Count in X_train', 'Count in X_test']\n",
        "\n",
        "    sentiment_mapping = {1: 'Neutral', 0: 'Negative', 2: 'Positive'}\n",
        "\n",
        "    # Add rows for X_train\n",
        "    for value, count_train in zip(unique_values_train, counts_train):\n",
        "        sentiment_label = sentiment_mapping[value]\n",
        "        count_test = counts_test[unique_values_test == value][0] if value in unique_values_test else 0\n",
        "        table.add_row([sentiment_label, count_train, count_test])\n",
        "\n",
        "    # Add rows for X_test unique values not present in X_train\n",
        "    for value in set(unique_values_test) - set(unique_values_train):\n",
        "        sentiment_label = sentiment_mapping[value]\n",
        "        count_train = 0\n",
        "        count_test = counts_test[unique_values_test == value][0]\n",
        "        table.add_row([sentiment_label, count_train, count_test])\n",
        "\n",
        "    print(table)\n",
        "\n",
        "# For X_train\n",
        "temp_train = X_train.loc[X_train['aspect'] == 'the course', 'label']\n",
        "unique_values_train, counts_train = np.unique(temp_train, return_counts=True)\n",
        "\n",
        "# For X_test\n",
        "temp_test = X_test.loc[X_test['aspect'] == 'the course', 'label']\n",
        "unique_values_test, counts_test = np.unique(temp_test, return_counts=True)\n",
        "\n",
        "# Print combined table\n",
        "print(\"Sentiment Values and Counts for 'the course' aspect:\")\n",
        "print_table(unique_values_train, counts_train, unique_values_test, counts_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0v2k05BNVoP"
      },
      "outputs": [],
      "source": [
        "X_parallel = []\n",
        "with (open(\"/content/drive/MyDrive/ASC/dataset/train.pkl\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_parallel.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break\n",
        "X_t = []\n",
        "with (open(\"/content/drive/MyDrive/ASC/dataset/test.pkl\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_t.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T23UPTeWPJV"
      },
      "outputs": [],
      "source": [
        "X_parallel = X_parallel[0]\n",
        "X_t = X_t[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lR2tyr5XYaH",
        "outputId": "0718f160-c159-413b-92eb-2cab5a7f1ee9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9800, 600)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X_t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cx08QXH0Lj8",
        "outputId": "d1aeefff-3aad-4c67-ba0f-e551517faeba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39400, 600)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X_parallel.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA9JMNdnrUjJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "basic = [i for i in range(3)]\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(np.array(basic).reshape(-1,1))\n",
        "y_enc = enc.transform(y_train[:39400].values.reshape(-1, 1))\n",
        "y_enc = [y_enc[j].toarray() for j in range(39400)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAlH0D6NsJFn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFYOfLh0toOY",
        "outputId": "7cf7e04d-a65b-46d8-f448-efa80ca8c640"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39400"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(y_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0mEBc_7ua5D"
      },
      "outputs": [],
      "source": [
        "y_enc = np.array(y_enc).reshape((39400, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRyG4Pz-vE16",
        "outputId": "29df2cf0-0e6f-42a4-b178-d818641e39b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "type(y_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bflukgNQuB8D",
        "outputId": "89d996e8-ec8a-4102-d05b-a3a0308e2683"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "type(X_parallel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzFvJPU5QSB9"
      },
      "source": [
        "# FULLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYsY1KRdvowt",
        "outputId": "c1f710b5-acd6-4a88-deae-a6e6ec8999a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "985/985 [==============================] - 4s 4ms/step - loss: 1.0608 - accuracy: 0.4172 - val_loss: 1.0411 - val_accuracy: 0.4614\n",
            "Epoch 2/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 1.0468 - accuracy: 0.4493 - val_loss: 1.0357 - val_accuracy: 0.4585\n",
            "Epoch 3/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 1.0404 - accuracy: 0.4717 - val_loss: 1.0301 - val_accuracy: 0.4722\n",
            "Epoch 4/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 1.0323 - accuracy: 0.4843 - val_loss: 1.0188 - val_accuracy: 0.5094\n",
            "Epoch 5/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 1.0197 - accuracy: 0.4974 - val_loss: 1.0089 - val_accuracy: 0.4953\n",
            "Epoch 6/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 1.0045 - accuracy: 0.5123 - val_loss: 0.9926 - val_accuracy: 0.5150\n",
            "Epoch 7/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.9863 - accuracy: 0.5297 - val_loss: 0.9706 - val_accuracy: 0.5437\n",
            "Epoch 8/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.9671 - accuracy: 0.5430 - val_loss: 0.9549 - val_accuracy: 0.5555\n",
            "Epoch 9/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.9494 - accuracy: 0.5548 - val_loss: 0.9378 - val_accuracy: 0.5605\n",
            "Epoch 10/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.9342 - accuracy: 0.5652 - val_loss: 0.9311 - val_accuracy: 0.5712\n",
            "Epoch 11/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.9220 - accuracy: 0.5720 - val_loss: 0.9098 - val_accuracy: 0.5887\n",
            "Epoch 12/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.9102 - accuracy: 0.5831 - val_loss: 0.9017 - val_accuracy: 0.5911\n",
            "Epoch 13/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.9005 - accuracy: 0.5874 - val_loss: 0.8939 - val_accuracy: 0.5952\n",
            "Epoch 14/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8921 - accuracy: 0.5952 - val_loss: 0.8824 - val_accuracy: 0.6003\n",
            "Epoch 15/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8847 - accuracy: 0.5968 - val_loss: 0.8752 - val_accuracy: 0.6138\n",
            "Epoch 16/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8786 - accuracy: 0.6035 - val_loss: 0.8696 - val_accuracy: 0.6188\n",
            "Epoch 17/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8721 - accuracy: 0.6084 - val_loss: 0.8644 - val_accuracy: 0.6244\n",
            "Epoch 18/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8666 - accuracy: 0.6139 - val_loss: 0.8610 - val_accuracy: 0.6231\n",
            "Epoch 19/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8621 - accuracy: 0.6174 - val_loss: 0.8537 - val_accuracy: 0.6217\n",
            "Epoch 20/1000\n",
            "985/985 [==============================] - 4s 4ms/step - loss: 0.8570 - accuracy: 0.6222 - val_loss: 0.8490 - val_accuracy: 0.6266\n",
            "Epoch 21/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8525 - accuracy: 0.6285 - val_loss: 0.8489 - val_accuracy: 0.6345\n",
            "Epoch 22/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8483 - accuracy: 0.6304 - val_loss: 0.8472 - val_accuracy: 0.6325\n",
            "Epoch 23/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8448 - accuracy: 0.6348 - val_loss: 0.8404 - val_accuracy: 0.6503\n",
            "Epoch 24/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8409 - accuracy: 0.6398 - val_loss: 0.8346 - val_accuracy: 0.6536\n",
            "Epoch 25/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8388 - accuracy: 0.6426 - val_loss: 0.8336 - val_accuracy: 0.6398\n",
            "Epoch 26/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8341 - accuracy: 0.6449 - val_loss: 0.8308 - val_accuracy: 0.6523\n",
            "Epoch 27/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8321 - accuracy: 0.6501 - val_loss: 0.8332 - val_accuracy: 0.6390\n",
            "Epoch 28/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8277 - accuracy: 0.6528 - val_loss: 0.8271 - val_accuracy: 0.6458\n",
            "Epoch 29/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8257 - accuracy: 0.6559 - val_loss: 0.8218 - val_accuracy: 0.6631\n",
            "Epoch 30/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8236 - accuracy: 0.6555 - val_loss: 0.8199 - val_accuracy: 0.6452\n",
            "Epoch 31/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8206 - accuracy: 0.6588 - val_loss: 0.8148 - val_accuracy: 0.6689\n",
            "Epoch 32/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8180 - accuracy: 0.6632 - val_loss: 0.8136 - val_accuracy: 0.6678\n",
            "Epoch 33/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8151 - accuracy: 0.6654 - val_loss: 0.8119 - val_accuracy: 0.6718\n",
            "Epoch 34/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8122 - accuracy: 0.6673 - val_loss: 0.8110 - val_accuracy: 0.6668\n",
            "Epoch 35/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8094 - accuracy: 0.6700 - val_loss: 0.8109 - val_accuracy: 0.6655\n",
            "Epoch 36/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8087 - accuracy: 0.6680 - val_loss: 0.8059 - val_accuracy: 0.6701\n",
            "Epoch 37/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8061 - accuracy: 0.6714 - val_loss: 0.8072 - val_accuracy: 0.6761\n",
            "Epoch 38/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8034 - accuracy: 0.6714 - val_loss: 0.8030 - val_accuracy: 0.6775\n",
            "Epoch 39/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8033 - accuracy: 0.6746 - val_loss: 0.8007 - val_accuracy: 0.6712\n",
            "Epoch 40/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.8004 - accuracy: 0.6737 - val_loss: 0.8037 - val_accuracy: 0.6756\n",
            "Epoch 41/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7994 - accuracy: 0.6760 - val_loss: 0.7991 - val_accuracy: 0.6845\n",
            "Epoch 42/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7963 - accuracy: 0.6785 - val_loss: 0.7983 - val_accuracy: 0.6777\n",
            "Epoch 43/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7956 - accuracy: 0.6788 - val_loss: 0.7974 - val_accuracy: 0.6852\n",
            "Epoch 44/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7943 - accuracy: 0.6804 - val_loss: 0.7914 - val_accuracy: 0.6854\n",
            "Epoch 45/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7905 - accuracy: 0.6822 - val_loss: 0.7912 - val_accuracy: 0.6892\n",
            "Epoch 46/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7911 - accuracy: 0.6818 - val_loss: 0.7923 - val_accuracy: 0.6824\n",
            "Epoch 47/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7883 - accuracy: 0.6847 - val_loss: 0.7968 - val_accuracy: 0.6806\n",
            "Epoch 48/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7893 - accuracy: 0.6837 - val_loss: 0.8007 - val_accuracy: 0.6766\n",
            "Epoch 49/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7860 - accuracy: 0.6859 - val_loss: 0.7955 - val_accuracy: 0.6872\n",
            "Epoch 50/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7849 - accuracy: 0.6881 - val_loss: 0.7962 - val_accuracy: 0.6857\n",
            "Epoch 51/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7820 - accuracy: 0.6878 - val_loss: 0.7838 - val_accuracy: 0.6900\n",
            "Epoch 52/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7826 - accuracy: 0.6871 - val_loss: 0.7827 - val_accuracy: 0.6967\n",
            "Epoch 53/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7808 - accuracy: 0.6892 - val_loss: 0.7813 - val_accuracy: 0.6951\n",
            "Epoch 54/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7778 - accuracy: 0.6909 - val_loss: 0.7867 - val_accuracy: 0.6829\n",
            "Epoch 55/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7771 - accuracy: 0.6888 - val_loss: 0.7846 - val_accuracy: 0.6846\n",
            "Epoch 56/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7762 - accuracy: 0.6901 - val_loss: 0.7774 - val_accuracy: 0.6897\n",
            "Epoch 57/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7749 - accuracy: 0.6919 - val_loss: 0.7810 - val_accuracy: 0.6844\n",
            "Epoch 58/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7737 - accuracy: 0.6930 - val_loss: 0.7780 - val_accuracy: 0.6944\n",
            "Epoch 59/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7720 - accuracy: 0.6927 - val_loss: 0.7751 - val_accuracy: 0.6990\n",
            "Epoch 60/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7709 - accuracy: 0.6934 - val_loss: 0.7739 - val_accuracy: 0.6972\n",
            "Epoch 61/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7709 - accuracy: 0.6945 - val_loss: 0.7741 - val_accuracy: 0.6863\n",
            "Epoch 62/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7698 - accuracy: 0.6957 - val_loss: 0.7770 - val_accuracy: 0.6961\n",
            "Epoch 63/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7685 - accuracy: 0.6955 - val_loss: 0.7779 - val_accuracy: 0.6980\n",
            "Epoch 64/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7680 - accuracy: 0.6944 - val_loss: 0.7734 - val_accuracy: 0.6915\n",
            "Epoch 65/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7652 - accuracy: 0.6966 - val_loss: 0.7689 - val_accuracy: 0.7009\n",
            "Epoch 66/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7648 - accuracy: 0.6976 - val_loss: 0.7690 - val_accuracy: 0.7025\n",
            "Epoch 67/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7656 - accuracy: 0.6987 - val_loss: 0.7721 - val_accuracy: 0.6999\n",
            "Epoch 68/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7625 - accuracy: 0.6991 - val_loss: 0.7677 - val_accuracy: 0.6977\n",
            "Epoch 69/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7628 - accuracy: 0.6983 - val_loss: 0.7700 - val_accuracy: 0.6968\n",
            "Epoch 70/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7614 - accuracy: 0.6990 - val_loss: 0.7653 - val_accuracy: 0.7033\n",
            "Epoch 71/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7602 - accuracy: 0.6993 - val_loss: 0.7734 - val_accuracy: 0.7052\n",
            "Epoch 72/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7584 - accuracy: 0.7003 - val_loss: 0.7667 - val_accuracy: 0.7042\n",
            "Epoch 73/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7587 - accuracy: 0.7019 - val_loss: 0.7622 - val_accuracy: 0.7016\n",
            "Epoch 74/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7586 - accuracy: 0.7015 - val_loss: 0.7682 - val_accuracy: 0.7024\n",
            "Epoch 75/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7577 - accuracy: 0.6994 - val_loss: 0.7654 - val_accuracy: 0.7069\n",
            "Epoch 76/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7568 - accuracy: 0.7007 - val_loss: 0.7615 - val_accuracy: 0.6989\n",
            "Epoch 77/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7539 - accuracy: 0.7035 - val_loss: 0.7652 - val_accuracy: 0.6876\n",
            "Epoch 78/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7543 - accuracy: 0.7034 - val_loss: 0.7633 - val_accuracy: 0.6902\n",
            "Epoch 79/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7519 - accuracy: 0.7043 - val_loss: 0.7733 - val_accuracy: 0.7036\n",
            "Epoch 80/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7508 - accuracy: 0.7052 - val_loss: 0.7597 - val_accuracy: 0.7004\n",
            "Epoch 81/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7520 - accuracy: 0.7045 - val_loss: 0.7580 - val_accuracy: 0.7057\n",
            "Epoch 82/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7503 - accuracy: 0.7065 - val_loss: 0.7610 - val_accuracy: 0.6987\n",
            "Epoch 83/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7511 - accuracy: 0.7056 - val_loss: 0.7583 - val_accuracy: 0.7048\n",
            "Epoch 84/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7495 - accuracy: 0.7051 - val_loss: 0.7637 - val_accuracy: 0.6944\n",
            "Epoch 85/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7492 - accuracy: 0.7048 - val_loss: 0.7668 - val_accuracy: 0.7036\n",
            "Epoch 86/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7480 - accuracy: 0.7060 - val_loss: 0.7609 - val_accuracy: 0.7049\n",
            "Epoch 87/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7464 - accuracy: 0.7068 - val_loss: 0.7668 - val_accuracy: 0.6964\n",
            "Epoch 88/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7469 - accuracy: 0.7076 - val_loss: 0.7600 - val_accuracy: 0.7070\n",
            "Epoch 89/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7450 - accuracy: 0.7075 - val_loss: 0.7607 - val_accuracy: 0.6985\n",
            "Epoch 90/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7439 - accuracy: 0.7070 - val_loss: 0.7671 - val_accuracy: 0.7060\n",
            "Epoch 91/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7439 - accuracy: 0.7092 - val_loss: 0.7669 - val_accuracy: 0.6966\n",
            "Epoch 92/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7433 - accuracy: 0.7070 - val_loss: 0.7532 - val_accuracy: 0.7074\n",
            "Epoch 93/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7423 - accuracy: 0.7072 - val_loss: 0.7585 - val_accuracy: 0.7041\n",
            "Epoch 94/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7422 - accuracy: 0.7097 - val_loss: 0.7566 - val_accuracy: 0.7034\n",
            "Epoch 95/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7412 - accuracy: 0.7094 - val_loss: 0.7615 - val_accuracy: 0.7058\n",
            "Epoch 96/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7402 - accuracy: 0.7093 - val_loss: 0.7590 - val_accuracy: 0.6996\n",
            "Epoch 97/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7387 - accuracy: 0.7111 - val_loss: 0.7495 - val_accuracy: 0.7036\n",
            "Epoch 98/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7392 - accuracy: 0.7111 - val_loss: 0.7473 - val_accuracy: 0.7079\n",
            "Epoch 99/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7406 - accuracy: 0.7117 - val_loss: 0.7586 - val_accuracy: 0.7099\n",
            "Epoch 100/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7384 - accuracy: 0.7113 - val_loss: 0.7525 - val_accuracy: 0.7104\n",
            "Epoch 101/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7374 - accuracy: 0.7123 - val_loss: 0.7586 - val_accuracy: 0.7024\n",
            "Epoch 102/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7385 - accuracy: 0.7110 - val_loss: 0.7614 - val_accuracy: 0.7062\n",
            "Epoch 103/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7372 - accuracy: 0.7118 - val_loss: 0.7482 - val_accuracy: 0.6986\n",
            "Epoch 104/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7360 - accuracy: 0.7114 - val_loss: 0.7461 - val_accuracy: 0.7148\n",
            "Epoch 105/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7365 - accuracy: 0.7130 - val_loss: 0.7450 - val_accuracy: 0.7112\n",
            "Epoch 106/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7355 - accuracy: 0.7129 - val_loss: 0.7493 - val_accuracy: 0.7077\n",
            "Epoch 107/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7348 - accuracy: 0.7151 - val_loss: 0.7433 - val_accuracy: 0.7080\n",
            "Epoch 108/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7345 - accuracy: 0.7136 - val_loss: 0.7500 - val_accuracy: 0.7124\n",
            "Epoch 109/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7346 - accuracy: 0.7120 - val_loss: 0.7619 - val_accuracy: 0.7047\n",
            "Epoch 110/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7343 - accuracy: 0.7122 - val_loss: 0.7458 - val_accuracy: 0.7085\n",
            "Epoch 111/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7318 - accuracy: 0.7145 - val_loss: 0.7469 - val_accuracy: 0.7077\n",
            "Epoch 112/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7306 - accuracy: 0.7150 - val_loss: 0.7549 - val_accuracy: 0.7061\n",
            "Epoch 113/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7313 - accuracy: 0.7141 - val_loss: 0.7537 - val_accuracy: 0.6905\n",
            "Epoch 114/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7310 - accuracy: 0.7148 - val_loss: 0.7445 - val_accuracy: 0.7157\n",
            "Epoch 115/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7296 - accuracy: 0.7167 - val_loss: 0.7471 - val_accuracy: 0.7123\n",
            "Epoch 116/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7285 - accuracy: 0.7138 - val_loss: 0.7452 - val_accuracy: 0.7118\n",
            "Epoch 117/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7291 - accuracy: 0.7147 - val_loss: 0.7539 - val_accuracy: 0.7070\n",
            "Epoch 118/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7285 - accuracy: 0.7164 - val_loss: 0.7609 - val_accuracy: 0.7020\n",
            "Epoch 119/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7282 - accuracy: 0.7182 - val_loss: 0.7432 - val_accuracy: 0.7071\n",
            "Epoch 120/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7270 - accuracy: 0.7161 - val_loss: 0.7418 - val_accuracy: 0.7119\n",
            "Epoch 121/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7272 - accuracy: 0.7175 - val_loss: 0.7640 - val_accuracy: 0.6997\n",
            "Epoch 122/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7263 - accuracy: 0.7178 - val_loss: 0.7537 - val_accuracy: 0.7025\n",
            "Epoch 123/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7276 - accuracy: 0.7165 - val_loss: 0.7381 - val_accuracy: 0.7137\n",
            "Epoch 124/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7250 - accuracy: 0.7161 - val_loss: 0.7389 - val_accuracy: 0.7074\n",
            "Epoch 125/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7261 - accuracy: 0.7180 - val_loss: 0.7404 - val_accuracy: 0.7074\n",
            "Epoch 126/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7243 - accuracy: 0.7188 - val_loss: 0.7368 - val_accuracy: 0.7140\n",
            "Epoch 127/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7248 - accuracy: 0.7183 - val_loss: 0.7429 - val_accuracy: 0.7088\n",
            "Epoch 128/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7239 - accuracy: 0.7187 - val_loss: 0.7415 - val_accuracy: 0.7100\n",
            "Epoch 129/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7233 - accuracy: 0.7184 - val_loss: 0.7391 - val_accuracy: 0.7113\n",
            "Epoch 130/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7229 - accuracy: 0.7175 - val_loss: 0.7428 - val_accuracy: 0.7093\n",
            "Epoch 131/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7244 - accuracy: 0.7165 - val_loss: 0.7387 - val_accuracy: 0.7137\n",
            "Epoch 132/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7230 - accuracy: 0.7202 - val_loss: 0.7580 - val_accuracy: 0.6961\n",
            "Epoch 133/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7227 - accuracy: 0.7197 - val_loss: 0.7359 - val_accuracy: 0.7128\n",
            "Epoch 134/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7224 - accuracy: 0.7185 - val_loss: 0.7360 - val_accuracy: 0.7154\n",
            "Epoch 135/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7233 - accuracy: 0.7183 - val_loss: 0.7396 - val_accuracy: 0.7044\n",
            "Epoch 136/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7207 - accuracy: 0.7210 - val_loss: 0.7365 - val_accuracy: 0.7187\n",
            "Epoch 137/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7216 - accuracy: 0.7198 - val_loss: 0.7416 - val_accuracy: 0.7184\n",
            "Epoch 138/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7204 - accuracy: 0.7205 - val_loss: 0.7402 - val_accuracy: 0.7058\n",
            "Epoch 139/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7200 - accuracy: 0.7219 - val_loss: 0.7331 - val_accuracy: 0.7133\n",
            "Epoch 140/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7193 - accuracy: 0.7217 - val_loss: 0.7386 - val_accuracy: 0.7168\n",
            "Epoch 141/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7187 - accuracy: 0.7237 - val_loss: 0.7344 - val_accuracy: 0.7154\n",
            "Epoch 142/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7191 - accuracy: 0.7208 - val_loss: 0.7568 - val_accuracy: 0.7080\n",
            "Epoch 143/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7186 - accuracy: 0.7200 - val_loss: 0.7420 - val_accuracy: 0.7143\n",
            "Epoch 144/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7187 - accuracy: 0.7213 - val_loss: 0.7366 - val_accuracy: 0.7166\n",
            "Epoch 145/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7183 - accuracy: 0.7225 - val_loss: 0.7362 - val_accuracy: 0.7110\n",
            "Epoch 146/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7191 - accuracy: 0.7213 - val_loss: 0.7366 - val_accuracy: 0.7124\n",
            "Epoch 147/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7186 - accuracy: 0.7215 - val_loss: 0.7407 - val_accuracy: 0.7197\n",
            "Epoch 148/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7161 - accuracy: 0.7223 - val_loss: 0.7374 - val_accuracy: 0.7218\n",
            "Epoch 149/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7167 - accuracy: 0.7218 - val_loss: 0.7725 - val_accuracy: 0.7018\n",
            "Epoch 150/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7159 - accuracy: 0.7223 - val_loss: 0.7315 - val_accuracy: 0.7197\n",
            "Epoch 151/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7161 - accuracy: 0.7236 - val_loss: 0.7544 - val_accuracy: 0.7020\n",
            "Epoch 152/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7155 - accuracy: 0.7242 - val_loss: 0.7371 - val_accuracy: 0.7179\n",
            "Epoch 153/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7146 - accuracy: 0.7222 - val_loss: 0.7415 - val_accuracy: 0.7151\n",
            "Epoch 154/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7146 - accuracy: 0.7240 - val_loss: 0.7378 - val_accuracy: 0.7209\n",
            "Epoch 155/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7148 - accuracy: 0.7223 - val_loss: 0.7415 - val_accuracy: 0.7207\n",
            "Epoch 156/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7135 - accuracy: 0.7227 - val_loss: 0.7371 - val_accuracy: 0.7124\n",
            "Epoch 157/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7141 - accuracy: 0.7239 - val_loss: 0.7331 - val_accuracy: 0.7145\n",
            "Epoch 158/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7119 - accuracy: 0.7262 - val_loss: 0.7313 - val_accuracy: 0.7150\n",
            "Epoch 159/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7160 - accuracy: 0.7221 - val_loss: 0.7308 - val_accuracy: 0.7110\n",
            "Epoch 160/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7138 - accuracy: 0.7227 - val_loss: 0.7346 - val_accuracy: 0.7164\n",
            "Epoch 161/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7135 - accuracy: 0.7251 - val_loss: 0.7454 - val_accuracy: 0.6990\n",
            "Epoch 162/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7114 - accuracy: 0.7255 - val_loss: 0.7325 - val_accuracy: 0.7192\n",
            "Epoch 163/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7141 - accuracy: 0.7232 - val_loss: 0.7280 - val_accuracy: 0.7184\n",
            "Epoch 164/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7113 - accuracy: 0.7254 - val_loss: 0.7295 - val_accuracy: 0.7155\n",
            "Epoch 165/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7097 - accuracy: 0.7271 - val_loss: 0.7273 - val_accuracy: 0.7192\n",
            "Epoch 166/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7098 - accuracy: 0.7248 - val_loss: 0.7312 - val_accuracy: 0.7150\n",
            "Epoch 167/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7120 - accuracy: 0.7239 - val_loss: 0.7314 - val_accuracy: 0.7201\n",
            "Epoch 168/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7107 - accuracy: 0.7263 - val_loss: 0.7549 - val_accuracy: 0.7077\n",
            "Epoch 169/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7101 - accuracy: 0.7254 - val_loss: 0.7278 - val_accuracy: 0.7235\n",
            "Epoch 170/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7112 - accuracy: 0.7263 - val_loss: 0.7274 - val_accuracy: 0.7225\n",
            "Epoch 171/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7079 - accuracy: 0.7266 - val_loss: 0.7563 - val_accuracy: 0.7057\n",
            "Epoch 172/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7105 - accuracy: 0.7263 - val_loss: 0.7343 - val_accuracy: 0.7188\n",
            "Epoch 173/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7081 - accuracy: 0.7282 - val_loss: 0.7265 - val_accuracy: 0.7178\n",
            "Epoch 174/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7099 - accuracy: 0.7261 - val_loss: 0.7277 - val_accuracy: 0.7199\n",
            "Epoch 175/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7094 - accuracy: 0.7269 - val_loss: 0.7325 - val_accuracy: 0.7152\n",
            "Epoch 176/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7081 - accuracy: 0.7276 - val_loss: 0.7287 - val_accuracy: 0.7185\n",
            "Epoch 177/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7095 - accuracy: 0.7272 - val_loss: 0.7401 - val_accuracy: 0.7150\n",
            "Epoch 178/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7075 - accuracy: 0.7276 - val_loss: 0.7389 - val_accuracy: 0.7122\n",
            "Epoch 179/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7055 - accuracy: 0.7276 - val_loss: 0.7381 - val_accuracy: 0.7096\n",
            "Epoch 180/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7094 - accuracy: 0.7280 - val_loss: 0.7245 - val_accuracy: 0.7203\n",
            "Epoch 181/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7076 - accuracy: 0.7286 - val_loss: 0.7322 - val_accuracy: 0.7122\n",
            "Epoch 182/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7058 - accuracy: 0.7277 - val_loss: 0.7254 - val_accuracy: 0.7187\n",
            "Epoch 183/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7087 - accuracy: 0.7278 - val_loss: 0.7333 - val_accuracy: 0.7168\n",
            "Epoch 184/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7088 - accuracy: 0.7266 - val_loss: 0.7398 - val_accuracy: 0.7188\n",
            "Epoch 185/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7051 - accuracy: 0.7287 - val_loss: 0.7383 - val_accuracy: 0.7093\n",
            "Epoch 186/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7070 - accuracy: 0.7278 - val_loss: 0.7328 - val_accuracy: 0.7166\n",
            "Epoch 187/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7056 - accuracy: 0.7281 - val_loss: 0.7443 - val_accuracy: 0.7226\n",
            "Epoch 188/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7054 - accuracy: 0.7289 - val_loss: 0.7354 - val_accuracy: 0.7180\n",
            "Epoch 189/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7047 - accuracy: 0.7281 - val_loss: 0.7238 - val_accuracy: 0.7245\n",
            "Epoch 190/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7043 - accuracy: 0.7287 - val_loss: 0.7252 - val_accuracy: 0.7184\n",
            "Epoch 191/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7039 - accuracy: 0.7282 - val_loss: 0.7326 - val_accuracy: 0.7207\n",
            "Epoch 192/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7042 - accuracy: 0.7287 - val_loss: 0.7307 - val_accuracy: 0.7119\n",
            "Epoch 193/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7064 - accuracy: 0.7288 - val_loss: 0.7249 - val_accuracy: 0.7217\n",
            "Epoch 194/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7034 - accuracy: 0.7288 - val_loss: 0.7289 - val_accuracy: 0.7166\n",
            "Epoch 195/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7023 - accuracy: 0.7307 - val_loss: 0.7288 - val_accuracy: 0.7179\n",
            "Epoch 196/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7032 - accuracy: 0.7285 - val_loss: 0.7315 - val_accuracy: 0.7232\n",
            "Epoch 197/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7042 - accuracy: 0.7299 - val_loss: 0.7249 - val_accuracy: 0.7202\n",
            "Epoch 198/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7019 - accuracy: 0.7290 - val_loss: 0.7306 - val_accuracy: 0.7042\n",
            "Epoch 199/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7023 - accuracy: 0.7293 - val_loss: 0.7458 - val_accuracy: 0.7171\n",
            "Epoch 200/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7038 - accuracy: 0.7281 - val_loss: 0.7363 - val_accuracy: 0.7244\n",
            "Epoch 201/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7051 - accuracy: 0.7287 - val_loss: 0.7294 - val_accuracy: 0.7204\n",
            "Epoch 202/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7023 - accuracy: 0.7291 - val_loss: 0.7255 - val_accuracy: 0.7212\n",
            "Epoch 203/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6997 - accuracy: 0.7324 - val_loss: 0.7225 - val_accuracy: 0.7221\n",
            "Epoch 204/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7010 - accuracy: 0.7307 - val_loss: 0.7192 - val_accuracy: 0.7216\n",
            "Epoch 205/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7009 - accuracy: 0.7306 - val_loss: 0.7342 - val_accuracy: 0.7218\n",
            "Epoch 206/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7022 - accuracy: 0.7303 - val_loss: 0.7385 - val_accuracy: 0.7244\n",
            "Epoch 207/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7028 - accuracy: 0.7307 - val_loss: 0.7255 - val_accuracy: 0.7284\n",
            "Epoch 208/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7027 - accuracy: 0.7285 - val_loss: 0.7222 - val_accuracy: 0.7237\n",
            "Epoch 209/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7017 - accuracy: 0.7294 - val_loss: 0.7298 - val_accuracy: 0.7164\n",
            "Epoch 210/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6987 - accuracy: 0.7309 - val_loss: 0.7298 - val_accuracy: 0.7166\n",
            "Epoch 211/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7003 - accuracy: 0.7306 - val_loss: 0.7217 - val_accuracy: 0.7202\n",
            "Epoch 212/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7008 - accuracy: 0.7305 - val_loss: 0.7292 - val_accuracy: 0.7218\n",
            "Epoch 213/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6990 - accuracy: 0.7312 - val_loss: 0.7215 - val_accuracy: 0.7240\n",
            "Epoch 214/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6986 - accuracy: 0.7330 - val_loss: 0.7213 - val_accuracy: 0.7164\n",
            "Epoch 215/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7006 - accuracy: 0.7305 - val_loss: 0.7486 - val_accuracy: 0.7124\n",
            "Epoch 216/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.7013 - accuracy: 0.7322 - val_loss: 0.7220 - val_accuracy: 0.7209\n",
            "Epoch 217/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6994 - accuracy: 0.7313 - val_loss: 0.7231 - val_accuracy: 0.7201\n",
            "Epoch 218/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6976 - accuracy: 0.7319 - val_loss: 0.7296 - val_accuracy: 0.7198\n",
            "Epoch 219/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6999 - accuracy: 0.7312 - val_loss: 0.7267 - val_accuracy: 0.7197\n",
            "Epoch 220/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6993 - accuracy: 0.7313 - val_loss: 0.7329 - val_accuracy: 0.7142\n",
            "Epoch 221/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6972 - accuracy: 0.7319 - val_loss: 0.7196 - val_accuracy: 0.7202\n",
            "Epoch 222/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6981 - accuracy: 0.7326 - val_loss: 0.7167 - val_accuracy: 0.7240\n",
            "Epoch 223/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6988 - accuracy: 0.7303 - val_loss: 0.7215 - val_accuracy: 0.7183\n",
            "Epoch 224/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6975 - accuracy: 0.7295 - val_loss: 0.7284 - val_accuracy: 0.7197\n",
            "Epoch 225/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6988 - accuracy: 0.7324 - val_loss: 0.7316 - val_accuracy: 0.7193\n",
            "Epoch 226/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6979 - accuracy: 0.7321 - val_loss: 0.7324 - val_accuracy: 0.7228\n",
            "Epoch 227/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.7328 - val_loss: 0.7196 - val_accuracy: 0.7250\n",
            "Epoch 228/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6962 - accuracy: 0.7317 - val_loss: 0.7218 - val_accuracy: 0.7260\n",
            "Epoch 229/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6976 - accuracy: 0.7311 - val_loss: 0.7167 - val_accuracy: 0.7209\n",
            "Epoch 230/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6957 - accuracy: 0.7324 - val_loss: 0.7213 - val_accuracy: 0.7272\n",
            "Epoch 231/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6970 - accuracy: 0.7313 - val_loss: 0.7551 - val_accuracy: 0.7176\n",
            "Epoch 232/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6975 - accuracy: 0.7312 - val_loss: 0.7561 - val_accuracy: 0.7034\n",
            "Epoch 233/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6969 - accuracy: 0.7324 - val_loss: 0.7253 - val_accuracy: 0.7146\n",
            "Epoch 234/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6955 - accuracy: 0.7330 - val_loss: 0.7329 - val_accuracy: 0.7178\n",
            "Epoch 235/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6963 - accuracy: 0.7345 - val_loss: 0.7436 - val_accuracy: 0.7250\n",
            "Epoch 236/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6991 - accuracy: 0.7291 - val_loss: 0.7171 - val_accuracy: 0.7230\n",
            "Epoch 237/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6959 - accuracy: 0.7321 - val_loss: 0.7288 - val_accuracy: 0.7199\n",
            "Epoch 238/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6958 - accuracy: 0.7322 - val_loss: 0.7239 - val_accuracy: 0.7197\n",
            "Epoch 239/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6951 - accuracy: 0.7340 - val_loss: 0.7291 - val_accuracy: 0.7217\n",
            "Epoch 240/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6978 - accuracy: 0.7322 - val_loss: 0.7211 - val_accuracy: 0.7206\n",
            "Epoch 241/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6939 - accuracy: 0.7346 - val_loss: 0.7222 - val_accuracy: 0.7194\n",
            "Epoch 242/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6968 - accuracy: 0.7320 - val_loss: 0.7592 - val_accuracy: 0.7055\n",
            "Epoch 243/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6978 - accuracy: 0.7332 - val_loss: 0.7167 - val_accuracy: 0.7226\n",
            "Epoch 244/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6948 - accuracy: 0.7345 - val_loss: 0.7423 - val_accuracy: 0.7201\n",
            "Epoch 245/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6933 - accuracy: 0.7337 - val_loss: 0.7346 - val_accuracy: 0.7228\n",
            "Epoch 246/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6942 - accuracy: 0.7344 - val_loss: 0.7183 - val_accuracy: 0.7251\n",
            "Epoch 247/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6948 - accuracy: 0.7316 - val_loss: 0.7498 - val_accuracy: 0.7132\n",
            "Epoch 248/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6942 - accuracy: 0.7355 - val_loss: 0.7242 - val_accuracy: 0.7282\n",
            "Epoch 249/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6918 - accuracy: 0.7342 - val_loss: 0.7417 - val_accuracy: 0.7148\n",
            "Epoch 250/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6938 - accuracy: 0.7334 - val_loss: 0.7241 - val_accuracy: 0.7176\n",
            "Epoch 251/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6953 - accuracy: 0.7320 - val_loss: 0.7205 - val_accuracy: 0.7223\n",
            "Epoch 252/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6925 - accuracy: 0.7339 - val_loss: 0.7230 - val_accuracy: 0.7093\n",
            "Epoch 253/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6934 - accuracy: 0.7332 - val_loss: 0.7141 - val_accuracy: 0.7272\n",
            "Epoch 254/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6932 - accuracy: 0.7348 - val_loss: 0.7466 - val_accuracy: 0.7140\n",
            "Epoch 255/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6949 - accuracy: 0.7341 - val_loss: 0.7321 - val_accuracy: 0.7201\n",
            "Epoch 256/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6943 - accuracy: 0.7352 - val_loss: 0.7171 - val_accuracy: 0.7287\n",
            "Epoch 257/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6931 - accuracy: 0.7342 - val_loss: 0.7183 - val_accuracy: 0.7209\n",
            "Epoch 258/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6924 - accuracy: 0.7350 - val_loss: 0.7422 - val_accuracy: 0.7157\n",
            "Epoch 259/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6931 - accuracy: 0.7352 - val_loss: 0.7153 - val_accuracy: 0.7259\n",
            "Epoch 260/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6910 - accuracy: 0.7348 - val_loss: 0.7192 - val_accuracy: 0.7266\n",
            "Epoch 261/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6918 - accuracy: 0.7355 - val_loss: 0.7120 - val_accuracy: 0.7275\n",
            "Epoch 262/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6920 - accuracy: 0.7355 - val_loss: 0.7192 - val_accuracy: 0.7211\n",
            "Epoch 263/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6913 - accuracy: 0.7350 - val_loss: 0.7233 - val_accuracy: 0.7245\n",
            "Epoch 264/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6925 - accuracy: 0.7348 - val_loss: 0.7160 - val_accuracy: 0.7283\n",
            "Epoch 265/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6914 - accuracy: 0.7337 - val_loss: 0.7356 - val_accuracy: 0.7162\n",
            "Epoch 266/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6918 - accuracy: 0.7355 - val_loss: 0.7220 - val_accuracy: 0.7260\n",
            "Epoch 267/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6917 - accuracy: 0.7352 - val_loss: 0.7335 - val_accuracy: 0.7266\n",
            "Epoch 268/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6918 - accuracy: 0.7346 - val_loss: 0.7112 - val_accuracy: 0.7263\n",
            "Epoch 269/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6895 - accuracy: 0.7363 - val_loss: 0.7300 - val_accuracy: 0.7213\n",
            "Epoch 270/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6921 - accuracy: 0.7334 - val_loss: 0.7251 - val_accuracy: 0.7192\n",
            "Epoch 271/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6907 - accuracy: 0.7340 - val_loss: 0.7307 - val_accuracy: 0.7214\n",
            "Epoch 272/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6916 - accuracy: 0.7354 - val_loss: 0.7145 - val_accuracy: 0.7278\n",
            "Epoch 273/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6901 - accuracy: 0.7366 - val_loss: 0.7144 - val_accuracy: 0.7214\n",
            "Epoch 274/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6904 - accuracy: 0.7374 - val_loss: 0.7117 - val_accuracy: 0.7259\n",
            "Epoch 275/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6898 - accuracy: 0.7368 - val_loss: 0.7291 - val_accuracy: 0.7203\n",
            "Epoch 276/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6902 - accuracy: 0.7362 - val_loss: 0.7309 - val_accuracy: 0.7206\n",
            "Epoch 277/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6897 - accuracy: 0.7327 - val_loss: 0.7143 - val_accuracy: 0.7263\n",
            "Epoch 278/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6906 - accuracy: 0.7378 - val_loss: 0.7233 - val_accuracy: 0.7241\n",
            "Epoch 279/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6876 - accuracy: 0.7372 - val_loss: 0.7133 - val_accuracy: 0.7303\n",
            "Epoch 280/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6890 - accuracy: 0.7374 - val_loss: 0.7183 - val_accuracy: 0.7264\n",
            "Epoch 281/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6899 - accuracy: 0.7356 - val_loss: 0.7308 - val_accuracy: 0.7159\n",
            "Epoch 282/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6896 - accuracy: 0.7352 - val_loss: 0.7211 - val_accuracy: 0.7268\n",
            "Epoch 283/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6915 - accuracy: 0.7355 - val_loss: 0.7129 - val_accuracy: 0.7250\n",
            "Epoch 284/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6878 - accuracy: 0.7370 - val_loss: 0.7190 - val_accuracy: 0.7204\n",
            "Epoch 285/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6885 - accuracy: 0.7371 - val_loss: 0.7138 - val_accuracy: 0.7255\n",
            "Epoch 286/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6896 - accuracy: 0.7352 - val_loss: 0.7111 - val_accuracy: 0.7296\n",
            "Epoch 287/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6878 - accuracy: 0.7362 - val_loss: 0.7116 - val_accuracy: 0.7236\n",
            "Epoch 288/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6882 - accuracy: 0.7383 - val_loss: 0.7095 - val_accuracy: 0.7269\n",
            "Epoch 289/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6901 - accuracy: 0.7364 - val_loss: 0.7244 - val_accuracy: 0.7216\n",
            "Epoch 290/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6871 - accuracy: 0.7372 - val_loss: 0.7278 - val_accuracy: 0.7176\n",
            "Epoch 291/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6879 - accuracy: 0.7384 - val_loss: 0.7320 - val_accuracy: 0.7240\n",
            "Epoch 292/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6875 - accuracy: 0.7379 - val_loss: 0.7110 - val_accuracy: 0.7313\n",
            "Epoch 293/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6878 - accuracy: 0.7375 - val_loss: 0.7290 - val_accuracy: 0.7272\n",
            "Epoch 294/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6864 - accuracy: 0.7358 - val_loss: 0.7302 - val_accuracy: 0.7288\n",
            "Epoch 295/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6890 - accuracy: 0.7360 - val_loss: 0.7333 - val_accuracy: 0.7170\n",
            "Epoch 296/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6869 - accuracy: 0.7354 - val_loss: 0.7177 - val_accuracy: 0.7251\n",
            "Epoch 297/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6853 - accuracy: 0.7387 - val_loss: 0.7264 - val_accuracy: 0.7217\n",
            "Epoch 298/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6875 - accuracy: 0.7357 - val_loss: 0.7184 - val_accuracy: 0.7277\n",
            "Epoch 299/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6874 - accuracy: 0.7372 - val_loss: 0.7187 - val_accuracy: 0.7234\n",
            "Epoch 300/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6869 - accuracy: 0.7369 - val_loss: 0.7214 - val_accuracy: 0.7197\n",
            "Epoch 301/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6867 - accuracy: 0.7358 - val_loss: 0.7116 - val_accuracy: 0.7260\n",
            "Epoch 302/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6867 - accuracy: 0.7392 - val_loss: 0.7181 - val_accuracy: 0.7199\n",
            "Epoch 303/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6888 - accuracy: 0.7368 - val_loss: 0.7086 - val_accuracy: 0.7279\n",
            "Epoch 304/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6849 - accuracy: 0.7378 - val_loss: 0.7174 - val_accuracy: 0.7278\n",
            "Epoch 305/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6866 - accuracy: 0.7377 - val_loss: 0.7076 - val_accuracy: 0.7284\n",
            "Epoch 306/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6859 - accuracy: 0.7388 - val_loss: 0.7222 - val_accuracy: 0.7223\n",
            "Epoch 307/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6876 - accuracy: 0.7376 - val_loss: 0.7217 - val_accuracy: 0.7253\n",
            "Epoch 308/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6859 - accuracy: 0.7394 - val_loss: 0.7200 - val_accuracy: 0.7302\n",
            "Epoch 309/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6838 - accuracy: 0.7381 - val_loss: 0.7146 - val_accuracy: 0.7266\n",
            "Epoch 310/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6868 - accuracy: 0.7380 - val_loss: 0.7143 - val_accuracy: 0.7280\n",
            "Epoch 311/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6841 - accuracy: 0.7391 - val_loss: 0.7099 - val_accuracy: 0.7275\n",
            "Epoch 312/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6855 - accuracy: 0.7386 - val_loss: 0.7364 - val_accuracy: 0.7211\n",
            "Epoch 313/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6846 - accuracy: 0.7378 - val_loss: 0.7140 - val_accuracy: 0.7272\n",
            "Epoch 314/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6847 - accuracy: 0.7382 - val_loss: 0.7104 - val_accuracy: 0.7270\n",
            "Epoch 315/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6854 - accuracy: 0.7393 - val_loss: 0.7144 - val_accuracy: 0.7296\n",
            "Epoch 316/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6851 - accuracy: 0.7381 - val_loss: 0.7129 - val_accuracy: 0.7261\n",
            "Epoch 317/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6855 - accuracy: 0.7389 - val_loss: 0.7176 - val_accuracy: 0.7277\n",
            "Epoch 318/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6860 - accuracy: 0.7384 - val_loss: 0.7083 - val_accuracy: 0.7256\n",
            "Epoch 319/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6837 - accuracy: 0.7397 - val_loss: 0.7179 - val_accuracy: 0.7279\n",
            "Epoch 320/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6849 - accuracy: 0.7407 - val_loss: 0.7127 - val_accuracy: 0.7288\n",
            "Epoch 321/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6833 - accuracy: 0.7380 - val_loss: 0.7254 - val_accuracy: 0.7223\n",
            "Epoch 322/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6862 - accuracy: 0.7366 - val_loss: 0.7186 - val_accuracy: 0.7274\n",
            "Epoch 323/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6843 - accuracy: 0.7393 - val_loss: 0.7078 - val_accuracy: 0.7294\n",
            "Epoch 324/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6845 - accuracy: 0.7388 - val_loss: 0.7169 - val_accuracy: 0.7268\n",
            "Epoch 325/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6833 - accuracy: 0.7400 - val_loss: 0.7199 - val_accuracy: 0.7234\n",
            "Epoch 326/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6848 - accuracy: 0.7397 - val_loss: 0.7205 - val_accuracy: 0.7270\n",
            "Epoch 327/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6841 - accuracy: 0.7384 - val_loss: 0.7107 - val_accuracy: 0.7286\n",
            "Epoch 328/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.7388 - val_loss: 0.7278 - val_accuracy: 0.7206\n",
            "Epoch 329/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6856 - accuracy: 0.7392 - val_loss: 0.7094 - val_accuracy: 0.7324\n",
            "Epoch 330/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.7383 - val_loss: 0.7055 - val_accuracy: 0.7307\n",
            "Epoch 331/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6857 - accuracy: 0.7374 - val_loss: 0.7168 - val_accuracy: 0.7232\n",
            "Epoch 332/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6846 - accuracy: 0.7387 - val_loss: 0.7146 - val_accuracy: 0.7155\n",
            "Epoch 333/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6843 - accuracy: 0.7406 - val_loss: 0.7072 - val_accuracy: 0.7280\n",
            "Epoch 334/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.7417 - val_loss: 0.7218 - val_accuracy: 0.7230\n",
            "Epoch 335/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.7401 - val_loss: 0.7069 - val_accuracy: 0.7316\n",
            "Epoch 336/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6824 - accuracy: 0.7411 - val_loss: 0.7111 - val_accuracy: 0.7317\n",
            "Epoch 337/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6827 - accuracy: 0.7396 - val_loss: 0.7100 - val_accuracy: 0.7287\n",
            "Epoch 338/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.7396 - val_loss: 0.7143 - val_accuracy: 0.7308\n",
            "Epoch 339/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6810 - accuracy: 0.7414 - val_loss: 0.7117 - val_accuracy: 0.7261\n",
            "Epoch 340/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6815 - accuracy: 0.7400 - val_loss: 0.7110 - val_accuracy: 0.7269\n",
            "Epoch 341/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6823 - accuracy: 0.7405 - val_loss: 0.7184 - val_accuracy: 0.7254\n",
            "Epoch 342/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6847 - accuracy: 0.7391 - val_loss: 0.7139 - val_accuracy: 0.7241\n",
            "Epoch 343/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6840 - accuracy: 0.7379 - val_loss: 0.7169 - val_accuracy: 0.7269\n",
            "Epoch 344/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6811 - accuracy: 0.7392 - val_loss: 0.7116 - val_accuracy: 0.7288\n",
            "Epoch 345/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6825 - accuracy: 0.7387 - val_loss: 0.7116 - val_accuracy: 0.7303\n",
            "Epoch 346/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6821 - accuracy: 0.7370 - val_loss: 0.7101 - val_accuracy: 0.7316\n",
            "Epoch 347/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6805 - accuracy: 0.7410 - val_loss: 0.7196 - val_accuracy: 0.7240\n",
            "Epoch 348/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6801 - accuracy: 0.7417 - val_loss: 0.7218 - val_accuracy: 0.7245\n",
            "Epoch 349/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6826 - accuracy: 0.7403 - val_loss: 0.7058 - val_accuracy: 0.7310\n",
            "Epoch 350/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6805 - accuracy: 0.7397 - val_loss: 0.7199 - val_accuracy: 0.7259\n",
            "Epoch 351/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6846 - accuracy: 0.7394 - val_loss: 0.7151 - val_accuracy: 0.7299\n",
            "Epoch 352/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6812 - accuracy: 0.7400 - val_loss: 0.7114 - val_accuracy: 0.7254\n",
            "Epoch 353/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.7397 - val_loss: 0.7063 - val_accuracy: 0.7345\n",
            "Epoch 354/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6797 - accuracy: 0.7416 - val_loss: 0.7159 - val_accuracy: 0.7260\n",
            "Epoch 355/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6825 - accuracy: 0.7400 - val_loss: 0.7097 - val_accuracy: 0.7308\n",
            "Epoch 356/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6801 - accuracy: 0.7397 - val_loss: 0.7135 - val_accuracy: 0.7266\n",
            "Epoch 357/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6821 - accuracy: 0.7401 - val_loss: 0.7203 - val_accuracy: 0.7208\n",
            "Epoch 358/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6806 - accuracy: 0.7385 - val_loss: 0.7046 - val_accuracy: 0.7284\n",
            "Epoch 359/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6824 - accuracy: 0.7400 - val_loss: 0.7078 - val_accuracy: 0.7278\n",
            "Epoch 360/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6801 - accuracy: 0.7395 - val_loss: 0.7058 - val_accuracy: 0.7302\n",
            "Epoch 361/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.7414 - val_loss: 0.7144 - val_accuracy: 0.7324\n",
            "Epoch 362/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6816 - accuracy: 0.7411 - val_loss: 0.7075 - val_accuracy: 0.7272\n",
            "Epoch 363/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6796 - accuracy: 0.7410 - val_loss: 0.7316 - val_accuracy: 0.7263\n",
            "Epoch 364/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.7389 - val_loss: 0.7112 - val_accuracy: 0.7315\n",
            "Epoch 365/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6801 - accuracy: 0.7414 - val_loss: 0.7181 - val_accuracy: 0.7266\n",
            "Epoch 366/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6827 - accuracy: 0.7413 - val_loss: 0.7445 - val_accuracy: 0.7142\n",
            "Epoch 367/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6812 - accuracy: 0.7416 - val_loss: 0.7142 - val_accuracy: 0.7317\n",
            "Epoch 368/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6813 - accuracy: 0.7409 - val_loss: 0.7120 - val_accuracy: 0.7263\n",
            "Epoch 369/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6807 - accuracy: 0.7383 - val_loss: 0.7033 - val_accuracy: 0.7297\n",
            "Epoch 370/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6795 - accuracy: 0.7392 - val_loss: 0.7156 - val_accuracy: 0.7246\n",
            "Epoch 371/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6778 - accuracy: 0.7419 - val_loss: 0.7300 - val_accuracy: 0.7222\n",
            "Epoch 372/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6787 - accuracy: 0.7410 - val_loss: 0.7160 - val_accuracy: 0.7291\n",
            "Epoch 373/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6812 - accuracy: 0.7401 - val_loss: 0.7325 - val_accuracy: 0.7242\n",
            "Epoch 374/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6797 - accuracy: 0.7398 - val_loss: 0.7074 - val_accuracy: 0.7303\n",
            "Epoch 375/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6783 - accuracy: 0.7422 - val_loss: 0.7167 - val_accuracy: 0.7209\n",
            "Epoch 376/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6811 - accuracy: 0.7398 - val_loss: 0.7132 - val_accuracy: 0.7305\n",
            "Epoch 377/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6780 - accuracy: 0.7417 - val_loss: 0.7071 - val_accuracy: 0.7293\n",
            "Epoch 378/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.7392 - val_loss: 0.7049 - val_accuracy: 0.7313\n",
            "Epoch 379/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.7406 - val_loss: 0.7156 - val_accuracy: 0.7245\n",
            "Epoch 380/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6786 - accuracy: 0.7422 - val_loss: 0.7095 - val_accuracy: 0.7316\n",
            "Epoch 381/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6805 - accuracy: 0.7411 - val_loss: 0.7500 - val_accuracy: 0.7231\n",
            "Epoch 382/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6784 - accuracy: 0.7427 - val_loss: 0.7057 - val_accuracy: 0.7308\n",
            "Epoch 383/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6782 - accuracy: 0.7406 - val_loss: 0.7054 - val_accuracy: 0.7263\n",
            "Epoch 384/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6788 - accuracy: 0.7398 - val_loss: 0.7204 - val_accuracy: 0.7270\n",
            "Epoch 385/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6797 - accuracy: 0.7410 - val_loss: 0.7261 - val_accuracy: 0.7279\n",
            "Epoch 386/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6769 - accuracy: 0.7427 - val_loss: 0.7076 - val_accuracy: 0.7275\n",
            "Epoch 387/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6778 - accuracy: 0.7412 - val_loss: 0.7521 - val_accuracy: 0.7199\n",
            "Epoch 388/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6792 - accuracy: 0.7411 - val_loss: 0.7062 - val_accuracy: 0.7321\n",
            "Epoch 389/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6788 - accuracy: 0.7394 - val_loss: 0.7190 - val_accuracy: 0.7227\n",
            "Epoch 390/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6774 - accuracy: 0.7403 - val_loss: 0.7326 - val_accuracy: 0.7286\n",
            "Epoch 391/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6795 - accuracy: 0.7432 - val_loss: 0.7130 - val_accuracy: 0.7308\n",
            "Epoch 392/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6774 - accuracy: 0.7417 - val_loss: 0.7159 - val_accuracy: 0.7310\n",
            "Epoch 393/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6804 - accuracy: 0.7405 - val_loss: 0.7157 - val_accuracy: 0.7263\n",
            "Epoch 394/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6810 - accuracy: 0.7395 - val_loss: 0.7213 - val_accuracy: 0.7179\n",
            "Epoch 395/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6774 - accuracy: 0.7423 - val_loss: 0.7141 - val_accuracy: 0.7310\n",
            "Epoch 396/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6773 - accuracy: 0.7411 - val_loss: 0.7028 - val_accuracy: 0.7305\n",
            "Epoch 397/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6805 - accuracy: 0.7389 - val_loss: 0.7136 - val_accuracy: 0.7259\n",
            "Epoch 398/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6764 - accuracy: 0.7411 - val_loss: 0.7062 - val_accuracy: 0.7305\n",
            "Epoch 399/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6758 - accuracy: 0.7425 - val_loss: 0.7113 - val_accuracy: 0.7287\n",
            "Epoch 400/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6788 - accuracy: 0.7415 - val_loss: 0.7526 - val_accuracy: 0.7127\n",
            "Epoch 401/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6772 - accuracy: 0.7408 - val_loss: 0.7069 - val_accuracy: 0.7343\n",
            "Epoch 402/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6778 - accuracy: 0.7415 - val_loss: 0.7198 - val_accuracy: 0.7198\n",
            "Epoch 403/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6772 - accuracy: 0.7408 - val_loss: 0.7200 - val_accuracy: 0.7234\n",
            "Epoch 404/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6775 - accuracy: 0.7402 - val_loss: 0.7018 - val_accuracy: 0.7326\n",
            "Epoch 405/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6751 - accuracy: 0.7431 - val_loss: 0.7102 - val_accuracy: 0.7313\n",
            "Epoch 406/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6801 - accuracy: 0.7411 - val_loss: 0.7430 - val_accuracy: 0.7161\n",
            "Epoch 407/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6765 - accuracy: 0.7430 - val_loss: 0.6999 - val_accuracy: 0.7334\n",
            "Epoch 408/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6758 - accuracy: 0.7417 - val_loss: 0.7135 - val_accuracy: 0.7301\n",
            "Epoch 409/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6781 - accuracy: 0.7428 - val_loss: 0.7114 - val_accuracy: 0.7326\n",
            "Epoch 410/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6773 - accuracy: 0.7423 - val_loss: 0.7095 - val_accuracy: 0.7244\n",
            "Epoch 411/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6765 - accuracy: 0.7424 - val_loss: 0.7155 - val_accuracy: 0.7254\n",
            "Epoch 412/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6780 - accuracy: 0.7419 - val_loss: 0.7051 - val_accuracy: 0.7312\n",
            "Epoch 413/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6766 - accuracy: 0.7407 - val_loss: 0.7121 - val_accuracy: 0.7310\n",
            "Epoch 414/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6775 - accuracy: 0.7431 - val_loss: 0.7026 - val_accuracy: 0.7306\n",
            "Epoch 415/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6759 - accuracy: 0.7443 - val_loss: 0.7217 - val_accuracy: 0.7253\n",
            "Epoch 416/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6751 - accuracy: 0.7417 - val_loss: 0.7125 - val_accuracy: 0.7204\n",
            "Epoch 417/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6749 - accuracy: 0.7434 - val_loss: 0.7117 - val_accuracy: 0.7283\n",
            "Epoch 418/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6762 - accuracy: 0.7442 - val_loss: 0.7035 - val_accuracy: 0.7339\n",
            "Epoch 419/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6741 - accuracy: 0.7423 - val_loss: 0.7106 - val_accuracy: 0.7332\n",
            "Epoch 420/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6778 - accuracy: 0.7423 - val_loss: 0.7279 - val_accuracy: 0.7235\n",
            "Epoch 421/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6787 - accuracy: 0.7406 - val_loss: 0.7093 - val_accuracy: 0.7311\n",
            "Epoch 422/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6779 - accuracy: 0.7428 - val_loss: 0.7144 - val_accuracy: 0.7299\n",
            "Epoch 423/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6763 - accuracy: 0.7431 - val_loss: 0.7153 - val_accuracy: 0.7273\n",
            "Epoch 424/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6777 - accuracy: 0.7433 - val_loss: 0.7016 - val_accuracy: 0.7350\n",
            "Epoch 425/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6757 - accuracy: 0.7427 - val_loss: 0.7049 - val_accuracy: 0.7317\n",
            "Epoch 426/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6730 - accuracy: 0.7436 - val_loss: 0.7177 - val_accuracy: 0.7315\n",
            "Epoch 427/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6753 - accuracy: 0.7436 - val_loss: 0.7296 - val_accuracy: 0.7260\n",
            "Epoch 428/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6763 - accuracy: 0.7425 - val_loss: 0.7180 - val_accuracy: 0.7222\n",
            "Epoch 429/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6749 - accuracy: 0.7419 - val_loss: 0.7089 - val_accuracy: 0.7320\n",
            "Epoch 430/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6742 - accuracy: 0.7444 - val_loss: 0.7064 - val_accuracy: 0.7263\n",
            "Epoch 431/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6750 - accuracy: 0.7437 - val_loss: 0.7189 - val_accuracy: 0.7230\n",
            "Epoch 432/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6745 - accuracy: 0.7431 - val_loss: 0.7188 - val_accuracy: 0.7264\n",
            "Epoch 433/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6741 - accuracy: 0.7428 - val_loss: 0.7020 - val_accuracy: 0.7308\n",
            "Epoch 434/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6752 - accuracy: 0.7410 - val_loss: 0.6994 - val_accuracy: 0.7338\n",
            "Epoch 435/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6764 - accuracy: 0.7422 - val_loss: 0.7146 - val_accuracy: 0.7250\n",
            "Epoch 436/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6729 - accuracy: 0.7426 - val_loss: 0.7096 - val_accuracy: 0.7297\n",
            "Epoch 437/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6760 - accuracy: 0.7449 - val_loss: 0.7046 - val_accuracy: 0.7339\n",
            "Epoch 438/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6727 - accuracy: 0.7433 - val_loss: 0.7013 - val_accuracy: 0.7324\n",
            "Epoch 439/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6750 - accuracy: 0.7425 - val_loss: 0.7001 - val_accuracy: 0.7362\n",
            "Epoch 440/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6745 - accuracy: 0.7431 - val_loss: 0.7067 - val_accuracy: 0.7280\n",
            "Epoch 441/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6759 - accuracy: 0.7424 - val_loss: 0.7236 - val_accuracy: 0.7230\n",
            "Epoch 442/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6749 - accuracy: 0.7430 - val_loss: 0.7393 - val_accuracy: 0.7230\n",
            "Epoch 443/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6767 - accuracy: 0.7423 - val_loss: 0.6993 - val_accuracy: 0.7330\n",
            "Epoch 444/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6737 - accuracy: 0.7433 - val_loss: 0.6995 - val_accuracy: 0.7354\n",
            "Epoch 445/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6754 - accuracy: 0.7426 - val_loss: 0.7140 - val_accuracy: 0.7258\n",
            "Epoch 446/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6743 - accuracy: 0.7424 - val_loss: 0.7029 - val_accuracy: 0.7336\n",
            "Epoch 447/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6716 - accuracy: 0.7443 - val_loss: 0.7087 - val_accuracy: 0.7297\n",
            "Epoch 448/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6761 - accuracy: 0.7426 - val_loss: 0.7026 - val_accuracy: 0.7319\n",
            "Epoch 449/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6738 - accuracy: 0.7437 - val_loss: 0.7367 - val_accuracy: 0.7242\n",
            "Epoch 450/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6725 - accuracy: 0.7442 - val_loss: 0.7115 - val_accuracy: 0.7289\n",
            "Epoch 451/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6745 - accuracy: 0.7437 - val_loss: 0.6986 - val_accuracy: 0.7349\n",
            "Epoch 452/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6747 - accuracy: 0.7430 - val_loss: 0.7056 - val_accuracy: 0.7332\n",
            "Epoch 453/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6750 - accuracy: 0.7435 - val_loss: 0.7350 - val_accuracy: 0.7150\n",
            "Epoch 454/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6723 - accuracy: 0.7457 - val_loss: 0.7037 - val_accuracy: 0.7305\n",
            "Epoch 455/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6739 - accuracy: 0.7419 - val_loss: 0.7081 - val_accuracy: 0.7292\n",
            "Epoch 456/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6743 - accuracy: 0.7452 - val_loss: 0.7282 - val_accuracy: 0.7273\n",
            "Epoch 457/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6735 - accuracy: 0.7422 - val_loss: 0.7126 - val_accuracy: 0.7278\n",
            "Epoch 458/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6768 - accuracy: 0.7421 - val_loss: 0.7030 - val_accuracy: 0.7332\n",
            "Epoch 459/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6723 - accuracy: 0.7444 - val_loss: 0.7045 - val_accuracy: 0.7316\n",
            "Epoch 460/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6732 - accuracy: 0.7440 - val_loss: 0.7164 - val_accuracy: 0.7273\n",
            "Epoch 461/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6727 - accuracy: 0.7434 - val_loss: 0.7027 - val_accuracy: 0.7348\n",
            "Epoch 462/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6741 - accuracy: 0.7435 - val_loss: 0.7125 - val_accuracy: 0.7278\n",
            "Epoch 463/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6741 - accuracy: 0.7436 - val_loss: 0.7147 - val_accuracy: 0.7251\n",
            "Epoch 464/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6737 - accuracy: 0.7445 - val_loss: 0.7020 - val_accuracy: 0.7296\n",
            "Epoch 465/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6730 - accuracy: 0.7437 - val_loss: 0.7137 - val_accuracy: 0.7331\n",
            "Epoch 466/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6711 - accuracy: 0.7444 - val_loss: 0.7064 - val_accuracy: 0.7305\n",
            "Epoch 467/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6738 - accuracy: 0.7435 - val_loss: 0.7010 - val_accuracy: 0.7348\n",
            "Epoch 468/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6725 - accuracy: 0.7440 - val_loss: 0.6986 - val_accuracy: 0.7349\n",
            "Epoch 469/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6762 - accuracy: 0.7413 - val_loss: 0.7007 - val_accuracy: 0.7355\n",
            "Epoch 470/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6721 - accuracy: 0.7444 - val_loss: 0.7127 - val_accuracy: 0.7275\n",
            "Epoch 471/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6719 - accuracy: 0.7429 - val_loss: 0.7075 - val_accuracy: 0.7330\n",
            "Epoch 472/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6717 - accuracy: 0.7444 - val_loss: 0.7008 - val_accuracy: 0.7348\n",
            "Epoch 473/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6710 - accuracy: 0.7439 - val_loss: 0.7199 - val_accuracy: 0.7299\n",
            "Epoch 474/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6716 - accuracy: 0.7439 - val_loss: 0.7334 - val_accuracy: 0.7260\n",
            "Epoch 475/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6726 - accuracy: 0.7443 - val_loss: 0.7078 - val_accuracy: 0.7278\n",
            "Epoch 476/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6705 - accuracy: 0.7448 - val_loss: 0.7315 - val_accuracy: 0.7234\n",
            "Epoch 477/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6739 - accuracy: 0.7424 - val_loss: 0.7039 - val_accuracy: 0.7343\n",
            "Epoch 478/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6712 - accuracy: 0.7442 - val_loss: 0.7206 - val_accuracy: 0.7301\n",
            "Epoch 479/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6738 - accuracy: 0.7435 - val_loss: 0.7814 - val_accuracy: 0.7170\n",
            "Epoch 480/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6716 - accuracy: 0.7446 - val_loss: 0.7002 - val_accuracy: 0.7324\n",
            "Epoch 481/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6725 - accuracy: 0.7446 - val_loss: 0.7098 - val_accuracy: 0.7302\n",
            "Epoch 482/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6739 - accuracy: 0.7437 - val_loss: 0.7406 - val_accuracy: 0.7185\n",
            "Epoch 483/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6739 - accuracy: 0.7441 - val_loss: 0.7018 - val_accuracy: 0.7316\n",
            "Epoch 484/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6731 - accuracy: 0.7438 - val_loss: 0.7360 - val_accuracy: 0.7242\n",
            "Epoch 485/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6708 - accuracy: 0.7428 - val_loss: 0.6978 - val_accuracy: 0.7350\n",
            "Epoch 486/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6701 - accuracy: 0.7438 - val_loss: 0.7593 - val_accuracy: 0.7102\n",
            "Epoch 487/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6740 - accuracy: 0.7440 - val_loss: 0.7068 - val_accuracy: 0.7287\n",
            "Epoch 488/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6717 - accuracy: 0.7464 - val_loss: 0.7014 - val_accuracy: 0.7334\n",
            "Epoch 489/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6707 - accuracy: 0.7458 - val_loss: 0.6984 - val_accuracy: 0.7345\n",
            "Epoch 490/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6715 - accuracy: 0.7446 - val_loss: 0.7025 - val_accuracy: 0.7326\n",
            "Epoch 491/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6712 - accuracy: 0.7436 - val_loss: 0.7194 - val_accuracy: 0.7288\n",
            "Epoch 492/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6692 - accuracy: 0.7452 - val_loss: 0.7005 - val_accuracy: 0.7359\n",
            "Epoch 493/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6726 - accuracy: 0.7448 - val_loss: 0.6997 - val_accuracy: 0.7320\n",
            "Epoch 494/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6728 - accuracy: 0.7427 - val_loss: 0.6985 - val_accuracy: 0.7315\n",
            "Epoch 495/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6692 - accuracy: 0.7460 - val_loss: 0.6972 - val_accuracy: 0.7335\n",
            "Epoch 496/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6724 - accuracy: 0.7430 - val_loss: 0.7224 - val_accuracy: 0.7222\n",
            "Epoch 497/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6744 - accuracy: 0.7424 - val_loss: 0.6969 - val_accuracy: 0.7364\n",
            "Epoch 498/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6702 - accuracy: 0.7451 - val_loss: 0.7011 - val_accuracy: 0.7315\n",
            "Epoch 499/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6701 - accuracy: 0.7449 - val_loss: 0.7083 - val_accuracy: 0.7279\n",
            "Epoch 500/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6720 - accuracy: 0.7460 - val_loss: 0.7096 - val_accuracy: 0.7277\n",
            "Epoch 501/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6701 - accuracy: 0.7439 - val_loss: 0.7012 - val_accuracy: 0.7334\n",
            "Epoch 502/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6688 - accuracy: 0.7455 - val_loss: 0.6958 - val_accuracy: 0.7371\n",
            "Epoch 503/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6727 - accuracy: 0.7437 - val_loss: 0.6968 - val_accuracy: 0.7372\n",
            "Epoch 504/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6707 - accuracy: 0.7469 - val_loss: 0.7183 - val_accuracy: 0.7344\n",
            "Epoch 505/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6723 - accuracy: 0.7444 - val_loss: 0.7135 - val_accuracy: 0.7310\n",
            "Epoch 506/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6686 - accuracy: 0.7469 - val_loss: 0.7110 - val_accuracy: 0.7277\n",
            "Epoch 507/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6753 - accuracy: 0.7435 - val_loss: 0.7060 - val_accuracy: 0.7327\n",
            "Epoch 508/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6709 - accuracy: 0.7441 - val_loss: 0.7019 - val_accuracy: 0.7324\n",
            "Epoch 509/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6693 - accuracy: 0.7448 - val_loss: 0.7124 - val_accuracy: 0.7317\n",
            "Epoch 510/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6711 - accuracy: 0.7454 - val_loss: 0.7000 - val_accuracy: 0.7319\n",
            "Epoch 511/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6689 - accuracy: 0.7462 - val_loss: 0.6971 - val_accuracy: 0.7381\n",
            "Epoch 512/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6695 - accuracy: 0.7458 - val_loss: 0.7327 - val_accuracy: 0.7189\n",
            "Epoch 513/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6711 - accuracy: 0.7440 - val_loss: 0.7097 - val_accuracy: 0.7306\n",
            "Epoch 514/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6723 - accuracy: 0.7450 - val_loss: 0.7009 - val_accuracy: 0.7348\n",
            "Epoch 515/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6680 - accuracy: 0.7460 - val_loss: 0.6967 - val_accuracy: 0.7352\n",
            "Epoch 516/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6714 - accuracy: 0.7444 - val_loss: 0.6996 - val_accuracy: 0.7305\n",
            "Epoch 517/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6718 - accuracy: 0.7439 - val_loss: 0.7084 - val_accuracy: 0.7326\n",
            "Epoch 518/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6689 - accuracy: 0.7467 - val_loss: 0.7070 - val_accuracy: 0.7330\n",
            "Epoch 519/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6689 - accuracy: 0.7460 - val_loss: 0.6997 - val_accuracy: 0.7319\n",
            "Epoch 520/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6684 - accuracy: 0.7455 - val_loss: 0.6956 - val_accuracy: 0.7353\n",
            "Epoch 521/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6710 - accuracy: 0.7458 - val_loss: 0.7205 - val_accuracy: 0.7260\n",
            "Epoch 522/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6683 - accuracy: 0.7460 - val_loss: 0.6981 - val_accuracy: 0.7336\n",
            "Epoch 523/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6698 - accuracy: 0.7460 - val_loss: 0.6966 - val_accuracy: 0.7336\n",
            "Epoch 524/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6693 - accuracy: 0.7471 - val_loss: 0.7068 - val_accuracy: 0.7278\n",
            "Epoch 525/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6727 - accuracy: 0.7444 - val_loss: 0.6948 - val_accuracy: 0.7383\n",
            "Epoch 526/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6703 - accuracy: 0.7457 - val_loss: 0.7033 - val_accuracy: 0.7374\n",
            "Epoch 527/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6687 - accuracy: 0.7448 - val_loss: 0.6964 - val_accuracy: 0.7325\n",
            "Epoch 528/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6703 - accuracy: 0.7448 - val_loss: 0.7010 - val_accuracy: 0.7284\n",
            "Epoch 529/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6710 - accuracy: 0.7453 - val_loss: 0.6982 - val_accuracy: 0.7336\n",
            "Epoch 530/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6709 - accuracy: 0.7442 - val_loss: 0.7200 - val_accuracy: 0.7264\n",
            "Epoch 531/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6689 - accuracy: 0.7457 - val_loss: 0.6957 - val_accuracy: 0.7368\n",
            "Epoch 532/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6690 - accuracy: 0.7473 - val_loss: 0.7084 - val_accuracy: 0.7332\n",
            "Epoch 533/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6681 - accuracy: 0.7452 - val_loss: 0.6999 - val_accuracy: 0.7301\n",
            "Epoch 534/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6691 - accuracy: 0.7459 - val_loss: 0.7131 - val_accuracy: 0.7283\n",
            "Epoch 535/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6703 - accuracy: 0.7470 - val_loss: 0.6975 - val_accuracy: 0.7365\n",
            "Epoch 536/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6711 - accuracy: 0.7453 - val_loss: 0.7098 - val_accuracy: 0.7324\n",
            "Epoch 537/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6676 - accuracy: 0.7470 - val_loss: 0.7070 - val_accuracy: 0.7335\n",
            "Epoch 538/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6688 - accuracy: 0.7476 - val_loss: 0.7028 - val_accuracy: 0.7321\n",
            "Epoch 539/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6680 - accuracy: 0.7459 - val_loss: 0.6953 - val_accuracy: 0.7376\n",
            "Epoch 540/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6704 - accuracy: 0.7457 - val_loss: 0.7000 - val_accuracy: 0.7343\n",
            "Epoch 541/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6686 - accuracy: 0.7460 - val_loss: 0.7002 - val_accuracy: 0.7339\n",
            "Epoch 542/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6679 - accuracy: 0.7465 - val_loss: 0.6947 - val_accuracy: 0.7405\n",
            "Epoch 543/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6718 - accuracy: 0.7445 - val_loss: 0.7185 - val_accuracy: 0.7298\n",
            "Epoch 544/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6689 - accuracy: 0.7446 - val_loss: 0.7280 - val_accuracy: 0.7316\n",
            "Epoch 545/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6696 - accuracy: 0.7467 - val_loss: 0.7102 - val_accuracy: 0.7289\n",
            "Epoch 546/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6671 - accuracy: 0.7470 - val_loss: 0.6995 - val_accuracy: 0.7360\n",
            "Epoch 547/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6675 - accuracy: 0.7468 - val_loss: 0.7202 - val_accuracy: 0.7277\n",
            "Epoch 548/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6685 - accuracy: 0.7470 - val_loss: 0.7029 - val_accuracy: 0.7344\n",
            "Epoch 549/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6675 - accuracy: 0.7462 - val_loss: 0.7046 - val_accuracy: 0.7302\n",
            "Epoch 550/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6684 - accuracy: 0.7465 - val_loss: 0.7036 - val_accuracy: 0.7349\n",
            "Epoch 551/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6690 - accuracy: 0.7478 - val_loss: 0.7106 - val_accuracy: 0.7345\n",
            "Epoch 552/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6691 - accuracy: 0.7475 - val_loss: 0.6967 - val_accuracy: 0.7340\n",
            "Epoch 553/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6691 - accuracy: 0.7452 - val_loss: 0.6941 - val_accuracy: 0.7395\n",
            "Epoch 554/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6696 - accuracy: 0.7460 - val_loss: 0.7283 - val_accuracy: 0.7307\n",
            "Epoch 555/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6676 - accuracy: 0.7464 - val_loss: 0.6980 - val_accuracy: 0.7354\n",
            "Epoch 556/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6680 - accuracy: 0.7476 - val_loss: 0.7236 - val_accuracy: 0.7279\n",
            "Epoch 557/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6688 - accuracy: 0.7453 - val_loss: 0.7084 - val_accuracy: 0.7311\n",
            "Epoch 558/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6695 - accuracy: 0.7459 - val_loss: 0.7007 - val_accuracy: 0.7350\n",
            "Epoch 559/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6658 - accuracy: 0.7492 - val_loss: 0.6949 - val_accuracy: 0.7400\n",
            "Epoch 560/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6703 - accuracy: 0.7467 - val_loss: 0.6950 - val_accuracy: 0.7383\n",
            "Epoch 561/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6673 - accuracy: 0.7469 - val_loss: 0.7015 - val_accuracy: 0.7344\n",
            "Epoch 562/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6688 - accuracy: 0.7460 - val_loss: 0.7035 - val_accuracy: 0.7340\n",
            "Epoch 563/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6676 - accuracy: 0.7461 - val_loss: 0.7085 - val_accuracy: 0.7348\n",
            "Epoch 564/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6682 - accuracy: 0.7460 - val_loss: 0.6984 - val_accuracy: 0.7334\n",
            "Epoch 565/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6698 - accuracy: 0.7465 - val_loss: 0.7052 - val_accuracy: 0.7325\n",
            "Epoch 566/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6664 - accuracy: 0.7467 - val_loss: 0.7090 - val_accuracy: 0.7313\n",
            "Epoch 567/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6673 - accuracy: 0.7477 - val_loss: 0.7257 - val_accuracy: 0.7264\n",
            "Epoch 568/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6658 - accuracy: 0.7463 - val_loss: 0.6953 - val_accuracy: 0.7343\n",
            "Epoch 569/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6683 - accuracy: 0.7450 - val_loss: 0.7589 - val_accuracy: 0.7147\n",
            "Epoch 570/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6656 - accuracy: 0.7465 - val_loss: 0.6993 - val_accuracy: 0.7386\n",
            "Epoch 571/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6673 - accuracy: 0.7462 - val_loss: 0.7072 - val_accuracy: 0.7316\n",
            "Epoch 572/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6660 - accuracy: 0.7462 - val_loss: 0.6975 - val_accuracy: 0.7382\n",
            "Epoch 573/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6682 - accuracy: 0.7470 - val_loss: 0.7038 - val_accuracy: 0.7263\n",
            "Epoch 574/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6675 - accuracy: 0.7477 - val_loss: 0.7093 - val_accuracy: 0.7358\n",
            "Epoch 575/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6679 - accuracy: 0.7473 - val_loss: 0.6983 - val_accuracy: 0.7365\n",
            "Epoch 576/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6665 - accuracy: 0.7470 - val_loss: 0.7313 - val_accuracy: 0.7293\n",
            "Epoch 577/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6676 - accuracy: 0.7459 - val_loss: 0.7058 - val_accuracy: 0.7322\n",
            "Epoch 578/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6681 - accuracy: 0.7479 - val_loss: 0.7103 - val_accuracy: 0.7326\n",
            "Epoch 579/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6674 - accuracy: 0.7478 - val_loss: 0.7156 - val_accuracy: 0.7249\n",
            "Epoch 580/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6684 - accuracy: 0.7450 - val_loss: 0.7155 - val_accuracy: 0.7292\n",
            "Epoch 581/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6666 - accuracy: 0.7495 - val_loss: 0.6944 - val_accuracy: 0.7357\n",
            "Epoch 582/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6670 - accuracy: 0.7465 - val_loss: 0.7061 - val_accuracy: 0.7282\n",
            "Epoch 583/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6670 - accuracy: 0.7492 - val_loss: 0.7147 - val_accuracy: 0.7263\n",
            "Epoch 584/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6669 - accuracy: 0.7470 - val_loss: 0.7024 - val_accuracy: 0.7282\n",
            "Epoch 585/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6681 - accuracy: 0.7479 - val_loss: 0.7084 - val_accuracy: 0.7292\n",
            "Epoch 586/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6695 - accuracy: 0.7464 - val_loss: 0.6931 - val_accuracy: 0.7377\n",
            "Epoch 587/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6663 - accuracy: 0.7480 - val_loss: 0.7049 - val_accuracy: 0.7349\n",
            "Epoch 588/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6662 - accuracy: 0.7487 - val_loss: 0.6981 - val_accuracy: 0.7363\n",
            "Epoch 589/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6679 - accuracy: 0.7459 - val_loss: 0.6991 - val_accuracy: 0.7346\n",
            "Epoch 590/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6650 - accuracy: 0.7463 - val_loss: 0.7102 - val_accuracy: 0.7326\n",
            "Epoch 591/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6673 - accuracy: 0.7463 - val_loss: 0.6931 - val_accuracy: 0.7388\n",
            "Epoch 592/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6669 - accuracy: 0.7474 - val_loss: 0.6926 - val_accuracy: 0.7369\n",
            "Epoch 593/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6682 - accuracy: 0.7467 - val_loss: 0.6942 - val_accuracy: 0.7387\n",
            "Epoch 594/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6686 - accuracy: 0.7474 - val_loss: 0.7244 - val_accuracy: 0.7254\n",
            "Epoch 595/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6657 - accuracy: 0.7480 - val_loss: 0.7042 - val_accuracy: 0.7287\n",
            "Epoch 596/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6635 - accuracy: 0.7481 - val_loss: 0.6952 - val_accuracy: 0.7345\n",
            "Epoch 597/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6679 - accuracy: 0.7481 - val_loss: 0.6945 - val_accuracy: 0.7392\n",
            "Epoch 598/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6666 - accuracy: 0.7480 - val_loss: 0.7266 - val_accuracy: 0.7277\n",
            "Epoch 599/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6706 - accuracy: 0.7443 - val_loss: 0.7026 - val_accuracy: 0.7312\n",
            "Epoch 600/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6667 - accuracy: 0.7471 - val_loss: 0.6963 - val_accuracy: 0.7360\n",
            "Epoch 601/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6653 - accuracy: 0.7454 - val_loss: 0.7049 - val_accuracy: 0.7344\n",
            "Epoch 602/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6659 - accuracy: 0.7483 - val_loss: 0.7196 - val_accuracy: 0.7279\n",
            "Epoch 603/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6669 - accuracy: 0.7472 - val_loss: 0.7091 - val_accuracy: 0.7321\n",
            "Epoch 604/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6669 - accuracy: 0.7470 - val_loss: 0.6969 - val_accuracy: 0.7284\n",
            "Epoch 605/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6649 - accuracy: 0.7478 - val_loss: 0.6978 - val_accuracy: 0.7355\n",
            "Epoch 606/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6649 - accuracy: 0.7472 - val_loss: 0.7211 - val_accuracy: 0.7209\n",
            "Epoch 607/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6660 - accuracy: 0.7490 - val_loss: 0.7240 - val_accuracy: 0.7275\n",
            "Epoch 608/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6634 - accuracy: 0.7487 - val_loss: 0.6965 - val_accuracy: 0.7353\n",
            "Epoch 609/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6656 - accuracy: 0.7482 - val_loss: 0.6958 - val_accuracy: 0.7371\n",
            "Epoch 610/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6672 - accuracy: 0.7454 - val_loss: 0.7188 - val_accuracy: 0.7335\n",
            "Epoch 611/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6651 - accuracy: 0.7472 - val_loss: 0.6982 - val_accuracy: 0.7362\n",
            "Epoch 612/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6656 - accuracy: 0.7486 - val_loss: 0.7042 - val_accuracy: 0.7343\n",
            "Epoch 613/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6665 - accuracy: 0.7485 - val_loss: 0.6929 - val_accuracy: 0.7401\n",
            "Epoch 614/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6645 - accuracy: 0.7470 - val_loss: 0.7065 - val_accuracy: 0.7330\n",
            "Epoch 615/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6677 - accuracy: 0.7455 - val_loss: 0.6963 - val_accuracy: 0.7379\n",
            "Epoch 616/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6649 - accuracy: 0.7473 - val_loss: 0.6933 - val_accuracy: 0.7381\n",
            "Epoch 617/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6669 - accuracy: 0.7464 - val_loss: 0.7080 - val_accuracy: 0.7289\n",
            "Epoch 618/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6654 - accuracy: 0.7482 - val_loss: 0.6926 - val_accuracy: 0.7396\n",
            "Epoch 619/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6647 - accuracy: 0.7469 - val_loss: 0.7231 - val_accuracy: 0.7311\n",
            "Epoch 620/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6657 - accuracy: 0.7465 - val_loss: 0.6982 - val_accuracy: 0.7373\n",
            "Epoch 621/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6646 - accuracy: 0.7480 - val_loss: 0.7067 - val_accuracy: 0.7325\n",
            "Epoch 622/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6646 - accuracy: 0.7473 - val_loss: 0.6986 - val_accuracy: 0.7373\n",
            "Epoch 623/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6656 - accuracy: 0.7476 - val_loss: 0.7085 - val_accuracy: 0.7298\n",
            "Epoch 624/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6662 - accuracy: 0.7467 - val_loss: 0.6969 - val_accuracy: 0.7350\n",
            "Epoch 625/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6661 - accuracy: 0.7463 - val_loss: 0.7072 - val_accuracy: 0.7374\n",
            "Epoch 626/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6658 - accuracy: 0.7469 - val_loss: 0.6935 - val_accuracy: 0.7377\n",
            "Epoch 627/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6641 - accuracy: 0.7495 - val_loss: 0.6952 - val_accuracy: 0.7363\n",
            "Epoch 628/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6667 - accuracy: 0.7465 - val_loss: 0.7051 - val_accuracy: 0.7350\n",
            "Epoch 629/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6654 - accuracy: 0.7486 - val_loss: 0.6978 - val_accuracy: 0.7362\n",
            "Epoch 630/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6660 - accuracy: 0.7464 - val_loss: 0.6944 - val_accuracy: 0.7381\n",
            "Epoch 631/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6641 - accuracy: 0.7474 - val_loss: 0.7313 - val_accuracy: 0.7259\n",
            "Epoch 632/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6654 - accuracy: 0.7464 - val_loss: 0.7240 - val_accuracy: 0.7312\n",
            "Epoch 633/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6665 - accuracy: 0.7468 - val_loss: 0.6948 - val_accuracy: 0.7330\n",
            "Epoch 634/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6659 - accuracy: 0.7472 - val_loss: 0.7017 - val_accuracy: 0.7311\n",
            "Epoch 635/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6645 - accuracy: 0.7477 - val_loss: 0.6957 - val_accuracy: 0.7368\n",
            "Epoch 636/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6660 - accuracy: 0.7465 - val_loss: 0.6982 - val_accuracy: 0.7321\n",
            "Epoch 637/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6684 - accuracy: 0.7454 - val_loss: 0.6987 - val_accuracy: 0.7311\n",
            "Epoch 638/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6649 - accuracy: 0.7468 - val_loss: 0.7136 - val_accuracy: 0.7341\n",
            "Epoch 639/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6657 - accuracy: 0.7478 - val_loss: 0.6994 - val_accuracy: 0.7297\n",
            "Epoch 640/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6645 - accuracy: 0.7491 - val_loss: 0.6963 - val_accuracy: 0.7358\n",
            "Epoch 641/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6650 - accuracy: 0.7488 - val_loss: 0.6955 - val_accuracy: 0.7341\n",
            "Epoch 642/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6639 - accuracy: 0.7483 - val_loss: 0.7005 - val_accuracy: 0.7343\n",
            "Epoch 643/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6641 - accuracy: 0.7486 - val_loss: 0.6955 - val_accuracy: 0.7326\n",
            "Epoch 644/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6649 - accuracy: 0.7481 - val_loss: 0.7057 - val_accuracy: 0.7279\n",
            "Epoch 645/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6651 - accuracy: 0.7481 - val_loss: 0.7171 - val_accuracy: 0.7232\n",
            "Epoch 646/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6670 - accuracy: 0.7489 - val_loss: 0.6979 - val_accuracy: 0.7338\n",
            "Epoch 647/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6651 - accuracy: 0.7457 - val_loss: 0.7014 - val_accuracy: 0.7332\n",
            "Epoch 648/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6663 - accuracy: 0.7489 - val_loss: 0.6965 - val_accuracy: 0.7379\n",
            "Epoch 649/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6628 - accuracy: 0.7488 - val_loss: 0.7086 - val_accuracy: 0.7320\n",
            "Epoch 650/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6651 - accuracy: 0.7499 - val_loss: 0.6943 - val_accuracy: 0.7390\n",
            "Epoch 651/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6639 - accuracy: 0.7483 - val_loss: 0.6924 - val_accuracy: 0.7363\n",
            "Epoch 652/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6650 - accuracy: 0.7469 - val_loss: 0.7122 - val_accuracy: 0.7239\n",
            "Epoch 653/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6670 - accuracy: 0.7470 - val_loss: 0.6996 - val_accuracy: 0.7365\n",
            "Epoch 654/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6624 - accuracy: 0.7480 - val_loss: 0.6966 - val_accuracy: 0.7331\n",
            "Epoch 655/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6655 - accuracy: 0.7489 - val_loss: 0.6982 - val_accuracy: 0.7345\n",
            "Epoch 656/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6649 - accuracy: 0.7483 - val_loss: 0.7188 - val_accuracy: 0.7340\n",
            "Epoch 657/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6633 - accuracy: 0.7476 - val_loss: 0.6922 - val_accuracy: 0.7387\n",
            "Epoch 658/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6650 - accuracy: 0.7472 - val_loss: 0.7118 - val_accuracy: 0.7294\n",
            "Epoch 659/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6655 - accuracy: 0.7485 - val_loss: 0.6960 - val_accuracy: 0.7359\n",
            "Epoch 660/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6627 - accuracy: 0.7503 - val_loss: 0.6961 - val_accuracy: 0.7398\n",
            "Epoch 661/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6628 - accuracy: 0.7487 - val_loss: 0.7035 - val_accuracy: 0.7331\n",
            "Epoch 662/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6644 - accuracy: 0.7466 - val_loss: 0.7044 - val_accuracy: 0.7344\n",
            "Epoch 663/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6668 - accuracy: 0.7474 - val_loss: 0.7026 - val_accuracy: 0.7363\n",
            "Epoch 664/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6635 - accuracy: 0.7504 - val_loss: 0.6976 - val_accuracy: 0.7378\n",
            "Epoch 665/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6653 - accuracy: 0.7478 - val_loss: 0.6959 - val_accuracy: 0.7374\n",
            "Epoch 666/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6621 - accuracy: 0.7499 - val_loss: 0.6928 - val_accuracy: 0.7331\n",
            "Epoch 667/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6652 - accuracy: 0.7475 - val_loss: 0.7052 - val_accuracy: 0.7289\n",
            "Epoch 668/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6622 - accuracy: 0.7483 - val_loss: 0.6949 - val_accuracy: 0.7383\n",
            "Epoch 669/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6646 - accuracy: 0.7491 - val_loss: 0.7960 - val_accuracy: 0.6769\n",
            "Epoch 670/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6647 - accuracy: 0.7480 - val_loss: 0.7121 - val_accuracy: 0.7364\n",
            "Epoch 671/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6643 - accuracy: 0.7464 - val_loss: 0.7032 - val_accuracy: 0.7338\n",
            "Epoch 672/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6651 - accuracy: 0.7474 - val_loss: 0.7102 - val_accuracy: 0.7322\n",
            "Epoch 673/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6633 - accuracy: 0.7480 - val_loss: 0.6975 - val_accuracy: 0.7357\n",
            "Epoch 674/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6653 - accuracy: 0.7476 - val_loss: 0.6906 - val_accuracy: 0.7393\n",
            "Epoch 675/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6627 - accuracy: 0.7487 - val_loss: 0.7069 - val_accuracy: 0.7350\n",
            "Epoch 676/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6651 - accuracy: 0.7484 - val_loss: 0.6906 - val_accuracy: 0.7383\n",
            "Epoch 677/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6641 - accuracy: 0.7499 - val_loss: 0.6945 - val_accuracy: 0.7365\n",
            "Epoch 678/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6647 - accuracy: 0.7469 - val_loss: 0.7128 - val_accuracy: 0.7317\n",
            "Epoch 679/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6639 - accuracy: 0.7511 - val_loss: 0.7169 - val_accuracy: 0.7209\n",
            "Epoch 680/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6640 - accuracy: 0.7497 - val_loss: 0.6960 - val_accuracy: 0.7385\n",
            "Epoch 681/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6629 - accuracy: 0.7480 - val_loss: 0.7252 - val_accuracy: 0.7263\n",
            "Epoch 682/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6629 - accuracy: 0.7482 - val_loss: 0.6913 - val_accuracy: 0.7392\n",
            "Epoch 683/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6623 - accuracy: 0.7502 - val_loss: 0.7326 - val_accuracy: 0.7242\n",
            "Epoch 684/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6653 - accuracy: 0.7499 - val_loss: 0.7077 - val_accuracy: 0.7364\n",
            "Epoch 685/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6620 - accuracy: 0.7476 - val_loss: 0.6954 - val_accuracy: 0.7392\n",
            "Epoch 686/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6640 - accuracy: 0.7480 - val_loss: 0.7027 - val_accuracy: 0.7343\n",
            "Epoch 687/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6627 - accuracy: 0.7493 - val_loss: 0.6951 - val_accuracy: 0.7371\n",
            "Epoch 688/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6628 - accuracy: 0.7486 - val_loss: 0.6918 - val_accuracy: 0.7387\n",
            "Epoch 689/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7495 - val_loss: 0.7129 - val_accuracy: 0.7353\n",
            "Epoch 690/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6640 - accuracy: 0.7493 - val_loss: 0.6913 - val_accuracy: 0.7387\n",
            "Epoch 691/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6618 - accuracy: 0.7476 - val_loss: 0.6927 - val_accuracy: 0.7393\n",
            "Epoch 692/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6629 - accuracy: 0.7478 - val_loss: 0.7047 - val_accuracy: 0.7329\n",
            "Epoch 693/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6616 - accuracy: 0.7499 - val_loss: 0.7070 - val_accuracy: 0.7284\n",
            "Epoch 694/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6646 - accuracy: 0.7496 - val_loss: 0.7066 - val_accuracy: 0.7310\n",
            "Epoch 695/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6624 - accuracy: 0.7492 - val_loss: 0.6948 - val_accuracy: 0.7374\n",
            "Epoch 696/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6636 - accuracy: 0.7500 - val_loss: 0.6948 - val_accuracy: 0.7378\n",
            "Epoch 697/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6616 - accuracy: 0.7504 - val_loss: 0.6965 - val_accuracy: 0.7310\n",
            "Epoch 698/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6636 - accuracy: 0.7462 - val_loss: 0.6915 - val_accuracy: 0.7415\n",
            "Epoch 699/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7509 - val_loss: 0.6986 - val_accuracy: 0.7358\n",
            "Epoch 700/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6631 - accuracy: 0.7484 - val_loss: 0.6917 - val_accuracy: 0.7387\n",
            "Epoch 701/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6622 - accuracy: 0.7501 - val_loss: 0.7221 - val_accuracy: 0.7226\n",
            "Epoch 702/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6624 - accuracy: 0.7487 - val_loss: 0.6939 - val_accuracy: 0.7377\n",
            "Epoch 703/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7505 - val_loss: 0.7689 - val_accuracy: 0.7173\n",
            "Epoch 704/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6630 - accuracy: 0.7478 - val_loss: 0.7078 - val_accuracy: 0.7329\n",
            "Epoch 705/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6628 - accuracy: 0.7467 - val_loss: 0.7062 - val_accuracy: 0.7279\n",
            "Epoch 706/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6621 - accuracy: 0.7485 - val_loss: 0.6903 - val_accuracy: 0.7367\n",
            "Epoch 707/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6628 - accuracy: 0.7490 - val_loss: 0.6932 - val_accuracy: 0.7322\n",
            "Epoch 708/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6593 - accuracy: 0.7497 - val_loss: 0.7082 - val_accuracy: 0.7232\n",
            "Epoch 709/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6617 - accuracy: 0.7479 - val_loss: 0.6914 - val_accuracy: 0.7378\n",
            "Epoch 710/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6632 - accuracy: 0.7460 - val_loss: 0.6973 - val_accuracy: 0.7277\n",
            "Epoch 711/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6627 - accuracy: 0.7488 - val_loss: 0.6965 - val_accuracy: 0.7299\n",
            "Epoch 712/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6619 - accuracy: 0.7482 - val_loss: 0.6964 - val_accuracy: 0.7365\n",
            "Epoch 713/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6653 - accuracy: 0.7471 - val_loss: 0.7097 - val_accuracy: 0.7242\n",
            "Epoch 714/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6629 - accuracy: 0.7497 - val_loss: 0.7037 - val_accuracy: 0.7338\n",
            "Epoch 715/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7501 - val_loss: 0.7002 - val_accuracy: 0.7374\n",
            "Epoch 716/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6663 - accuracy: 0.7465 - val_loss: 0.6973 - val_accuracy: 0.7383\n",
            "Epoch 717/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6620 - accuracy: 0.7487 - val_loss: 0.7098 - val_accuracy: 0.7344\n",
            "Epoch 718/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6628 - accuracy: 0.7488 - val_loss: 0.6927 - val_accuracy: 0.7401\n",
            "Epoch 719/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6633 - accuracy: 0.7487 - val_loss: 0.7216 - val_accuracy: 0.7308\n",
            "Epoch 720/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6643 - accuracy: 0.7486 - val_loss: 0.6966 - val_accuracy: 0.7335\n",
            "Epoch 721/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6629 - accuracy: 0.7491 - val_loss: 0.6973 - val_accuracy: 0.7387\n",
            "Epoch 722/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6625 - accuracy: 0.7476 - val_loss: 0.6944 - val_accuracy: 0.7332\n",
            "Epoch 723/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6614 - accuracy: 0.7482 - val_loss: 0.6964 - val_accuracy: 0.7357\n",
            "Epoch 724/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6614 - accuracy: 0.7497 - val_loss: 0.6920 - val_accuracy: 0.7385\n",
            "Epoch 725/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7498 - val_loss: 0.6931 - val_accuracy: 0.7396\n",
            "Epoch 726/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6604 - accuracy: 0.7503 - val_loss: 0.6994 - val_accuracy: 0.7360\n",
            "Epoch 727/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6611 - accuracy: 0.7491 - val_loss: 0.7891 - val_accuracy: 0.6994\n",
            "Epoch 728/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6639 - accuracy: 0.7493 - val_loss: 0.7010 - val_accuracy: 0.7345\n",
            "Epoch 729/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6629 - accuracy: 0.7501 - val_loss: 0.7013 - val_accuracy: 0.7355\n",
            "Epoch 730/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7483 - val_loss: 0.7069 - val_accuracy: 0.7343\n",
            "Epoch 731/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6609 - accuracy: 0.7503 - val_loss: 0.7056 - val_accuracy: 0.7325\n",
            "Epoch 732/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6600 - accuracy: 0.7511 - val_loss: 0.6940 - val_accuracy: 0.7376\n",
            "Epoch 733/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6606 - accuracy: 0.7500 - val_loss: 0.6922 - val_accuracy: 0.7379\n",
            "Epoch 734/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6599 - accuracy: 0.7509 - val_loss: 0.6957 - val_accuracy: 0.7373\n",
            "Epoch 735/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6608 - accuracy: 0.7486 - val_loss: 0.7035 - val_accuracy: 0.7346\n",
            "Epoch 736/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6608 - accuracy: 0.7510 - val_loss: 0.6977 - val_accuracy: 0.7379\n",
            "Epoch 737/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6609 - accuracy: 0.7497 - val_loss: 0.7028 - val_accuracy: 0.7313\n",
            "Epoch 738/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6641 - accuracy: 0.7467 - val_loss: 0.6909 - val_accuracy: 0.7364\n",
            "Epoch 739/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6593 - accuracy: 0.7496 - val_loss: 0.7038 - val_accuracy: 0.7346\n",
            "Epoch 740/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6631 - accuracy: 0.7501 - val_loss: 0.6966 - val_accuracy: 0.7310\n",
            "Epoch 741/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6623 - accuracy: 0.7497 - val_loss: 0.6909 - val_accuracy: 0.7405\n",
            "Epoch 742/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6607 - accuracy: 0.7504 - val_loss: 0.7227 - val_accuracy: 0.7183\n",
            "Epoch 743/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6619 - accuracy: 0.7486 - val_loss: 0.6969 - val_accuracy: 0.7307\n",
            "Epoch 744/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6585 - accuracy: 0.7485 - val_loss: 0.7102 - val_accuracy: 0.7240\n",
            "Epoch 745/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6609 - accuracy: 0.7515 - val_loss: 0.6888 - val_accuracy: 0.7416\n",
            "Epoch 746/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6598 - accuracy: 0.7490 - val_loss: 0.6908 - val_accuracy: 0.7340\n",
            "Epoch 747/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6605 - accuracy: 0.7496 - val_loss: 0.7102 - val_accuracy: 0.7313\n",
            "Epoch 748/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6622 - accuracy: 0.7480 - val_loss: 0.6908 - val_accuracy: 0.7398\n",
            "Epoch 749/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6606 - accuracy: 0.7505 - val_loss: 0.7025 - val_accuracy: 0.7301\n",
            "Epoch 750/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6599 - accuracy: 0.7503 - val_loss: 0.6944 - val_accuracy: 0.7391\n",
            "Epoch 751/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6616 - accuracy: 0.7480 - val_loss: 0.7209 - val_accuracy: 0.7279\n",
            "Epoch 752/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6633 - accuracy: 0.7501 - val_loss: 0.6924 - val_accuracy: 0.7392\n",
            "Epoch 753/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6596 - accuracy: 0.7491 - val_loss: 0.6946 - val_accuracy: 0.7388\n",
            "Epoch 754/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6602 - accuracy: 0.7496 - val_loss: 0.7116 - val_accuracy: 0.7330\n",
            "Epoch 755/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6600 - accuracy: 0.7505 - val_loss: 0.7015 - val_accuracy: 0.7336\n",
            "Epoch 756/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6601 - accuracy: 0.7489 - val_loss: 0.7109 - val_accuracy: 0.7335\n",
            "Epoch 757/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6599 - accuracy: 0.7518 - val_loss: 0.6897 - val_accuracy: 0.7398\n",
            "Epoch 758/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6619 - accuracy: 0.7490 - val_loss: 0.6995 - val_accuracy: 0.7278\n",
            "Epoch 759/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6613 - accuracy: 0.7484 - val_loss: 0.6945 - val_accuracy: 0.7378\n",
            "Epoch 760/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6602 - accuracy: 0.7501 - val_loss: 0.6968 - val_accuracy: 0.7395\n",
            "Epoch 761/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6613 - accuracy: 0.7512 - val_loss: 0.6966 - val_accuracy: 0.7365\n",
            "Epoch 762/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6578 - accuracy: 0.7513 - val_loss: 0.7058 - val_accuracy: 0.7364\n",
            "Epoch 763/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6647 - accuracy: 0.7472 - val_loss: 0.6945 - val_accuracy: 0.7392\n",
            "Epoch 764/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6631 - accuracy: 0.7489 - val_loss: 0.7102 - val_accuracy: 0.7298\n",
            "Epoch 765/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6591 - accuracy: 0.7502 - val_loss: 0.7066 - val_accuracy: 0.7330\n",
            "Epoch 766/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6608 - accuracy: 0.7513 - val_loss: 0.6968 - val_accuracy: 0.7313\n",
            "Epoch 767/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6604 - accuracy: 0.7490 - val_loss: 0.7109 - val_accuracy: 0.7312\n",
            "Epoch 768/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6600 - accuracy: 0.7501 - val_loss: 0.6918 - val_accuracy: 0.7390\n",
            "Epoch 769/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6600 - accuracy: 0.7492 - val_loss: 0.7135 - val_accuracy: 0.7272\n",
            "Epoch 770/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6617 - accuracy: 0.7490 - val_loss: 0.6961 - val_accuracy: 0.7383\n",
            "Epoch 771/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6601 - accuracy: 0.7497 - val_loss: 0.6943 - val_accuracy: 0.7280\n",
            "Epoch 772/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7495 - val_loss: 0.7362 - val_accuracy: 0.7246\n",
            "Epoch 773/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7499 - val_loss: 0.7174 - val_accuracy: 0.7313\n",
            "Epoch 774/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6627 - accuracy: 0.7493 - val_loss: 0.6897 - val_accuracy: 0.7388\n",
            "Epoch 775/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6594 - accuracy: 0.7508 - val_loss: 0.6928 - val_accuracy: 0.7371\n",
            "Epoch 776/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6598 - accuracy: 0.7504 - val_loss: 0.6975 - val_accuracy: 0.7371\n",
            "Epoch 777/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6616 - accuracy: 0.7509 - val_loss: 0.7190 - val_accuracy: 0.7294\n",
            "Epoch 778/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6613 - accuracy: 0.7489 - val_loss: 0.7019 - val_accuracy: 0.7346\n",
            "Epoch 779/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6610 - accuracy: 0.7513 - val_loss: 0.6958 - val_accuracy: 0.7312\n",
            "Epoch 780/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6611 - accuracy: 0.7484 - val_loss: 0.7086 - val_accuracy: 0.7345\n",
            "Epoch 781/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6615 - accuracy: 0.7486 - val_loss: 0.7065 - val_accuracy: 0.7327\n",
            "Epoch 782/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6610 - accuracy: 0.7492 - val_loss: 0.6954 - val_accuracy: 0.7259\n",
            "Epoch 783/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6596 - accuracy: 0.7495 - val_loss: 0.7133 - val_accuracy: 0.7339\n",
            "Epoch 784/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6584 - accuracy: 0.7514 - val_loss: 0.7066 - val_accuracy: 0.7335\n",
            "Epoch 785/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6584 - accuracy: 0.7516 - val_loss: 0.6919 - val_accuracy: 0.7367\n",
            "Epoch 786/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6619 - accuracy: 0.7506 - val_loss: 0.7197 - val_accuracy: 0.7297\n",
            "Epoch 787/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6602 - accuracy: 0.7490 - val_loss: 0.7000 - val_accuracy: 0.7282\n",
            "Epoch 788/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7484 - val_loss: 0.6963 - val_accuracy: 0.7374\n",
            "Epoch 789/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6592 - accuracy: 0.7514 - val_loss: 0.6898 - val_accuracy: 0.7406\n",
            "Epoch 790/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7497 - val_loss: 0.7226 - val_accuracy: 0.7311\n",
            "Epoch 791/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7508 - val_loss: 0.7195 - val_accuracy: 0.7266\n",
            "Epoch 792/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6596 - accuracy: 0.7505 - val_loss: 0.6905 - val_accuracy: 0.7392\n",
            "Epoch 793/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7505 - val_loss: 0.6951 - val_accuracy: 0.7360\n",
            "Epoch 794/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6584 - accuracy: 0.7500 - val_loss: 0.6947 - val_accuracy: 0.7396\n",
            "Epoch 795/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6610 - accuracy: 0.7503 - val_loss: 0.6949 - val_accuracy: 0.7385\n",
            "Epoch 796/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6609 - accuracy: 0.7477 - val_loss: 0.6943 - val_accuracy: 0.7368\n",
            "Epoch 797/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6569 - accuracy: 0.7517 - val_loss: 0.6979 - val_accuracy: 0.7382\n",
            "Epoch 798/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6603 - accuracy: 0.7502 - val_loss: 0.6970 - val_accuracy: 0.7360\n",
            "Epoch 799/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6602 - accuracy: 0.7489 - val_loss: 0.6956 - val_accuracy: 0.7381\n",
            "Epoch 800/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6608 - accuracy: 0.7506 - val_loss: 0.6986 - val_accuracy: 0.7302\n",
            "Epoch 801/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6589 - accuracy: 0.7502 - val_loss: 0.6991 - val_accuracy: 0.7376\n",
            "Epoch 802/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6603 - accuracy: 0.7500 - val_loss: 0.6929 - val_accuracy: 0.7379\n",
            "Epoch 803/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6595 - accuracy: 0.7515 - val_loss: 0.7439 - val_accuracy: 0.7275\n",
            "Epoch 804/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6607 - accuracy: 0.7489 - val_loss: 0.6995 - val_accuracy: 0.7369\n",
            "Epoch 805/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6587 - accuracy: 0.7505 - val_loss: 0.6985 - val_accuracy: 0.7315\n",
            "Epoch 806/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6583 - accuracy: 0.7515 - val_loss: 0.6919 - val_accuracy: 0.7383\n",
            "Epoch 807/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6600 - accuracy: 0.7493 - val_loss: 0.6952 - val_accuracy: 0.7373\n",
            "Epoch 808/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6642 - accuracy: 0.7497 - val_loss: 0.7058 - val_accuracy: 0.7340\n",
            "Epoch 809/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6583 - accuracy: 0.7491 - val_loss: 0.7077 - val_accuracy: 0.7291\n",
            "Epoch 810/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6594 - accuracy: 0.7511 - val_loss: 0.7279 - val_accuracy: 0.7165\n",
            "Epoch 811/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6566 - accuracy: 0.7519 - val_loss: 0.7002 - val_accuracy: 0.7355\n",
            "Epoch 812/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6594 - accuracy: 0.7495 - val_loss: 0.6909 - val_accuracy: 0.7404\n",
            "Epoch 813/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6603 - accuracy: 0.7500 - val_loss: 0.6925 - val_accuracy: 0.7358\n",
            "Epoch 814/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6577 - accuracy: 0.7500 - val_loss: 0.7030 - val_accuracy: 0.7381\n",
            "Epoch 815/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6603 - accuracy: 0.7497 - val_loss: 0.7089 - val_accuracy: 0.7352\n",
            "Epoch 816/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6623 - accuracy: 0.7502 - val_loss: 0.6952 - val_accuracy: 0.7368\n",
            "Epoch 817/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6606 - accuracy: 0.7505 - val_loss: 0.6873 - val_accuracy: 0.7411\n",
            "Epoch 818/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6599 - accuracy: 0.7504 - val_loss: 0.7086 - val_accuracy: 0.7341\n",
            "Epoch 819/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6567 - accuracy: 0.7488 - val_loss: 0.6939 - val_accuracy: 0.7373\n",
            "Epoch 820/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6580 - accuracy: 0.7494 - val_loss: 0.7049 - val_accuracy: 0.7322\n",
            "Epoch 821/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7489 - val_loss: 0.6979 - val_accuracy: 0.7362\n",
            "Epoch 822/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6614 - accuracy: 0.7478 - val_loss: 0.7010 - val_accuracy: 0.7317\n",
            "Epoch 823/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6593 - accuracy: 0.7510 - val_loss: 0.6888 - val_accuracy: 0.7397\n",
            "Epoch 824/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6606 - accuracy: 0.7501 - val_loss: 0.6897 - val_accuracy: 0.7393\n",
            "Epoch 825/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6565 - accuracy: 0.7520 - val_loss: 0.7082 - val_accuracy: 0.7268\n",
            "Epoch 826/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6609 - accuracy: 0.7495 - val_loss: 0.6955 - val_accuracy: 0.7376\n",
            "Epoch 827/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6578 - accuracy: 0.7517 - val_loss: 0.6951 - val_accuracy: 0.7363\n",
            "Epoch 828/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6585 - accuracy: 0.7508 - val_loss: 0.6947 - val_accuracy: 0.7284\n",
            "Epoch 829/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6598 - accuracy: 0.7489 - val_loss: 0.7024 - val_accuracy: 0.7354\n",
            "Epoch 830/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6638 - accuracy: 0.7492 - val_loss: 0.6869 - val_accuracy: 0.7410\n",
            "Epoch 831/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6595 - accuracy: 0.7497 - val_loss: 0.6887 - val_accuracy: 0.7418\n",
            "Epoch 832/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6591 - accuracy: 0.7527 - val_loss: 0.6906 - val_accuracy: 0.7391\n",
            "Epoch 833/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6600 - accuracy: 0.7527 - val_loss: 0.6898 - val_accuracy: 0.7385\n",
            "Epoch 834/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6606 - accuracy: 0.7512 - val_loss: 0.6896 - val_accuracy: 0.7410\n",
            "Epoch 835/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7497 - val_loss: 0.6921 - val_accuracy: 0.7382\n",
            "Epoch 836/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6568 - accuracy: 0.7511 - val_loss: 0.6888 - val_accuracy: 0.7397\n",
            "Epoch 837/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6582 - accuracy: 0.7515 - val_loss: 0.6931 - val_accuracy: 0.7395\n",
            "Epoch 838/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6594 - accuracy: 0.7492 - val_loss: 0.7004 - val_accuracy: 0.7362\n",
            "Epoch 839/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6580 - accuracy: 0.7496 - val_loss: 0.7107 - val_accuracy: 0.7307\n",
            "Epoch 840/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6613 - accuracy: 0.7521 - val_loss: 0.6884 - val_accuracy: 0.7363\n",
            "Epoch 841/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6598 - accuracy: 0.7498 - val_loss: 0.6901 - val_accuracy: 0.7412\n",
            "Epoch 842/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6608 - accuracy: 0.7510 - val_loss: 0.7201 - val_accuracy: 0.7317\n",
            "Epoch 843/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6603 - accuracy: 0.7503 - val_loss: 0.6981 - val_accuracy: 0.7319\n",
            "Epoch 844/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6586 - accuracy: 0.7526 - val_loss: 0.7017 - val_accuracy: 0.7325\n",
            "Epoch 845/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6570 - accuracy: 0.7503 - val_loss: 0.7081 - val_accuracy: 0.7340\n",
            "Epoch 846/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6595 - accuracy: 0.7515 - val_loss: 0.6930 - val_accuracy: 0.7391\n",
            "Epoch 847/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6594 - accuracy: 0.7507 - val_loss: 0.6951 - val_accuracy: 0.7402\n",
            "Epoch 848/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6588 - accuracy: 0.7512 - val_loss: 0.7020 - val_accuracy: 0.7368\n",
            "Epoch 849/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6592 - accuracy: 0.7509 - val_loss: 0.6935 - val_accuracy: 0.7339\n",
            "Epoch 850/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6612 - accuracy: 0.7505 - val_loss: 0.7012 - val_accuracy: 0.7364\n",
            "Epoch 851/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6568 - accuracy: 0.7510 - val_loss: 0.7181 - val_accuracy: 0.7332\n",
            "Epoch 852/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6592 - accuracy: 0.7515 - val_loss: 0.7038 - val_accuracy: 0.7287\n",
            "Epoch 853/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6582 - accuracy: 0.7512 - val_loss: 0.7019 - val_accuracy: 0.7382\n",
            "Epoch 854/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6626 - accuracy: 0.7497 - val_loss: 0.6865 - val_accuracy: 0.7421\n",
            "Epoch 855/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6586 - accuracy: 0.7496 - val_loss: 0.6936 - val_accuracy: 0.7371\n",
            "Epoch 856/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6569 - accuracy: 0.7503 - val_loss: 0.6971 - val_accuracy: 0.7332\n",
            "Epoch 857/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6585 - accuracy: 0.7526 - val_loss: 0.6939 - val_accuracy: 0.7345\n",
            "Epoch 858/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6589 - accuracy: 0.7495 - val_loss: 0.7043 - val_accuracy: 0.7392\n",
            "Epoch 859/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7505 - val_loss: 0.6877 - val_accuracy: 0.7397\n",
            "Epoch 860/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6578 - accuracy: 0.7496 - val_loss: 0.7115 - val_accuracy: 0.7242\n",
            "Epoch 861/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6580 - accuracy: 0.7516 - val_loss: 0.7004 - val_accuracy: 0.7344\n",
            "Epoch 862/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7493 - val_loss: 0.7054 - val_accuracy: 0.7358\n",
            "Epoch 863/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6596 - accuracy: 0.7519 - val_loss: 0.6883 - val_accuracy: 0.7391\n",
            "Epoch 864/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6580 - accuracy: 0.7503 - val_loss: 0.6936 - val_accuracy: 0.7330\n",
            "Epoch 865/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7510 - val_loss: 0.6860 - val_accuracy: 0.7404\n",
            "Epoch 866/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6579 - accuracy: 0.7502 - val_loss: 0.6926 - val_accuracy: 0.7390\n",
            "Epoch 867/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6563 - accuracy: 0.7527 - val_loss: 0.7030 - val_accuracy: 0.7358\n",
            "Epoch 868/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6593 - accuracy: 0.7486 - val_loss: 0.6920 - val_accuracy: 0.7387\n",
            "Epoch 869/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6573 - accuracy: 0.7517 - val_loss: 0.6902 - val_accuracy: 0.7369\n",
            "Epoch 870/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6590 - accuracy: 0.7502 - val_loss: 0.7350 - val_accuracy: 0.7187\n",
            "Epoch 871/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6591 - accuracy: 0.7497 - val_loss: 0.7097 - val_accuracy: 0.7266\n",
            "Epoch 872/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6602 - accuracy: 0.7500 - val_loss: 0.6946 - val_accuracy: 0.7387\n",
            "Epoch 873/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6585 - accuracy: 0.7512 - val_loss: 0.7035 - val_accuracy: 0.7352\n",
            "Epoch 874/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7514 - val_loss: 0.7275 - val_accuracy: 0.7216\n",
            "Epoch 875/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6575 - accuracy: 0.7507 - val_loss: 0.6907 - val_accuracy: 0.7407\n",
            "Epoch 876/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6585 - accuracy: 0.7506 - val_loss: 0.6864 - val_accuracy: 0.7423\n",
            "Epoch 877/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6591 - accuracy: 0.7515 - val_loss: 0.6946 - val_accuracy: 0.7376\n",
            "Epoch 878/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7528 - val_loss: 0.7029 - val_accuracy: 0.7273\n",
            "Epoch 879/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6571 - accuracy: 0.7500 - val_loss: 0.7054 - val_accuracy: 0.7346\n",
            "Epoch 880/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6562 - accuracy: 0.7518 - val_loss: 0.6925 - val_accuracy: 0.7352\n",
            "Epoch 881/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6566 - accuracy: 0.7506 - val_loss: 0.7121 - val_accuracy: 0.7334\n",
            "Epoch 882/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6568 - accuracy: 0.7526 - val_loss: 0.6892 - val_accuracy: 0.7359\n",
            "Epoch 883/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6587 - accuracy: 0.7500 - val_loss: 0.6859 - val_accuracy: 0.7391\n",
            "Epoch 884/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6591 - accuracy: 0.7489 - val_loss: 0.6860 - val_accuracy: 0.7402\n",
            "Epoch 885/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7483 - val_loss: 0.7286 - val_accuracy: 0.7174\n",
            "Epoch 886/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6574 - accuracy: 0.7501 - val_loss: 0.7188 - val_accuracy: 0.7264\n",
            "Epoch 887/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6586 - accuracy: 0.7504 - val_loss: 0.6882 - val_accuracy: 0.7420\n",
            "Epoch 888/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6583 - accuracy: 0.7497 - val_loss: 0.6871 - val_accuracy: 0.7395\n",
            "Epoch 889/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6584 - accuracy: 0.7505 - val_loss: 0.6966 - val_accuracy: 0.7362\n",
            "Epoch 890/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6578 - accuracy: 0.7513 - val_loss: 0.6984 - val_accuracy: 0.7365\n",
            "Epoch 891/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6589 - accuracy: 0.7492 - val_loss: 0.7050 - val_accuracy: 0.7355\n",
            "Epoch 892/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6555 - accuracy: 0.7520 - val_loss: 0.7333 - val_accuracy: 0.7264\n",
            "Epoch 893/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6598 - accuracy: 0.7498 - val_loss: 0.6917 - val_accuracy: 0.7339\n",
            "Epoch 894/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6571 - accuracy: 0.7525 - val_loss: 0.6906 - val_accuracy: 0.7400\n",
            "Epoch 895/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7509 - val_loss: 0.6978 - val_accuracy: 0.7367\n",
            "Epoch 896/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6556 - accuracy: 0.7522 - val_loss: 0.6912 - val_accuracy: 0.7379\n",
            "Epoch 897/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6554 - accuracy: 0.7525 - val_loss: 0.6932 - val_accuracy: 0.7325\n",
            "Epoch 898/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6595 - accuracy: 0.7511 - val_loss: 0.7144 - val_accuracy: 0.7313\n",
            "Epoch 899/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6569 - accuracy: 0.7510 - val_loss: 0.6995 - val_accuracy: 0.7369\n",
            "Epoch 900/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6561 - accuracy: 0.7510 - val_loss: 0.6908 - val_accuracy: 0.7379\n",
            "Epoch 901/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6597 - accuracy: 0.7496 - val_loss: 0.6944 - val_accuracy: 0.7419\n",
            "Epoch 902/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6584 - accuracy: 0.7512 - val_loss: 0.6908 - val_accuracy: 0.7346\n",
            "Epoch 903/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6577 - accuracy: 0.7507 - val_loss: 0.6925 - val_accuracy: 0.7374\n",
            "Epoch 904/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6555 - accuracy: 0.7515 - val_loss: 0.7087 - val_accuracy: 0.7359\n",
            "Epoch 905/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6586 - accuracy: 0.7514 - val_loss: 0.6992 - val_accuracy: 0.7385\n",
            "Epoch 906/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6555 - accuracy: 0.7528 - val_loss: 0.7261 - val_accuracy: 0.7306\n",
            "Epoch 907/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7521 - val_loss: 0.7031 - val_accuracy: 0.7322\n",
            "Epoch 908/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6566 - accuracy: 0.7512 - val_loss: 0.7031 - val_accuracy: 0.7330\n",
            "Epoch 909/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6564 - accuracy: 0.7530 - val_loss: 0.6932 - val_accuracy: 0.7382\n",
            "Epoch 910/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6584 - accuracy: 0.7490 - val_loss: 0.6873 - val_accuracy: 0.7383\n",
            "Epoch 911/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6585 - accuracy: 0.7508 - val_loss: 0.6905 - val_accuracy: 0.7405\n",
            "Epoch 912/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6571 - accuracy: 0.7515 - val_loss: 0.7099 - val_accuracy: 0.7364\n",
            "Epoch 913/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6583 - accuracy: 0.7511 - val_loss: 0.7026 - val_accuracy: 0.7378\n",
            "Epoch 914/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6569 - accuracy: 0.7498 - val_loss: 0.7001 - val_accuracy: 0.7297\n",
            "Epoch 915/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6582 - accuracy: 0.7507 - val_loss: 0.7013 - val_accuracy: 0.7350\n",
            "Epoch 916/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6580 - accuracy: 0.7502 - val_loss: 0.7133 - val_accuracy: 0.7321\n",
            "Epoch 917/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6541 - accuracy: 0.7508 - val_loss: 0.7272 - val_accuracy: 0.7225\n",
            "Epoch 918/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6590 - accuracy: 0.7508 - val_loss: 0.7012 - val_accuracy: 0.7348\n",
            "Epoch 919/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7521 - val_loss: 0.7260 - val_accuracy: 0.7338\n",
            "Epoch 920/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6562 - accuracy: 0.7523 - val_loss: 0.6903 - val_accuracy: 0.7368\n",
            "Epoch 921/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6577 - accuracy: 0.7508 - val_loss: 0.6968 - val_accuracy: 0.7371\n",
            "Epoch 922/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6583 - accuracy: 0.7510 - val_loss: 0.7181 - val_accuracy: 0.7299\n",
            "Epoch 923/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6565 - accuracy: 0.7506 - val_loss: 0.6925 - val_accuracy: 0.7359\n",
            "Epoch 924/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6590 - accuracy: 0.7505 - val_loss: 0.6912 - val_accuracy: 0.7364\n",
            "Epoch 925/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6570 - accuracy: 0.7522 - val_loss: 0.6912 - val_accuracy: 0.7397\n",
            "Epoch 926/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6578 - accuracy: 0.7514 - val_loss: 0.6938 - val_accuracy: 0.7400\n",
            "Epoch 927/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6536 - accuracy: 0.7524 - val_loss: 0.6896 - val_accuracy: 0.7411\n",
            "Epoch 928/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6574 - accuracy: 0.7503 - val_loss: 0.7011 - val_accuracy: 0.7369\n",
            "Epoch 929/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6578 - accuracy: 0.7503 - val_loss: 0.6999 - val_accuracy: 0.7385\n",
            "Epoch 930/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6571 - accuracy: 0.7525 - val_loss: 0.6886 - val_accuracy: 0.7386\n",
            "Epoch 931/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7509 - val_loss: 0.7145 - val_accuracy: 0.7319\n",
            "Epoch 932/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6547 - accuracy: 0.7499 - val_loss: 0.6880 - val_accuracy: 0.7430\n",
            "Epoch 933/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6573 - accuracy: 0.7512 - val_loss: 0.6872 - val_accuracy: 0.7379\n",
            "Epoch 934/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6552 - accuracy: 0.7514 - val_loss: 0.7130 - val_accuracy: 0.7352\n",
            "Epoch 935/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6556 - accuracy: 0.7510 - val_loss: 0.6929 - val_accuracy: 0.7357\n",
            "Epoch 936/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6560 - accuracy: 0.7519 - val_loss: 0.7019 - val_accuracy: 0.7368\n",
            "Epoch 937/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6553 - accuracy: 0.7526 - val_loss: 0.7201 - val_accuracy: 0.7350\n",
            "Epoch 938/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6574 - accuracy: 0.7515 - val_loss: 0.6903 - val_accuracy: 0.7392\n",
            "Epoch 939/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6581 - accuracy: 0.7502 - val_loss: 0.7430 - val_accuracy: 0.7150\n",
            "Epoch 940/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6568 - accuracy: 0.7520 - val_loss: 0.6872 - val_accuracy: 0.7412\n",
            "Epoch 941/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6564 - accuracy: 0.7502 - val_loss: 0.7013 - val_accuracy: 0.7302\n",
            "Epoch 942/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6579 - accuracy: 0.7496 - val_loss: 0.7153 - val_accuracy: 0.7346\n",
            "Epoch 943/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6564 - accuracy: 0.7514 - val_loss: 0.6892 - val_accuracy: 0.7395\n",
            "Epoch 944/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6550 - accuracy: 0.7521 - val_loss: 0.6944 - val_accuracy: 0.7334\n",
            "Epoch 945/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6568 - accuracy: 0.7509 - val_loss: 0.6906 - val_accuracy: 0.7409\n",
            "Epoch 946/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6575 - accuracy: 0.7520 - val_loss: 0.6944 - val_accuracy: 0.7360\n",
            "Epoch 947/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7514 - val_loss: 0.6879 - val_accuracy: 0.7414\n",
            "Epoch 948/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6559 - accuracy: 0.7518 - val_loss: 0.6950 - val_accuracy: 0.7378\n",
            "Epoch 949/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7507 - val_loss: 0.6936 - val_accuracy: 0.7397\n",
            "Epoch 950/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6553 - accuracy: 0.7520 - val_loss: 0.6877 - val_accuracy: 0.7385\n",
            "Epoch 951/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6556 - accuracy: 0.7504 - val_loss: 0.7008 - val_accuracy: 0.7363\n",
            "Epoch 952/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6577 - accuracy: 0.7521 - val_loss: 0.6921 - val_accuracy: 0.7349\n",
            "Epoch 953/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7515 - val_loss: 0.6861 - val_accuracy: 0.7412\n",
            "Epoch 954/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6557 - accuracy: 0.7517 - val_loss: 0.6912 - val_accuracy: 0.7386\n",
            "Epoch 955/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6595 - accuracy: 0.7508 - val_loss: 0.6843 - val_accuracy: 0.7424\n",
            "Epoch 956/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6553 - accuracy: 0.7524 - val_loss: 0.6988 - val_accuracy: 0.7297\n",
            "Epoch 957/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6617 - accuracy: 0.7512 - val_loss: 0.7230 - val_accuracy: 0.7194\n",
            "Epoch 958/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6551 - accuracy: 0.7525 - val_loss: 0.6923 - val_accuracy: 0.7373\n",
            "Epoch 959/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6548 - accuracy: 0.7524 - val_loss: 0.7097 - val_accuracy: 0.7350\n",
            "Epoch 960/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6553 - accuracy: 0.7519 - val_loss: 0.6995 - val_accuracy: 0.7371\n",
            "Epoch 961/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6562 - accuracy: 0.7517 - val_loss: 0.6872 - val_accuracy: 0.7405\n",
            "Epoch 962/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6564 - accuracy: 0.7532 - val_loss: 0.6860 - val_accuracy: 0.7410\n",
            "Epoch 963/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6561 - accuracy: 0.7496 - val_loss: 0.6901 - val_accuracy: 0.7388\n",
            "Epoch 964/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6570 - accuracy: 0.7519 - val_loss: 0.6871 - val_accuracy: 0.7420\n",
            "Epoch 965/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6568 - accuracy: 0.7534 - val_loss: 0.6933 - val_accuracy: 0.7358\n",
            "Epoch 966/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6575 - accuracy: 0.7505 - val_loss: 0.6866 - val_accuracy: 0.7411\n",
            "Epoch 967/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6567 - accuracy: 0.7528 - val_loss: 0.6958 - val_accuracy: 0.7400\n",
            "Epoch 968/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6561 - accuracy: 0.7511 - val_loss: 0.7071 - val_accuracy: 0.7346\n",
            "Epoch 969/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6552 - accuracy: 0.7518 - val_loss: 0.7000 - val_accuracy: 0.7320\n",
            "Epoch 970/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6542 - accuracy: 0.7531 - val_loss: 0.6862 - val_accuracy: 0.7406\n",
            "Epoch 971/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6542 - accuracy: 0.7521 - val_loss: 0.7153 - val_accuracy: 0.7256\n",
            "Epoch 972/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6553 - accuracy: 0.7520 - val_loss: 0.6994 - val_accuracy: 0.7346\n",
            "Epoch 973/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6580 - accuracy: 0.7514 - val_loss: 0.7134 - val_accuracy: 0.7329\n",
            "Epoch 974/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6558 - accuracy: 0.7505 - val_loss: 0.7030 - val_accuracy: 0.7367\n",
            "Epoch 975/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6559 - accuracy: 0.7519 - val_loss: 0.7386 - val_accuracy: 0.7230\n",
            "Epoch 976/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7518 - val_loss: 0.6970 - val_accuracy: 0.7388\n",
            "Epoch 977/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6551 - accuracy: 0.7511 - val_loss: 0.6967 - val_accuracy: 0.7385\n",
            "Epoch 978/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6562 - accuracy: 0.7497 - val_loss: 0.7106 - val_accuracy: 0.7360\n",
            "Epoch 979/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6567 - accuracy: 0.7505 - val_loss: 0.7146 - val_accuracy: 0.7273\n",
            "Epoch 980/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6566 - accuracy: 0.7512 - val_loss: 0.6982 - val_accuracy: 0.7345\n",
            "Epoch 981/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6551 - accuracy: 0.7517 - val_loss: 0.6963 - val_accuracy: 0.7331\n",
            "Epoch 982/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7496 - val_loss: 0.6989 - val_accuracy: 0.7371\n",
            "Epoch 983/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6577 - accuracy: 0.7508 - val_loss: 0.6935 - val_accuracy: 0.7344\n",
            "Epoch 984/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6548 - accuracy: 0.7508 - val_loss: 0.6948 - val_accuracy: 0.7391\n",
            "Epoch 985/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6572 - accuracy: 0.7519 - val_loss: 0.6895 - val_accuracy: 0.7381\n",
            "Epoch 986/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6575 - accuracy: 0.7492 - val_loss: 0.6965 - val_accuracy: 0.7340\n",
            "Epoch 987/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6528 - accuracy: 0.7536 - val_loss: 0.6851 - val_accuracy: 0.7398\n",
            "Epoch 988/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6563 - accuracy: 0.7505 - val_loss: 0.7025 - val_accuracy: 0.7349\n",
            "Epoch 989/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6541 - accuracy: 0.7525 - val_loss: 0.7134 - val_accuracy: 0.7316\n",
            "Epoch 990/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6569 - accuracy: 0.7523 - val_loss: 0.6892 - val_accuracy: 0.7382\n",
            "Epoch 991/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6556 - accuracy: 0.7519 - val_loss: 0.7129 - val_accuracy: 0.7352\n",
            "Epoch 992/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6569 - accuracy: 0.7533 - val_loss: 0.7411 - val_accuracy: 0.7199\n",
            "Epoch 993/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6561 - accuracy: 0.7515 - val_loss: 0.6924 - val_accuracy: 0.7379\n",
            "Epoch 994/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6576 - accuracy: 0.7516 - val_loss: 0.7009 - val_accuracy: 0.7376\n",
            "Epoch 995/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6563 - accuracy: 0.7523 - val_loss: 0.6977 - val_accuracy: 0.7334\n",
            "Epoch 996/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6556 - accuracy: 0.7520 - val_loss: 0.6968 - val_accuracy: 0.7321\n",
            "Epoch 997/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6565 - accuracy: 0.7517 - val_loss: 0.6965 - val_accuracy: 0.7377\n",
            "Epoch 998/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6549 - accuracy: 0.7519 - val_loss: 0.6887 - val_accuracy: 0.7400\n",
            "Epoch 999/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6569 - accuracy: 0.7527 - val_loss: 0.6904 - val_accuracy: 0.7382\n",
            "Epoch 1000/1000\n",
            "985/985 [==============================] - 3s 3ms/step - loss: 0.6537 - accuracy: 0.7525 - val_loss: 0.6950 - val_accuracy: 0.7376\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping,Callback,CSVLogger\n",
        "import gc\n",
        "class GarbageCollectorCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(4, input_shape=(600,), activation='linear'),  # Linear layer\n",
        "    Dense(3, activation='softmax')  # Softmax layer for classification\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "lambda_reg = 0.01\n",
        "def custom_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int64)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    cross_entropy_loss = -tf.reduce_sum(y_true * tf.math.log(y_pred))\n",
        "    regularization_term = lambda_reg * tf.reduce_sum([tf.reduce_sum(tf.square(w)) for w in model.trainable_weights])\n",
        "    return cross_entropy_loss + regularization_term\n",
        "\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/baseline/'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'model_weights_epoch{epoch:02d}_val_acc{val_accuracy:.4f}.h5'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "#filepath_1 = os.path.join(checkpoint_filepath,'with_biLSTM_layers_weights.best.hdf5')\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback, history_logger_1]\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss_function, metrics=[ 'accuracy'])\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
        "\n",
        "# Assuming X_parallel, y_train, and other variables are defined\n",
        "history = model.fit(\n",
        "    x=np.array(X_parallel), y=y_enc,\n",
        "    epochs=1000,\n",
        "    validation_split=0.2,\n",
        "    callbacks=callbacks_list_1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTUQksPdvowu"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "\n",
        "# model = Sequential([\n",
        "#     Dense(4, input_shape=(600,), activation='linear'),  # Linear layer\n",
        "#     Dense(3, activation='softmax')  # Softmax layer for classification\n",
        "# ])\n",
        "\n",
        "\n",
        "\n",
        "# loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# lambda_reg = 0.01\n",
        "# def custom_loss(y_true, y_pred):\n",
        "#     y_true = tf.cast(y_true, tf.int64)\n",
        "#     y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "#     cross_entropy_loss = -tf.reduce_sum(y_true * tf.math.log(y_pred))\n",
        "#     regularization_term = lambda_reg * tf.reduce_sum([tf.reduce_sum(tf.square(w)) for w in model.trainable_weights])\n",
        "#     return cross_entropy_loss + regularization_term\n",
        "\n",
        "# def perplexity(y_true, y_pred):\n",
        "#     y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n",
        "#     cross_entropy = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
        "#     perplexity = tf.exp(tf.reduce_mean(cross_entropy))\n",
        "#     return perplexity\n",
        "\n",
        "# checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/'\n",
        "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     filepath=os.path.join(checkpoint_filepath, 'model_weights_epoch{epoch:02d}_val_acc{val_accuracy:.4f}.h5'),\n",
        "#     save_weights_only=True,\n",
        "#     monitor='val_accuracy',\n",
        "#     mode='max',\n",
        "#     save_best_only=True)\n",
        "\n",
        "\n",
        "# model.compile(optimizer='adam', loss=loss_function, metrics=[ 'accuracy', perplexity])\n",
        "\n",
        "# model.load_weights(os.listdir(checkpoint_filepath)[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "ZigN9IEgvowu",
        "outputId": "0e581c95-296f-4281-a003-089ce2487ecb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_filepath = '/content/drive/MyDrive/ASC/results/plots/'\n",
        "\n",
        "# Assuming 'history' contains your training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])  # Use 'acc' instead of 'accuracy'\n",
        "plt.plot(history.history['val_accuracy'])  # Use 'val_acc' instead of 'val_accuracy'\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(plot_filepath+'metrcis.jpg')\n",
        "plt.savefig(plot_filepath+'metrcis.svg')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLDm3AT-vowv",
        "outputId": "0e145783-e3d8-4446-e6e5-15ab0ec118b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "307/307 [==============================] - 0s 1ms/step\n",
            "Sentiment Class \"Negative\":\n",
            "  Precision: 0.48119692680954307\n",
            "  Recall: 0.37327478042659973\n",
            "  F1 Score: 0.42042042042042044\n",
            "Sentiment Class \"Neutral\":\n",
            "  Precision: 0.44250780437044746\n",
            "  Recall: 0.5187557182067704\n",
            "  F1 Score: 0.4776077495437316\n",
            "Sentiment Class \"Positive\":\n",
            "  Precision: 0.6080964685615848\n",
            "  Recall: 0.6354635463546354\n",
            "  F1 Score: 0.6214788732394366\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming y_test[:9800] represents the true labels for the first 9800 samples\n",
        "# Modify it accordingly if your true labels cover a different range\n",
        "\n",
        "sentiment_mapping = {1: 'Neutral', 0: 'Negative', 2: 'Positive'}\n",
        "\n",
        "predicted_probs = model.predict(X_t)\n",
        "predicted_labels = np.argmax(predicted_probs, axis=1)\n",
        "\n",
        "# Compute precision, recall, and f1 score for each class\n",
        "precision = precision_score(y_test[:9800], predicted_labels, average=None)\n",
        "recall = recall_score(y_test[:9800], predicted_labels, average=None)\n",
        "f1 = f1_score(y_test[:9800], predicted_labels, average=None)\n",
        "metrics_dictionary = {'precision': precision,\n",
        "                      'recall': recall,\n",
        "                      'f1': f1\n",
        "                      }\n",
        "dataframe = pd.DataFrame(metrics_dictionary)\n",
        "dataframe.to_csv(plot_filepath+'metrics.csv')\n",
        "\n",
        "\n",
        "# Print precision, recall, and f1 score for each class\n",
        "for i in range(len(precision)):\n",
        "    sentiment_class = sentiment_mapping[i]\n",
        "    print(f'Sentiment Class \"{sentiment_class}\":')\n",
        "    print(f'  Precision: {precision[i]}')\n",
        "    print(f'  Recall: {recall[i]}')\n",
        "    print(f'  F1 Score: {f1[i]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cg6XCnTypNZB",
        "outputId": "323acbac-adf7-4c49-c16b-e5ba27fe00de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "307/307 [==============================] - 0s 1ms/step\n",
            "+-----------------+---------------------+---------------------+---------------------+\n",
            "| Sentiment Class |      Precision      |        Recall       |       F1 Score      |\n",
            "+-----------------+---------------------+---------------------+---------------------+\n",
            "|     Negative    | 0.48119692680954307 | 0.37327478042659973 | 0.42042042042042044 |\n",
            "|     Neutral     | 0.44250780437044746 |  0.5187557182067704 |  0.4776077495437316 |\n",
            "|     Positive    |  0.6080964685615848 |  0.6354635463546354 |  0.6214788732394366 |\n",
            "+-----------------+---------------------+---------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Assuming y_test[:9800] represents the true labels for the first 9800 samples\n",
        "# Modify it accordingly if your true labels cover a different range\n",
        "\n",
        "sentiment_mapping = {1: 'Neutral', 0: 'Negative', 2: 'Positive'}\n",
        "\n",
        "predicted_probs = model.predict(X_t)\n",
        "predicted_labels = np.argmax(predicted_probs, axis=1)\n",
        "\n",
        "# Compute precision, recall, and f1 score for each class\n",
        "precision = precision_score(y_test[:9800], predicted_labels, average=None)\n",
        "recall = recall_score(y_test[:9800], predicted_labels, average=None)\n",
        "f1 = f1_score(y_test[:9800], predicted_labels, average=None)\n",
        "\n",
        "# Create a table\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Sentiment Class\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "\n",
        "# Populate the table with data\n",
        "for i in range(len(precision)):\n",
        "    sentiment_class = sentiment_mapping[i]\n",
        "    table.add_row([sentiment_class, precision[i], recall[i], f1[i]])\n",
        "\n",
        "# Print the table\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU0KMxL8qabk",
        "outputId": "9789bb37-5861-4b70-c1fc-42e97a784139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 0s 1ms/step\n",
            "293/293 [==============================] - 0s 1ms/step\n",
            "Scores for 'the teacher' aspect:\n",
            "+-------------------+---------------------+---------------------+---------------------+\n",
            "| the teacher Class |      Precision      |        Recall       |       F1 Score      |\n",
            "+-------------------+---------------------+---------------------+---------------------+\n",
            "|      Class 0      |  0.5526315789473685 | 0.44366197183098594 | 0.49218750000000006 |\n",
            "|      Class 1      | 0.48847926267281105 |  0.7066666666666667 |  0.5776566757493188 |\n",
            "|      Class 2      |  0.5047619047619047 |  0.3680555555555556 | 0.42570281124497994 |\n",
            "+-------------------+---------------------+---------------------+---------------------+\n",
            "\n",
            "Scores for 'the course' aspect:\n",
            "+------------------+---------------------+--------------------+---------------------+\n",
            "| the course Class |      Precision      |       Recall       |       F1 Score      |\n",
            "+------------------+---------------------+--------------------+---------------------+\n",
            "|     Class 0      | 0.47774480712166173 | 0.3699934340118188 | 0.41702127659574467 |\n",
            "|     Class 1      |  0.4397573752412462 | 0.509747523170342  | 0.47217288336293667 |\n",
            "|     Class 2      |  0.6113084665482534 | 0.6475384132957039 |  0.6289020861885184 |\n",
            "+------------------+---------------------+--------------------+---------------------+\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Separate data based on the aspect\n",
        "y_true_aspect_1, y_true_aspect_2 = [], []\n",
        "x_true_aspect_1, x_true_aspect_2 = [], []\n",
        "index = 0\n",
        "tt = X_test['aspect'].tolist()\n",
        "\n",
        "y_test = y_test.tolist()\n",
        "for id in range(9800):  # Check if the index is within bounds\n",
        "    if tt[id] == 'the teacher':\n",
        "        x_true_aspect_1.append(X_t[id])\n",
        "        y_true_aspect_1.append(y_test[id])\n",
        "    else:\n",
        "        x_true_aspect_2.append(X_t[id])\n",
        "        y_true_aspect_2.append(y_test[id])\n",
        "\n",
        "\n",
        "# Predict labels for each aspect\n",
        "predicted_probs = model.predict(np.array(x_true_aspect_1))\n",
        "predicted_labels_teacher = np.argmax(predicted_probs, axis=1)\n",
        "predicted_probs = model.predict(np.array(x_true_aspect_2))\n",
        "predicted_labels_course = np.argmax(predicted_probs, axis=1)\n",
        "\n",
        "# Function to calculate and print scores\n",
        "def print_scores(y_true, predicted_labels, aspect_name):\n",
        "    precision = precision_score(y_true, predicted_labels, average=None)\n",
        "    recall = recall_score(y_true, predicted_labels, average=None)\n",
        "    f1 = f1_score(y_true, predicted_labels, average=None)\n",
        "\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [f\"{aspect_name} Class\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
        "\n",
        "    for i in range(len(precision)):\n",
        "        table.add_row([f'Class {i}', precision[i], recall[i], f1[i]])\n",
        "\n",
        "    print(table)\n",
        "\n",
        "# Assuming you have predicted_labels_teacher and predicted_labels_course defined somewhere in your code\n",
        "\n",
        "# Print scores for 'the teacher' aspect\n",
        "print(\"Scores for 'the teacher' aspect:\")\n",
        "print_scores(y_true_aspect_1, predicted_labels_teacher, 'the teacher')\n",
        "\n",
        "# Print scores for 'the course' aspect\n",
        "print(\"\\nScores for 'the course' aspect:\")\n",
        "print_scores(y_true_aspect_2, predicted_labels_course, 'the course')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9PC34AkuOKK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPXDm52FuVFx"
      },
      "source": [
        "# Use bert to extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en2yLoaYJtSj"
      },
      "outputs": [],
      "source": [
        "x_tt = X_test['processed_review'].tolist()\n",
        "x_tt = x_tt[:9800]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfoQRBTcut7a"
      },
      "outputs": [],
      "source": [
        "x = X_train['processed_review'].tolist()\n",
        "x = x[:39400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B-Tdud-uf62",
        "outputId": "4ca50870-30ba-4d8f-e81a-52561eb0d5cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "BATCH_SIZE=512#@param{type\"slider\",min:64,max:1024,step:16}\n",
        "if model.device ==\"cpu\":\n",
        "  raise ValueError(\"Cuda not set up properly\")\n",
        "\n",
        "\n",
        "# Define a function for batching and processing\n",
        "def process_batch(texts, max_length=512, batch_size=BATCH_SIZE ,title='train_bert'):\n",
        "    tokenized_texts = [tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length) for text in texts]\n",
        "    max_len = max(len(tokens) for tokens in tokenized_texts)\n",
        "    padded_texts = [tokens + [0] * (max_len - len(tokens)) for tokens in tokenized_texts]\n",
        "    input_ids = torch.tensor(padded_texts)\n",
        "\n",
        "    # Process in batches with tqdm\n",
        "    batches = [input_ids[i:i+batch_size] for i in range(39400, len(input_ids), batch_size)]\n",
        "    bert_embeddings = []\n",
        "    id =0\n",
        "    # Add tqdm for progress bar\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for batch in tqdm(batches, desc=\"Processing batches\", leave=False):\n",
        "          outputs = model(batch.to(device))\n",
        "          with open(f'/content/drive/MyDrive/ASC/dataset/{title}/{id}.pkl', 'wb') as f:\n",
        "             pickle.dump(outputs.last_hidden_state, f)\n",
        "          id +=1\n",
        "\n",
        "    return\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MoTCAZeQb3O"
      },
      "outputs": [],
      "source": [
        "def load_hidden_state_batch(filepaths):\n",
        "    results = []\n",
        "\n",
        "    for filepath in tqdm(filepaths, desc=\"Loading hidden state batches\"):\n",
        "        result = load_from_pickle(filepath).to(dtype=torch.float16)\n",
        "        # torch.cuda.empty_cache()  # Empty cache to release any unused memory\n",
        "        result = result.to(torch.device(\"cpu\")).detach().numpy()\n",
        "        results.append(result)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX5pOu6RKbBi"
      },
      "source": [
        "**train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZU8yt4cKZPB"
      },
      "outputs": [],
      "source": [
        "process_batch(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6997c082ed4d453287a9ba5bc738ec21",
            "6672f81a6f2143ff8b720f7baf095e72",
            "2d0eee1f3b0c4c9ea7acd838110b3af3",
            "8634adaf3d244e19a5e6c8c8062c1dd8",
            "2dd4df77a60246d9911e01458611f31e",
            "8144ebf2d7474238aeb4c3d3e9acf831",
            "7a2c28d304c34be1a49af630d75be4cb",
            "2e8536677b3d469fb430d4b1f78be004",
            "dccbb9b9da3f4c28b998a0e24359545d",
            "1622398cd29a43608dcad4e38acf8ee4",
            "4b09c76daa7c4051bdeb489e3b2a3ab7"
          ]
        },
        "id": "IiDmzslv7Ufy",
        "outputId": "58784126-83f4-4651-e339-aeda556d8a40"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6997c082ed4d453287a9ba5bc738ec21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading hidden state batches:   0%|          | 0/77 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filepaths = [f\"/content/drive/MyDrive/ASC/dataset/train_bert/{i}.pkl\" for i in range(77)]\n",
        "X_bert_train = load_hidden_state_batch(filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue_qTacaQ99p"
      },
      "outputs": [],
      "source": [
        "X_bert_train = np.concatenate(X_bert_train, axis=0)\n",
        "gc.collect()\n",
        "X_bert_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItR64hjpRq3V"
      },
      "outputs": [],
      "source": [
        "X_bert_train = X_bert_train.reshape((39400, 393216))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP0uAqCMR3uT"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/ASC/dataset/train_bert.pkl', 'wb') as f:\n",
        "    pickle.dump(X_bert_train, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkPY_p8eShx5"
      },
      "outputs": [],
      "source": [
        "X_bert_train = []\n",
        "with (open('/content/drive/MyDrive/ASC/dataset/train_bert.pkl', \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_bert_train.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLue93j_SvQI"
      },
      "outputs": [],
      "source": [
        "X_bert_train = np.array(X_bert_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YV-_q6FTghA"
      },
      "outputs": [],
      "source": [
        "X_bert_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx1h5bD8Thn7"
      },
      "outputs": [],
      "source": [
        "X_parallel.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmapkSVxT184"
      },
      "outputs": [],
      "source": [
        "chunk_size = 1000\n",
        "num_chunks = (X_bert_train.shape[0] + chunk_size - 1) // chunk_size\n",
        "\n",
        "X_bert_concat = np.empty((X_bert_train.shape[0], X_bert_train.shape[1] + X_parallel.shape[1]), dtype=X_bert_train.dtype)\n",
        "\n",
        "for i in range(num_chunks):\n",
        "    start = i * chunk_size\n",
        "    end = min(start + chunk_size, X_bert_train.shape[0])\n",
        "\n",
        "    X_chunk = np.concatenate((X_bert_train[start:end], X_parallel[start:end]), axis=1)  # Concatenate corresponding chunks\n",
        "    X_bert_concat[start:end] = X_chunk\n",
        "del X_bert_train\n",
        "del X_parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3rYOXtYUs7m"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/ASC/dataset/final_train_bert.pkl', 'wb') as f:\n",
        "#     pickle.dump(X_bert_concat, f)\n",
        "# del X_bert_concat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzqo1xM1U6ZP"
      },
      "outputs": [],
      "source": [
        "X_final_train_bert = []\n",
        "with (open('/content/drive/MyDrive/ASC/dataset/final_train_bert.pkl', \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_final_train_bert.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYxm4BK2VOec"
      },
      "outputs": [],
      "source": [
        "X_final_train_bert = np.array(X_final_train_bert[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZLK80QQVRIw",
        "outputId": "f9565ca0-b4b8-4073-bd37-a3923fb410c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(39400, 393816)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_final_train_bert.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuuOwJDbKdcl"
      },
      "source": [
        "**test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55,
          "referenced_widgets": [
            "87494cf6e61d435f9fb78ce6aa8372f5",
            "77b0ddec8b8748fd8c49df83609bb1ce",
            "bafcfb5170d548f5abe39d5ecaf108e3",
            "725539b6439343ddb2ba0785560bd571",
            "84ff62e3589048d7ab5c6f53b2b1685b",
            "7990c8e071084241809c543684a0af21",
            "b6546da356c249a9977f8dd40d766f43",
            "3d03644918284653b3dae16568d91157",
            "919f5881244649aaa284c3853433a2f2",
            "e9add8dd026f49a19f874527851a68b5",
            "30fda049dfc341adb4758cd70dd13c4f"
          ]
        },
        "id": "WBPijts0QTXW",
        "outputId": "04f5b552-3d0a-4e7d-dd0b-858305037013"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87494cf6e61d435f9fb78ce6aa8372f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing batches:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        }
      ],
      "source": [
        "process_batch(np.concatenate((x, x_tt), axis=0) , title=\"test_bert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "446d9c7a37a942e2b58597b8570dd09b",
            "3cb03d51a5ee427699a655225ef00fa0",
            "7274f09a8f784b97ab326eb05e57d589",
            "d6efaae7539a418c98931e2ea57f04e0",
            "bef340b858054d16ae7cc19b419fa96e",
            "1419ebdd11ad459d8457a7a403f911ef",
            "2f1588d89b9040dd8cb1dd9ef64b53d5",
            "23d8a46406404615939dc08a271acdb1",
            "932c789225e342c8976e37ff1be24118",
            "1c72258392f540a3bd7873e11a056e41",
            "6d08a68af6e4469e967c14c1950866f2"
          ]
        },
        "id": "1_4pcEZhQkdn",
        "outputId": "3c2570fe-cf7c-412d-9e7e-0f74e3aa7fc0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "446d9c7a37a942e2b58597b8570dd09b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading hidden state batches:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "filepaths = [f\"/content/drive/MyDrive/ASC/dataset/test_bert/{i}.pkl\" for i in range(20)]\n",
        "X_bert_test = load_hidden_state_batch(filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhCdGpGSZnAF",
        "outputId": "bb14ffe6-2d4e-43e8-ce6b-f6295194d545"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_bert_test[0][0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6VI4pJS9MFw",
        "outputId": "c3f1e02c-e9df-4b71-cea6-1f1bc44c8567"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9800, 512, 768)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_bert_test = np.concatenate(X_bert_test, axis=0)\n",
        "gc.collect()\n",
        "X_bert_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwrMtAeqZZE0",
        "outputId": "7dee7380-314b-46eb-a0ba-0585c3e4f03d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "393216"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "512 * 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8lKxuLREKYw"
      },
      "outputs": [],
      "source": [
        "X_bert_test = X_bert_test.reshape((9800, 393216))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szs5LTspa2In"
      },
      "outputs": [],
      "source": [
        "# with open('/content/drive/MyDrive/ASC/dataset/test_bert.pkl', 'wb') as f:\n",
        "#     pickle.dump(X_bert_test, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NbSinMSlT6C"
      },
      "outputs": [],
      "source": [
        "X_bert_test = []\n",
        "with (open('/content/drive/MyDrive/ASC/dataset/test_bert.pkl', \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_bert_test.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM95bXwOluBb"
      },
      "outputs": [],
      "source": [
        "X_bert_test = np.array(X_bert_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvtagZi0jKsU",
        "outputId": "7c662538-13b8-4f78-f8e0-9d0c0db39f70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9800, 393216)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_bert_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hHkNg-nc-4G",
        "outputId": "9d13f2f6-909e-40e8-e6b6-5215fe11dd4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9800, 600)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTeQlm5cihxs"
      },
      "outputs": [],
      "source": [
        "chunk_size = 1000\n",
        "num_chunks = (X_bert_test.shape[0] + chunk_size - 1) // chunk_size\n",
        "\n",
        "X_bert_concat_t = np.empty((X_bert_test.shape[0], X_bert_test.shape[1] + X_t.shape[1]), dtype=X_bert_test.dtype)\n",
        "\n",
        "for i in range(num_chunks):\n",
        "    start = i * chunk_size\n",
        "    end = min(start + chunk_size, X_bert_test.shape[0])\n",
        "\n",
        "    X_chunk = np.concatenate((X_bert_test[start:end], X_t[start:end]), axis=1)  # Concatenate corresponding chunks\n",
        "    X_bert_concat_t[start:end] = X_chunk\n",
        "del X_bert_test\n",
        "del X_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl5lkmbM89iF"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/ASC/dataset/final_test_bert.pkl', 'wb') as f:\n",
        "    pickle.dump(X_bert_concat_t, f)\n",
        "del X_bert_concat_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzSLqXS5-qh4"
      },
      "outputs": [],
      "source": [
        "X_final_test_bert = []\n",
        "with (open('/content/drive/MyDrive/ASC/dataset/final_test_bert.pkl', \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            X_final_test_bert.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHSQCRUW_t3o"
      },
      "outputs": [],
      "source": [
        "X_final_test_bert = np.array(X_final_test_bert[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq2-DP9DAaJI",
        "outputId": "a67ceb4a-f926-492e-b6a7-f40a2bf96e25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(9800, 393816)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_final_test_bert.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxezKl6cF_Wp"
      },
      "source": [
        "# Deep Learning models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pk6CWPUV04f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D\n",
        "import os\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping,Callback,CSVLogger\n",
        "import gc\n",
        "class GarbageCollectorCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Reshape, Conv1D, GlobalAveragePooling1D, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOijCvqYgtxL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    with open('/content/drive/MyDrive/ASC/dataset/final_train_bert.pkl', \"rb\") as f:\n",
        "        X_final_train_bert = np.load(f, allow_pickle=True)\n",
        "except IOError:  # Handle potential errors during loading\n",
        "    X_final_train_bert = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qO00Dsw3t6a"
      },
      "source": [
        "# EXP1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm4fuBvdB0TA",
        "outputId": "1baa6a34-b3ca-400a-ec13-343d008ed1c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-32-5a790787681f>:48: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "138/138 [==============================] - 78s 557ms/step - loss: 5.9007 - accuracy: 0.3335 - val_loss: 6.0066 - val_accuracy: 0.3438\n",
            "Epoch 2/500\n",
            "138/138 [==============================] - 75s 549ms/step - loss: 2.7884 - accuracy: 0.3510 - val_loss: 1.0667 - val_accuracy: 0.4260\n",
            "Epoch 3/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 2.4026 - accuracy: 0.3642 - val_loss: 1.0871 - val_accuracy: 0.4305\n",
            "Epoch 4/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 1.2349 - accuracy: 0.3791 - val_loss: 1.2410 - val_accuracy: 0.4490\n",
            "Epoch 5/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 1.1358 - accuracy: 0.3953 - val_loss: 1.0376 - val_accuracy: 0.4568\n",
            "Epoch 6/500\n",
            "138/138 [==============================] - 74s 540ms/step - loss: 1.0751 - accuracy: 0.4187 - val_loss: 1.0243 - val_accuracy: 0.4750\n",
            "Epoch 7/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.1037 - accuracy: 0.4255 - val_loss: 1.0149 - val_accuracy: 0.4706\n",
            "Epoch 8/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 1.0672 - accuracy: 0.4289 - val_loss: 1.0911 - val_accuracy: 0.3591\n",
            "Epoch 9/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0443 - accuracy: 0.4467 - val_loss: 1.0048 - val_accuracy: 0.4906\n",
            "Epoch 10/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0431 - accuracy: 0.4576 - val_loss: 1.4745 - val_accuracy: 0.3352\n",
            "Epoch 11/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0528 - accuracy: 0.4619 - val_loss: 1.1686 - val_accuracy: 0.3609\n",
            "Epoch 12/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0428 - accuracy: 0.4648 - val_loss: 0.9737 - val_accuracy: 0.4747\n",
            "Epoch 13/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0283 - accuracy: 0.4718 - val_loss: 0.9355 - val_accuracy: 0.5359\n",
            "Epoch 14/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0351 - accuracy: 0.4735 - val_loss: 0.9900 - val_accuracy: 0.5672\n",
            "Epoch 15/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0279 - accuracy: 0.4731 - val_loss: 0.9685 - val_accuracy: 0.4839\n",
            "Epoch 16/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0394 - accuracy: 0.4730 - val_loss: 1.0862 - val_accuracy: 0.4253\n",
            "Epoch 17/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 1.0147 - accuracy: 0.4884 - val_loss: 1.0644 - val_accuracy: 0.4474\n",
            "Epoch 18/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 1.0132 - accuracy: 0.4993 - val_loss: 1.2597 - val_accuracy: 0.4036\n",
            "Epoch 19/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 1.0168 - accuracy: 0.5073 - val_loss: 0.9350 - val_accuracy: 0.5435\n",
            "Epoch 20/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9853 - accuracy: 0.5091 - val_loss: 0.9202 - val_accuracy: 0.5836\n",
            "Epoch 21/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9790 - accuracy: 0.5086 - val_loss: 0.9418 - val_accuracy: 0.4841\n",
            "Epoch 22/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9661 - accuracy: 0.5170 - val_loss: 0.8904 - val_accuracy: 0.5839\n",
            "Epoch 23/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9755 - accuracy: 0.5195 - val_loss: 1.2858 - val_accuracy: 0.4260\n",
            "Epoch 24/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9744 - accuracy: 0.5262 - val_loss: 0.9171 - val_accuracy: 0.5208\n",
            "Epoch 25/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9627 - accuracy: 0.5258 - val_loss: 0.9151 - val_accuracy: 0.5719\n",
            "Epoch 26/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9710 - accuracy: 0.5215 - val_loss: 0.9107 - val_accuracy: 0.5503\n",
            "Epoch 27/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9662 - accuracy: 0.5336 - val_loss: 0.9575 - val_accuracy: 0.4984\n",
            "Epoch 28/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9652 - accuracy: 0.5283 - val_loss: 0.8836 - val_accuracy: 0.5891\n",
            "Epoch 29/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9460 - accuracy: 0.5493 - val_loss: 0.8707 - val_accuracy: 0.5945\n",
            "Epoch 30/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9275 - accuracy: 0.5534 - val_loss: 0.8783 - val_accuracy: 0.6049\n",
            "Epoch 31/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9295 - accuracy: 0.5463 - val_loss: 0.9273 - val_accuracy: 0.5568\n",
            "Epoch 32/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.9622 - accuracy: 0.5427 - val_loss: 0.8938 - val_accuracy: 0.5734\n",
            "Epoch 33/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9215 - accuracy: 0.5563 - val_loss: 0.8678 - val_accuracy: 0.5839\n",
            "Epoch 34/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9430 - accuracy: 0.5422 - val_loss: 0.8534 - val_accuracy: 0.6167\n",
            "Epoch 35/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9083 - accuracy: 0.5612 - val_loss: 0.9507 - val_accuracy: 0.5435\n",
            "Epoch 36/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.9228 - accuracy: 0.5611 - val_loss: 0.8561 - val_accuracy: 0.5771\n",
            "Epoch 37/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.9114 - accuracy: 0.5646 - val_loss: 0.8692 - val_accuracy: 0.5984\n",
            "Epoch 38/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.8985 - accuracy: 0.5708 - val_loss: 0.8169 - val_accuracy: 0.6310\n",
            "Epoch 39/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9001 - accuracy: 0.5781 - val_loss: 0.8161 - val_accuracy: 0.6411\n",
            "Epoch 40/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9046 - accuracy: 0.5774 - val_loss: 0.8389 - val_accuracy: 0.6336\n",
            "Epoch 41/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8866 - accuracy: 0.5928 - val_loss: 1.0007 - val_accuracy: 0.5083\n",
            "Epoch 42/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9163 - accuracy: 0.5750 - val_loss: 0.8254 - val_accuracy: 0.6451\n",
            "Epoch 43/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.9014 - accuracy: 0.5848 - val_loss: 0.9427 - val_accuracy: 0.4966\n",
            "Epoch 44/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8913 - accuracy: 0.5745 - val_loss: 0.9746 - val_accuracy: 0.5125\n",
            "Epoch 45/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.8951 - accuracy: 0.5758 - val_loss: 0.8597 - val_accuracy: 0.5885\n",
            "Epoch 46/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8776 - accuracy: 0.5943 - val_loss: 1.9751 - val_accuracy: 0.3852\n",
            "Epoch 47/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.8852 - accuracy: 0.5906 - val_loss: 0.8150 - val_accuracy: 0.6633\n",
            "Epoch 48/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.8679 - accuracy: 0.6052 - val_loss: 0.8609 - val_accuracy: 0.6133\n",
            "Epoch 49/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8466 - accuracy: 0.6139 - val_loss: 0.8670 - val_accuracy: 0.5885\n",
            "Epoch 50/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8608 - accuracy: 0.6144 - val_loss: 0.8069 - val_accuracy: 0.6323\n",
            "Epoch 51/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8457 - accuracy: 0.6183 - val_loss: 0.8552 - val_accuracy: 0.6052\n",
            "Epoch 52/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.8509 - accuracy: 0.6204 - val_loss: 0.7934 - val_accuracy: 0.6344\n",
            "Epoch 53/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8636 - accuracy: 0.6111 - val_loss: 0.7828 - val_accuracy: 0.6711\n",
            "Epoch 54/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.8255 - accuracy: 0.6353 - val_loss: 1.4110 - val_accuracy: 0.4464\n",
            "Epoch 55/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8497 - accuracy: 0.6151 - val_loss: 0.8356 - val_accuracy: 0.5990\n",
            "Epoch 56/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8226 - accuracy: 0.6352 - val_loss: 1.0925 - val_accuracy: 0.5328\n",
            "Epoch 57/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8220 - accuracy: 0.6377 - val_loss: 0.8039 - val_accuracy: 0.6724\n",
            "Epoch 58/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8196 - accuracy: 0.6400 - val_loss: 0.8488 - val_accuracy: 0.5930\n",
            "Epoch 59/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.8205 - accuracy: 0.6360 - val_loss: 0.8451 - val_accuracy: 0.5706\n",
            "Epoch 60/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8176 - accuracy: 0.6356 - val_loss: 0.8178 - val_accuracy: 0.6253\n",
            "Epoch 61/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8193 - accuracy: 0.6435 - val_loss: 0.8330 - val_accuracy: 0.6081\n",
            "Epoch 62/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8105 - accuracy: 0.6523 - val_loss: 1.3798 - val_accuracy: 0.5036\n",
            "Epoch 63/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.8104 - accuracy: 0.6454 - val_loss: 0.7535 - val_accuracy: 0.6714\n",
            "Epoch 64/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.8067 - accuracy: 0.6460 - val_loss: 0.7210 - val_accuracy: 0.7141\n",
            "Epoch 65/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.8084 - accuracy: 0.6492 - val_loss: 0.9115 - val_accuracy: 0.5724\n",
            "Epoch 66/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7933 - accuracy: 0.6639 - val_loss: 0.7571 - val_accuracy: 0.6870\n",
            "Epoch 67/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.7930 - accuracy: 0.6549 - val_loss: 0.7646 - val_accuracy: 0.6953\n",
            "Epoch 68/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7901 - accuracy: 0.6587 - val_loss: 0.7727 - val_accuracy: 0.6474\n",
            "Epoch 69/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.8005 - accuracy: 0.6579 - val_loss: 0.7526 - val_accuracy: 0.6919\n",
            "Epoch 70/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7898 - accuracy: 0.6552 - val_loss: 0.7393 - val_accuracy: 0.7159\n",
            "Epoch 71/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7944 - accuracy: 0.6580 - val_loss: 0.7180 - val_accuracy: 0.7128\n",
            "Epoch 72/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7747 - accuracy: 0.6673 - val_loss: 0.7384 - val_accuracy: 0.6938\n",
            "Epoch 73/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7818 - accuracy: 0.6640 - val_loss: 0.7264 - val_accuracy: 0.7135\n",
            "Epoch 74/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7671 - accuracy: 0.6725 - val_loss: 0.7044 - val_accuracy: 0.7133\n",
            "Epoch 75/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7686 - accuracy: 0.6723 - val_loss: 0.8240 - val_accuracy: 0.6164\n",
            "Epoch 76/500\n",
            "138/138 [==============================] - 74s 540ms/step - loss: 0.7759 - accuracy: 0.6707 - val_loss: 0.7238 - val_accuracy: 0.7130\n",
            "Epoch 77/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7558 - accuracy: 0.6765 - val_loss: 0.7081 - val_accuracy: 0.7156\n",
            "Epoch 78/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7706 - accuracy: 0.6732 - val_loss: 0.7246 - val_accuracy: 0.7057\n",
            "Epoch 79/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7615 - accuracy: 0.6743 - val_loss: 1.0857 - val_accuracy: 0.5279\n",
            "Epoch 80/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7623 - accuracy: 0.6805 - val_loss: 0.6941 - val_accuracy: 0.7148\n",
            "Epoch 81/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7630 - accuracy: 0.6771 - val_loss: 0.7704 - val_accuracy: 0.6865\n",
            "Epoch 82/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7596 - accuracy: 0.6777 - val_loss: 0.8152 - val_accuracy: 0.6591\n",
            "Epoch 83/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7618 - accuracy: 0.6765 - val_loss: 0.8640 - val_accuracy: 0.6078\n",
            "Epoch 84/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7547 - accuracy: 0.6849 - val_loss: 0.7513 - val_accuracy: 0.6794\n",
            "Epoch 85/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7441 - accuracy: 0.6870 - val_loss: 0.8676 - val_accuracy: 0.5891\n",
            "Epoch 86/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7587 - accuracy: 0.6834 - val_loss: 0.7310 - val_accuracy: 0.6701\n",
            "Epoch 87/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7373 - accuracy: 0.6940 - val_loss: 0.6909 - val_accuracy: 0.7148\n",
            "Epoch 88/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7472 - accuracy: 0.6831 - val_loss: 0.8493 - val_accuracy: 0.6672\n",
            "Epoch 89/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7452 - accuracy: 0.6850 - val_loss: 0.6804 - val_accuracy: 0.7273\n",
            "Epoch 90/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7476 - accuracy: 0.6895 - val_loss: 0.7400 - val_accuracy: 0.6763\n",
            "Epoch 91/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7393 - accuracy: 0.6863 - val_loss: 0.6907 - val_accuracy: 0.7271\n",
            "Epoch 92/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7431 - accuracy: 0.6918 - val_loss: 0.7087 - val_accuracy: 0.7143\n",
            "Epoch 93/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7388 - accuracy: 0.6928 - val_loss: 0.7575 - val_accuracy: 0.6823\n",
            "Epoch 94/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7450 - accuracy: 0.6875 - val_loss: 0.6929 - val_accuracy: 0.7185\n",
            "Epoch 95/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7423 - accuracy: 0.6855 - val_loss: 0.7213 - val_accuracy: 0.7008\n",
            "Epoch 96/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7319 - accuracy: 0.6907 - val_loss: 0.7108 - val_accuracy: 0.6997\n",
            "Epoch 97/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7406 - accuracy: 0.6915 - val_loss: 0.7009 - val_accuracy: 0.7154\n",
            "Epoch 98/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7325 - accuracy: 0.6949 - val_loss: 0.6786 - val_accuracy: 0.7281\n",
            "Epoch 99/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7355 - accuracy: 0.6979 - val_loss: 0.6765 - val_accuracy: 0.7302\n",
            "Epoch 100/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7398 - accuracy: 0.6872 - val_loss: 0.6910 - val_accuracy: 0.7245\n",
            "Epoch 101/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7240 - accuracy: 0.6983 - val_loss: 0.7206 - val_accuracy: 0.7013\n",
            "Epoch 102/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7319 - accuracy: 0.6917 - val_loss: 0.7116 - val_accuracy: 0.7133\n",
            "Epoch 103/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7274 - accuracy: 0.6953 - val_loss: 0.7235 - val_accuracy: 0.6958\n",
            "Epoch 104/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7232 - accuracy: 0.6961 - val_loss: 0.6726 - val_accuracy: 0.7281\n",
            "Epoch 105/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7518 - accuracy: 0.6907 - val_loss: 0.6834 - val_accuracy: 0.7247\n",
            "Epoch 106/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7248 - accuracy: 0.6955 - val_loss: 0.6625 - val_accuracy: 0.7359\n",
            "Epoch 107/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7182 - accuracy: 0.6978 - val_loss: 0.7641 - val_accuracy: 0.6893\n",
            "Epoch 108/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7180 - accuracy: 0.6971 - val_loss: 0.6741 - val_accuracy: 0.7279\n",
            "Epoch 109/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7055 - accuracy: 0.7071 - val_loss: 0.6634 - val_accuracy: 0.7312\n",
            "Epoch 110/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7224 - accuracy: 0.7001 - val_loss: 0.7140 - val_accuracy: 0.6992\n",
            "Epoch 111/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7173 - accuracy: 0.6989 - val_loss: 0.7032 - val_accuracy: 0.7211\n",
            "Epoch 112/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7145 - accuracy: 0.7025 - val_loss: 0.6476 - val_accuracy: 0.7437\n",
            "Epoch 113/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7109 - accuracy: 0.7026 - val_loss: 0.6534 - val_accuracy: 0.7383\n",
            "Epoch 114/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7141 - accuracy: 0.7023 - val_loss: 0.9731 - val_accuracy: 0.5607\n",
            "Epoch 115/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7140 - accuracy: 0.7030 - val_loss: 0.6736 - val_accuracy: 0.7307\n",
            "Epoch 116/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7126 - accuracy: 0.7079 - val_loss: 0.7896 - val_accuracy: 0.6469\n",
            "Epoch 117/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7063 - accuracy: 0.7043 - val_loss: 0.6778 - val_accuracy: 0.7268\n",
            "Epoch 118/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7146 - accuracy: 0.7012 - val_loss: 0.6602 - val_accuracy: 0.7349\n",
            "Epoch 119/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7178 - accuracy: 0.7096 - val_loss: 0.6573 - val_accuracy: 0.7339\n",
            "Epoch 120/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7036 - accuracy: 0.7087 - val_loss: 0.7518 - val_accuracy: 0.6833\n",
            "Epoch 121/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7076 - accuracy: 0.7077 - val_loss: 0.7291 - val_accuracy: 0.7057\n",
            "Epoch 122/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7237 - accuracy: 0.7023 - val_loss: 0.7623 - val_accuracy: 0.6549\n",
            "Epoch 123/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6963 - accuracy: 0.7125 - val_loss: 0.6859 - val_accuracy: 0.7284\n",
            "Epoch 124/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7157 - accuracy: 0.7050 - val_loss: 0.6546 - val_accuracy: 0.7440\n",
            "Epoch 125/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.7013 - accuracy: 0.7088 - val_loss: 0.7010 - val_accuracy: 0.7076\n",
            "Epoch 126/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.7019 - accuracy: 0.7089 - val_loss: 0.7281 - val_accuracy: 0.6763\n",
            "Epoch 127/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6967 - accuracy: 0.7129 - val_loss: 0.7652 - val_accuracy: 0.6859\n",
            "Epoch 128/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7068 - accuracy: 0.7094 - val_loss: 0.9393 - val_accuracy: 0.6234\n",
            "Epoch 129/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7034 - accuracy: 0.7092 - val_loss: 0.7202 - val_accuracy: 0.6893\n",
            "Epoch 130/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6856 - accuracy: 0.7150 - val_loss: 0.7012 - val_accuracy: 0.7182\n",
            "Epoch 131/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6973 - accuracy: 0.7130 - val_loss: 0.6547 - val_accuracy: 0.7422\n",
            "Epoch 132/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.7247 - accuracy: 0.7085 - val_loss: 0.6506 - val_accuracy: 0.7474\n",
            "Epoch 133/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6744 - accuracy: 0.7216 - val_loss: 0.6629 - val_accuracy: 0.7297\n",
            "Epoch 134/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6865 - accuracy: 0.7169 - val_loss: 0.9185 - val_accuracy: 0.5990\n",
            "Epoch 135/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6887 - accuracy: 0.7151 - val_loss: 0.6255 - val_accuracy: 0.7448\n",
            "Epoch 136/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6800 - accuracy: 0.7185 - val_loss: 0.6842 - val_accuracy: 0.7177\n",
            "Epoch 137/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6811 - accuracy: 0.7174 - val_loss: 0.7355 - val_accuracy: 0.6695\n",
            "Epoch 138/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6895 - accuracy: 0.7125 - val_loss: 0.6877 - val_accuracy: 0.7258\n",
            "Epoch 139/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6875 - accuracy: 0.7175 - val_loss: 0.6436 - val_accuracy: 0.7409\n",
            "Epoch 140/500\n",
            "138/138 [==============================] - 74s 540ms/step - loss: 0.6916 - accuracy: 0.7118 - val_loss: 0.6560 - val_accuracy: 0.7487\n",
            "Epoch 141/500\n",
            "138/138 [==============================] - 74s 540ms/step - loss: 0.6750 - accuracy: 0.7207 - val_loss: 0.7369 - val_accuracy: 0.6922\n",
            "Epoch 142/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6799 - accuracy: 0.7188 - val_loss: 0.6260 - val_accuracy: 0.7521\n",
            "Epoch 143/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6927 - accuracy: 0.7188 - val_loss: 0.6507 - val_accuracy: 0.7419\n",
            "Epoch 144/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6770 - accuracy: 0.7230 - val_loss: 0.6698 - val_accuracy: 0.7237\n",
            "Epoch 145/500\n",
            "138/138 [==============================] - 74s 540ms/step - loss: 0.6759 - accuracy: 0.7212 - val_loss: 0.6419 - val_accuracy: 0.7427\n",
            "Epoch 146/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6901 - accuracy: 0.7208 - val_loss: 0.6441 - val_accuracy: 0.7437\n",
            "Epoch 147/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6770 - accuracy: 0.7232 - val_loss: 0.6476 - val_accuracy: 0.7380\n",
            "Epoch 148/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6818 - accuracy: 0.7224 - val_loss: 0.6901 - val_accuracy: 0.7117\n",
            "Epoch 149/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6809 - accuracy: 0.7215 - val_loss: 0.7258 - val_accuracy: 0.7083\n",
            "Epoch 150/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6789 - accuracy: 0.7213 - val_loss: 0.6518 - val_accuracy: 0.7430\n",
            "Epoch 151/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6821 - accuracy: 0.7197 - val_loss: 0.6565 - val_accuracy: 0.7396\n",
            "Epoch 152/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6857 - accuracy: 0.7245 - val_loss: 0.6744 - val_accuracy: 0.7255\n",
            "Epoch 153/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6731 - accuracy: 0.7267 - val_loss: 0.6268 - val_accuracy: 0.7503\n",
            "Epoch 154/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6663 - accuracy: 0.7263 - val_loss: 0.6202 - val_accuracy: 0.7536\n",
            "Epoch 155/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6774 - accuracy: 0.7245 - val_loss: 0.7863 - val_accuracy: 0.6625\n",
            "Epoch 156/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6876 - accuracy: 0.7199 - val_loss: 0.6664 - val_accuracy: 0.7273\n",
            "Epoch 157/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6733 - accuracy: 0.7215 - val_loss: 0.8416 - val_accuracy: 0.6424\n",
            "Epoch 158/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6778 - accuracy: 0.7212 - val_loss: 1.1135 - val_accuracy: 0.4734\n",
            "Epoch 159/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6758 - accuracy: 0.7223 - val_loss: 0.7014 - val_accuracy: 0.7411\n",
            "Epoch 160/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6630 - accuracy: 0.7280 - val_loss: 0.7880 - val_accuracy: 0.6490\n",
            "Epoch 161/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6716 - accuracy: 0.7214 - val_loss: 0.6567 - val_accuracy: 0.7388\n",
            "Epoch 162/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6705 - accuracy: 0.7268 - val_loss: 0.6373 - val_accuracy: 0.7487\n",
            "Epoch 163/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6656 - accuracy: 0.7263 - val_loss: 0.6692 - val_accuracy: 0.7310\n",
            "Epoch 164/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6703 - accuracy: 0.7242 - val_loss: 0.8057 - val_accuracy: 0.6620\n",
            "Epoch 165/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6722 - accuracy: 0.7249 - val_loss: 0.9388 - val_accuracy: 0.6094\n",
            "Epoch 166/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6725 - accuracy: 0.7263 - val_loss: 0.6717 - val_accuracy: 0.7315\n",
            "Epoch 167/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6797 - accuracy: 0.7205 - val_loss: 0.6379 - val_accuracy: 0.7451\n",
            "Epoch 168/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6717 - accuracy: 0.7238 - val_loss: 0.6087 - val_accuracy: 0.7573\n",
            "Epoch 169/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6725 - accuracy: 0.7207 - val_loss: 0.7112 - val_accuracy: 0.7029\n",
            "Epoch 170/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6696 - accuracy: 0.7266 - val_loss: 0.8070 - val_accuracy: 0.6690\n",
            "Epoch 171/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6666 - accuracy: 0.7244 - val_loss: 0.6266 - val_accuracy: 0.7581\n",
            "Epoch 172/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6653 - accuracy: 0.7288 - val_loss: 0.6682 - val_accuracy: 0.7237\n",
            "Epoch 173/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6622 - accuracy: 0.7282 - val_loss: 0.6487 - val_accuracy: 0.7448\n",
            "Epoch 174/500\n",
            "138/138 [==============================] - 73s 534ms/step - loss: 0.6621 - accuracy: 0.7310 - val_loss: 0.6949 - val_accuracy: 0.7164\n",
            "Epoch 175/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6581 - accuracy: 0.7302 - val_loss: 0.6221 - val_accuracy: 0.7474\n",
            "Epoch 176/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6737 - accuracy: 0.7236 - val_loss: 0.6333 - val_accuracy: 0.7516\n",
            "Epoch 177/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6522 - accuracy: 0.7318 - val_loss: 0.6173 - val_accuracy: 0.7615\n",
            "Epoch 178/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6618 - accuracy: 0.7280 - val_loss: 0.7964 - val_accuracy: 0.6661\n",
            "Epoch 179/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6616 - accuracy: 0.7279 - val_loss: 0.7319 - val_accuracy: 0.7172\n",
            "Epoch 180/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6615 - accuracy: 0.7279 - val_loss: 0.6265 - val_accuracy: 0.7508\n",
            "Epoch 181/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6594 - accuracy: 0.7288 - val_loss: 0.6469 - val_accuracy: 0.7419\n",
            "Epoch 182/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6492 - accuracy: 0.7322 - val_loss: 0.6161 - val_accuracy: 0.7547\n",
            "Epoch 183/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6618 - accuracy: 0.7311 - val_loss: 0.6129 - val_accuracy: 0.7591\n",
            "Epoch 184/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6560 - accuracy: 0.7328 - val_loss: 0.8180 - val_accuracy: 0.6531\n",
            "Epoch 185/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6625 - accuracy: 0.7305 - val_loss: 0.6288 - val_accuracy: 0.7477\n",
            "Epoch 186/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6549 - accuracy: 0.7349 - val_loss: 0.6680 - val_accuracy: 0.7250\n",
            "Epoch 187/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6606 - accuracy: 0.7299 - val_loss: 0.6434 - val_accuracy: 0.7422\n",
            "Epoch 188/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6486 - accuracy: 0.7338 - val_loss: 0.6307 - val_accuracy: 0.7516\n",
            "Epoch 189/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6582 - accuracy: 0.7290 - val_loss: 0.7470 - val_accuracy: 0.6909\n",
            "Epoch 190/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6582 - accuracy: 0.7316 - val_loss: 0.6493 - val_accuracy: 0.7326\n",
            "Epoch 191/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6524 - accuracy: 0.7359 - val_loss: 0.7103 - val_accuracy: 0.7104\n",
            "Epoch 192/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6480 - accuracy: 0.7376 - val_loss: 0.6090 - val_accuracy: 0.7638\n",
            "Epoch 193/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6501 - accuracy: 0.7362 - val_loss: 0.6972 - val_accuracy: 0.7229\n",
            "Epoch 194/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6509 - accuracy: 0.7376 - val_loss: 0.6050 - val_accuracy: 0.7625\n",
            "Epoch 195/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6496 - accuracy: 0.7341 - val_loss: 0.6934 - val_accuracy: 0.7031\n",
            "Epoch 196/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6538 - accuracy: 0.7340 - val_loss: 0.6156 - val_accuracy: 0.7620\n",
            "Epoch 197/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6453 - accuracy: 0.7392 - val_loss: 0.6156 - val_accuracy: 0.7518\n",
            "Epoch 198/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6611 - accuracy: 0.7356 - val_loss: 0.6077 - val_accuracy: 0.7648\n",
            "Epoch 199/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6499 - accuracy: 0.7340 - val_loss: 0.6585 - val_accuracy: 0.7326\n",
            "Epoch 200/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6458 - accuracy: 0.7351 - val_loss: 0.7158 - val_accuracy: 0.6904\n",
            "Epoch 201/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6443 - accuracy: 0.7374 - val_loss: 0.6366 - val_accuracy: 0.7414\n",
            "Epoch 202/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6437 - accuracy: 0.7335 - val_loss: 0.7139 - val_accuracy: 0.7135\n",
            "Epoch 203/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6551 - accuracy: 0.7344 - val_loss: 0.6401 - val_accuracy: 0.7448\n",
            "Epoch 204/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6589 - accuracy: 0.7316 - val_loss: 0.6130 - val_accuracy: 0.7568\n",
            "Epoch 205/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6386 - accuracy: 0.7425 - val_loss: 0.6268 - val_accuracy: 0.7492\n",
            "Epoch 206/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6465 - accuracy: 0.7365 - val_loss: 0.7810 - val_accuracy: 0.6604\n",
            "Epoch 207/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6470 - accuracy: 0.7365 - val_loss: 0.9852 - val_accuracy: 0.5919\n",
            "Epoch 208/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6393 - accuracy: 0.7417 - val_loss: 0.6403 - val_accuracy: 0.7385\n",
            "Epoch 209/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6461 - accuracy: 0.7368 - val_loss: 0.6427 - val_accuracy: 0.7461\n",
            "Epoch 210/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6414 - accuracy: 0.7359 - val_loss: 0.6133 - val_accuracy: 0.7646\n",
            "Epoch 211/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6523 - accuracy: 0.7367 - val_loss: 0.8271 - val_accuracy: 0.6706\n",
            "Epoch 212/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6432 - accuracy: 0.7387 - val_loss: 0.6087 - val_accuracy: 0.7638\n",
            "Epoch 213/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6444 - accuracy: 0.7396 - val_loss: 0.7739 - val_accuracy: 0.6922\n",
            "Epoch 214/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6384 - accuracy: 0.7424 - val_loss: 0.9798 - val_accuracy: 0.6242\n",
            "Epoch 215/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6513 - accuracy: 0.7371 - val_loss: 0.6241 - val_accuracy: 0.7547\n",
            "Epoch 216/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6458 - accuracy: 0.7403 - val_loss: 1.0882 - val_accuracy: 0.5810\n",
            "Epoch 217/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6483 - accuracy: 0.7370 - val_loss: 0.6979 - val_accuracy: 0.7076\n",
            "Epoch 218/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6360 - accuracy: 0.7414 - val_loss: 0.8760 - val_accuracy: 0.6542\n",
            "Epoch 219/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6355 - accuracy: 0.7397 - val_loss: 0.7082 - val_accuracy: 0.7180\n",
            "Epoch 220/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6352 - accuracy: 0.7450 - val_loss: 1.3411 - val_accuracy: 0.5247\n",
            "Epoch 221/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6494 - accuracy: 0.7386 - val_loss: 0.7400 - val_accuracy: 0.6878\n",
            "Epoch 222/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6343 - accuracy: 0.7421 - val_loss: 0.6741 - val_accuracy: 0.7169\n",
            "Epoch 223/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6437 - accuracy: 0.7389 - val_loss: 0.6265 - val_accuracy: 0.7589\n",
            "Epoch 224/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6308 - accuracy: 0.7446 - val_loss: 0.6799 - val_accuracy: 0.7195\n",
            "Epoch 225/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6396 - accuracy: 0.7412 - val_loss: 0.7110 - val_accuracy: 0.7161\n",
            "Epoch 226/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6335 - accuracy: 0.7446 - val_loss: 0.6099 - val_accuracy: 0.7667\n",
            "Epoch 227/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6447 - accuracy: 0.7415 - val_loss: 0.6129 - val_accuracy: 0.7576\n",
            "Epoch 228/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6280 - accuracy: 0.7460 - val_loss: 0.6603 - val_accuracy: 0.7307\n",
            "Epoch 229/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6286 - accuracy: 0.7447 - val_loss: 0.6657 - val_accuracy: 0.7266\n",
            "Epoch 230/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6342 - accuracy: 0.7409 - val_loss: 0.6045 - val_accuracy: 0.7706\n",
            "Epoch 231/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6335 - accuracy: 0.7418 - val_loss: 0.6183 - val_accuracy: 0.7669\n",
            "Epoch 232/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6344 - accuracy: 0.7416 - val_loss: 0.9534 - val_accuracy: 0.6323\n",
            "Epoch 233/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6322 - accuracy: 0.7454 - val_loss: 0.6399 - val_accuracy: 0.7445\n",
            "Epoch 234/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6301 - accuracy: 0.7464 - val_loss: 0.6329 - val_accuracy: 0.7573\n",
            "Epoch 235/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6422 - accuracy: 0.7407 - val_loss: 0.6198 - val_accuracy: 0.7607\n",
            "Epoch 236/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6317 - accuracy: 0.7443 - val_loss: 0.6561 - val_accuracy: 0.7380\n",
            "Epoch 237/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6284 - accuracy: 0.7457 - val_loss: 0.6771 - val_accuracy: 0.7260\n",
            "Epoch 238/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6315 - accuracy: 0.7442 - val_loss: 0.6892 - val_accuracy: 0.7120\n",
            "Epoch 239/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6267 - accuracy: 0.7472 - val_loss: 0.6065 - val_accuracy: 0.7555\n",
            "Epoch 240/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6234 - accuracy: 0.7481 - val_loss: 0.7987 - val_accuracy: 0.6581\n",
            "Epoch 241/500\n",
            "138/138 [==============================] - 73s 534ms/step - loss: 0.6299 - accuracy: 0.7444 - val_loss: 0.5962 - val_accuracy: 0.7659\n",
            "Epoch 242/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6279 - accuracy: 0.7456 - val_loss: 0.6159 - val_accuracy: 0.7617\n",
            "Epoch 243/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6251 - accuracy: 0.7476 - val_loss: 0.6953 - val_accuracy: 0.7146\n",
            "Epoch 244/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6203 - accuracy: 0.7487 - val_loss: 0.6009 - val_accuracy: 0.7677\n",
            "Epoch 245/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6249 - accuracy: 0.7476 - val_loss: 0.6162 - val_accuracy: 0.7565\n",
            "Epoch 246/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6372 - accuracy: 0.7451 - val_loss: 0.6514 - val_accuracy: 0.7427\n",
            "Epoch 247/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6296 - accuracy: 0.7441 - val_loss: 0.6136 - val_accuracy: 0.7625\n",
            "Epoch 248/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6342 - accuracy: 0.7464 - val_loss: 0.6215 - val_accuracy: 0.7565\n",
            "Epoch 249/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6332 - accuracy: 0.7441 - val_loss: 0.6231 - val_accuracy: 0.7552\n",
            "Epoch 250/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6239 - accuracy: 0.7462 - val_loss: 0.6384 - val_accuracy: 0.7391\n",
            "Epoch 251/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6224 - accuracy: 0.7473 - val_loss: 0.7720 - val_accuracy: 0.6628\n",
            "Epoch 252/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6285 - accuracy: 0.7456 - val_loss: 0.6100 - val_accuracy: 0.7563\n",
            "Epoch 253/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6189 - accuracy: 0.7493 - val_loss: 0.8202 - val_accuracy: 0.6521\n",
            "Epoch 254/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6224 - accuracy: 0.7453 - val_loss: 0.6095 - val_accuracy: 0.7576\n",
            "Epoch 255/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6222 - accuracy: 0.7469 - val_loss: 0.7146 - val_accuracy: 0.6977\n",
            "Epoch 256/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6284 - accuracy: 0.7436 - val_loss: 0.6000 - val_accuracy: 0.7607\n",
            "Epoch 257/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6123 - accuracy: 0.7499 - val_loss: 0.6058 - val_accuracy: 0.7617\n",
            "Epoch 258/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6174 - accuracy: 0.7513 - val_loss: 0.6709 - val_accuracy: 0.7305\n",
            "Epoch 259/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6219 - accuracy: 0.7463 - val_loss: 0.5988 - val_accuracy: 0.7667\n",
            "Epoch 260/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6117 - accuracy: 0.7504 - val_loss: 0.7346 - val_accuracy: 0.7003\n",
            "Epoch 261/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6233 - accuracy: 0.7476 - val_loss: 0.6378 - val_accuracy: 0.7445\n",
            "Epoch 262/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6173 - accuracy: 0.7493 - val_loss: 0.6942 - val_accuracy: 0.7013\n",
            "Epoch 263/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6143 - accuracy: 0.7512 - val_loss: 0.6620 - val_accuracy: 0.7289\n",
            "Epoch 264/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6135 - accuracy: 0.7521 - val_loss: 0.8690 - val_accuracy: 0.6292\n",
            "Epoch 265/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6219 - accuracy: 0.7486 - val_loss: 0.6602 - val_accuracy: 0.7427\n",
            "Epoch 266/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6156 - accuracy: 0.7510 - val_loss: 1.0475 - val_accuracy: 0.6021\n",
            "Epoch 267/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6138 - accuracy: 0.7516 - val_loss: 0.6139 - val_accuracy: 0.7583\n",
            "Epoch 268/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6272 - accuracy: 0.7505 - val_loss: 0.6708 - val_accuracy: 0.7279\n",
            "Epoch 269/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6240 - accuracy: 0.7459 - val_loss: 0.6260 - val_accuracy: 0.7568\n",
            "Epoch 270/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6318 - accuracy: 0.7456 - val_loss: 0.6988 - val_accuracy: 0.7031\n",
            "Epoch 271/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6169 - accuracy: 0.7523 - val_loss: 0.6680 - val_accuracy: 0.7307\n",
            "Epoch 272/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6191 - accuracy: 0.7503 - val_loss: 0.6699 - val_accuracy: 0.7375\n",
            "Epoch 273/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6112 - accuracy: 0.7534 - val_loss: 0.6121 - val_accuracy: 0.7672\n",
            "Epoch 274/500\n",
            "138/138 [==============================] - 73s 534ms/step - loss: 0.6112 - accuracy: 0.7513 - val_loss: 0.7229 - val_accuracy: 0.7039\n",
            "Epoch 275/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6198 - accuracy: 0.7460 - val_loss: 0.6437 - val_accuracy: 0.7521\n",
            "Epoch 276/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6122 - accuracy: 0.7530 - val_loss: 0.7892 - val_accuracy: 0.6659\n",
            "Epoch 277/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6152 - accuracy: 0.7521 - val_loss: 0.8471 - val_accuracy: 0.6286\n",
            "Epoch 278/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6092 - accuracy: 0.7548 - val_loss: 0.6367 - val_accuracy: 0.7422\n",
            "Epoch 279/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6204 - accuracy: 0.7503 - val_loss: 0.7108 - val_accuracy: 0.7125\n",
            "Epoch 280/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6147 - accuracy: 0.7537 - val_loss: 0.5805 - val_accuracy: 0.7703\n",
            "Epoch 281/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6074 - accuracy: 0.7533 - val_loss: 0.6786 - val_accuracy: 0.7411\n",
            "Epoch 282/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.6117 - accuracy: 0.7523 - val_loss: 0.6332 - val_accuracy: 0.7417\n",
            "Epoch 283/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6134 - accuracy: 0.7539 - val_loss: 0.6223 - val_accuracy: 0.7448\n",
            "Epoch 284/500\n",
            "138/138 [==============================] - 74s 539ms/step - loss: 0.6078 - accuracy: 0.7554 - val_loss: 0.7719 - val_accuracy: 0.6875\n",
            "Epoch 285/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6138 - accuracy: 0.7538 - val_loss: 0.6701 - val_accuracy: 0.7188\n",
            "Epoch 286/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6144 - accuracy: 0.7503 - val_loss: 0.7589 - val_accuracy: 0.6904\n",
            "Epoch 287/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6047 - accuracy: 0.7548 - val_loss: 0.5959 - val_accuracy: 0.7706\n",
            "Epoch 288/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6104 - accuracy: 0.7506 - val_loss: 0.5930 - val_accuracy: 0.7641\n",
            "Epoch 289/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6044 - accuracy: 0.7548 - val_loss: 0.6904 - val_accuracy: 0.7568\n",
            "Epoch 290/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6060 - accuracy: 0.7539 - val_loss: 0.5867 - val_accuracy: 0.7716\n",
            "Epoch 291/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6059 - accuracy: 0.7553 - val_loss: 0.6027 - val_accuracy: 0.7688\n",
            "Epoch 292/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6050 - accuracy: 0.7546 - val_loss: 0.6678 - val_accuracy: 0.7432\n",
            "Epoch 293/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6096 - accuracy: 0.7540 - val_loss: 0.6354 - val_accuracy: 0.7565\n",
            "Epoch 294/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6111 - accuracy: 0.7514 - val_loss: 0.7160 - val_accuracy: 0.6966\n",
            "Epoch 295/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6038 - accuracy: 0.7541 - val_loss: 0.6024 - val_accuracy: 0.7648\n",
            "Epoch 296/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6073 - accuracy: 0.7509 - val_loss: 0.6248 - val_accuracy: 0.7490\n",
            "Epoch 297/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6100 - accuracy: 0.7571 - val_loss: 0.6908 - val_accuracy: 0.7294\n",
            "Epoch 298/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6037 - accuracy: 0.7535 - val_loss: 0.6254 - val_accuracy: 0.7458\n",
            "Epoch 299/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5980 - accuracy: 0.7587 - val_loss: 0.6473 - val_accuracy: 0.7451\n",
            "Epoch 300/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6125 - accuracy: 0.7539 - val_loss: 0.9145 - val_accuracy: 0.6036\n",
            "Epoch 301/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6144 - accuracy: 0.7541 - val_loss: 0.6559 - val_accuracy: 0.7375\n",
            "Epoch 302/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6076 - accuracy: 0.7537 - val_loss: 0.7765 - val_accuracy: 0.7003\n",
            "Epoch 303/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6116 - accuracy: 0.7515 - val_loss: 0.6146 - val_accuracy: 0.7612\n",
            "Epoch 304/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6161 - accuracy: 0.7508 - val_loss: 0.6081 - val_accuracy: 0.7672\n",
            "Epoch 305/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6041 - accuracy: 0.7557 - val_loss: 0.7452 - val_accuracy: 0.7018\n",
            "Epoch 306/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6152 - accuracy: 0.7508 - val_loss: 0.6482 - val_accuracy: 0.7367\n",
            "Epoch 307/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5940 - accuracy: 0.7584 - val_loss: 0.7768 - val_accuracy: 0.7060\n",
            "Epoch 308/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6058 - accuracy: 0.7562 - val_loss: 0.8151 - val_accuracy: 0.6576\n",
            "Epoch 309/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6029 - accuracy: 0.7579 - val_loss: 1.1723 - val_accuracy: 0.5846\n",
            "Epoch 310/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6095 - accuracy: 0.7526 - val_loss: 0.5958 - val_accuracy: 0.7654\n",
            "Epoch 311/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5942 - accuracy: 0.7634 - val_loss: 0.6571 - val_accuracy: 0.7435\n",
            "Epoch 312/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6089 - accuracy: 0.7545 - val_loss: 0.6624 - val_accuracy: 0.7320\n",
            "Epoch 313/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6044 - accuracy: 0.7570 - val_loss: 0.6359 - val_accuracy: 0.7469\n",
            "Epoch 314/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6036 - accuracy: 0.7538 - val_loss: 0.6441 - val_accuracy: 0.7422\n",
            "Epoch 315/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6099 - accuracy: 0.7525 - val_loss: 0.6871 - val_accuracy: 0.7404\n",
            "Epoch 316/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6005 - accuracy: 0.7577 - val_loss: 0.5986 - val_accuracy: 0.7698\n",
            "Epoch 317/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5985 - accuracy: 0.7631 - val_loss: 0.6349 - val_accuracy: 0.7404\n",
            "Epoch 318/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6030 - accuracy: 0.7582 - val_loss: 0.7751 - val_accuracy: 0.6823\n",
            "Epoch 319/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6019 - accuracy: 0.7578 - val_loss: 0.7568 - val_accuracy: 0.7060\n",
            "Epoch 320/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5923 - accuracy: 0.7616 - val_loss: 0.6835 - val_accuracy: 0.7385\n",
            "Epoch 321/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5998 - accuracy: 0.7576 - val_loss: 0.6709 - val_accuracy: 0.7240\n",
            "Epoch 322/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5978 - accuracy: 0.7594 - val_loss: 0.6050 - val_accuracy: 0.7742\n",
            "Epoch 323/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5988 - accuracy: 0.7590 - val_loss: 0.6458 - val_accuracy: 0.7518\n",
            "Epoch 324/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5957 - accuracy: 0.7618 - val_loss: 0.6593 - val_accuracy: 0.7336\n",
            "Epoch 325/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6075 - accuracy: 0.7600 - val_loss: 0.6219 - val_accuracy: 0.7539\n",
            "Epoch 326/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5945 - accuracy: 0.7589 - val_loss: 0.6621 - val_accuracy: 0.7570\n",
            "Epoch 327/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6012 - accuracy: 0.7567 - val_loss: 0.6623 - val_accuracy: 0.7255\n",
            "Epoch 328/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6083 - accuracy: 0.7556 - val_loss: 0.6098 - val_accuracy: 0.7672\n",
            "Epoch 329/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6122 - accuracy: 0.7555 - val_loss: 0.7548 - val_accuracy: 0.6870\n",
            "Epoch 330/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5954 - accuracy: 0.7632 - val_loss: 0.6643 - val_accuracy: 0.7365\n",
            "Epoch 331/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6205 - accuracy: 0.7536 - val_loss: 0.5867 - val_accuracy: 0.7740\n",
            "Epoch 332/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5935 - accuracy: 0.7594 - val_loss: 0.6206 - val_accuracy: 0.7560\n",
            "Epoch 333/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5947 - accuracy: 0.7615 - val_loss: 0.8423 - val_accuracy: 0.6672\n",
            "Epoch 334/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6034 - accuracy: 0.7571 - val_loss: 0.5836 - val_accuracy: 0.7737\n",
            "Epoch 335/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5996 - accuracy: 0.7562 - val_loss: 0.6034 - val_accuracy: 0.7695\n",
            "Epoch 336/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5993 - accuracy: 0.7623 - val_loss: 1.5695 - val_accuracy: 0.4906\n",
            "Epoch 337/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6037 - accuracy: 0.7569 - val_loss: 0.8022 - val_accuracy: 0.6849\n",
            "Epoch 338/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6035 - accuracy: 0.7574 - val_loss: 0.6536 - val_accuracy: 0.7388\n",
            "Epoch 339/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5949 - accuracy: 0.7590 - val_loss: 0.6564 - val_accuracy: 0.7573\n",
            "Epoch 340/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6041 - accuracy: 0.7582 - val_loss: 0.7325 - val_accuracy: 0.7023\n",
            "Epoch 341/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6040 - accuracy: 0.7556 - val_loss: 0.5989 - val_accuracy: 0.7654\n",
            "Epoch 342/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5964 - accuracy: 0.7596 - val_loss: 0.6210 - val_accuracy: 0.7555\n",
            "Epoch 343/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5985 - accuracy: 0.7612 - val_loss: 0.6046 - val_accuracy: 0.7667\n",
            "Epoch 344/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5936 - accuracy: 0.7624 - val_loss: 0.6149 - val_accuracy: 0.7615\n",
            "Epoch 345/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6025 - accuracy: 0.7584 - val_loss: 0.5815 - val_accuracy: 0.7794\n",
            "Epoch 346/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6058 - accuracy: 0.7612 - val_loss: 0.6001 - val_accuracy: 0.7552\n",
            "Epoch 347/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5933 - accuracy: 0.7623 - val_loss: 0.7863 - val_accuracy: 0.6956\n",
            "Epoch 348/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6154 - accuracy: 0.7526 - val_loss: 0.6048 - val_accuracy: 0.7638\n",
            "Epoch 349/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5958 - accuracy: 0.7628 - val_loss: 0.6461 - val_accuracy: 0.7422\n",
            "Epoch 350/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.6040 - accuracy: 0.7557 - val_loss: 0.5983 - val_accuracy: 0.7758\n",
            "Epoch 351/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5980 - accuracy: 0.7625 - val_loss: 0.8115 - val_accuracy: 0.6932\n",
            "Epoch 352/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5932 - accuracy: 0.7602 - val_loss: 0.6087 - val_accuracy: 0.7526\n",
            "Epoch 353/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5960 - accuracy: 0.7598 - val_loss: 0.5939 - val_accuracy: 0.7682\n",
            "Epoch 354/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6020 - accuracy: 0.7546 - val_loss: 0.6122 - val_accuracy: 0.7505\n",
            "Epoch 355/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5811 - accuracy: 0.7647 - val_loss: 0.6499 - val_accuracy: 0.7477\n",
            "Epoch 356/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5987 - accuracy: 0.7560 - val_loss: 0.6437 - val_accuracy: 0.7638\n",
            "Epoch 357/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5957 - accuracy: 0.7605 - val_loss: 0.7174 - val_accuracy: 0.7125\n",
            "Epoch 358/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6545 - accuracy: 0.7654 - val_loss: 0.6014 - val_accuracy: 0.7638\n",
            "Epoch 359/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6075 - accuracy: 0.7559 - val_loss: 0.5871 - val_accuracy: 0.7797\n",
            "Epoch 360/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5900 - accuracy: 0.7633 - val_loss: 0.6084 - val_accuracy: 0.7724\n",
            "Epoch 361/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5938 - accuracy: 0.7613 - val_loss: 0.7026 - val_accuracy: 0.7409\n",
            "Epoch 362/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6050 - accuracy: 0.7614 - val_loss: 0.6291 - val_accuracy: 0.7622\n",
            "Epoch 363/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6070 - accuracy: 0.7575 - val_loss: 0.6006 - val_accuracy: 0.7576\n",
            "Epoch 364/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5890 - accuracy: 0.7630 - val_loss: 0.7937 - val_accuracy: 0.7307\n",
            "Epoch 365/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6049 - accuracy: 0.7582 - val_loss: 0.6376 - val_accuracy: 0.7635\n",
            "Epoch 366/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5945 - accuracy: 0.7624 - val_loss: 0.7039 - val_accuracy: 0.7297\n",
            "Epoch 367/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5988 - accuracy: 0.7595 - val_loss: 0.5925 - val_accuracy: 0.7755\n",
            "Epoch 368/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.6093 - accuracy: 0.7573 - val_loss: 0.5851 - val_accuracy: 0.7656\n",
            "Epoch 369/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5731 - accuracy: 0.7706 - val_loss: 0.8079 - val_accuracy: 0.6945\n",
            "Epoch 370/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6051 - accuracy: 0.7555 - val_loss: 0.5780 - val_accuracy: 0.7784\n",
            "Epoch 371/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5956 - accuracy: 0.7613 - val_loss: 0.6115 - val_accuracy: 0.7742\n",
            "Epoch 372/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5988 - accuracy: 0.7593 - val_loss: 0.6604 - val_accuracy: 0.7430\n",
            "Epoch 373/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6099 - accuracy: 0.7542 - val_loss: 0.6478 - val_accuracy: 0.7576\n",
            "Epoch 374/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5911 - accuracy: 0.7639 - val_loss: 0.6238 - val_accuracy: 0.7630\n",
            "Epoch 375/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6016 - accuracy: 0.7616 - val_loss: 0.6757 - val_accuracy: 0.7312\n",
            "Epoch 376/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6009 - accuracy: 0.7593 - val_loss: 0.6245 - val_accuracy: 0.7586\n",
            "Epoch 377/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5961 - accuracy: 0.7628 - val_loss: 0.6279 - val_accuracy: 0.7492\n",
            "Epoch 378/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5933 - accuracy: 0.7626 - val_loss: 0.6211 - val_accuracy: 0.7724\n",
            "Epoch 379/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6165 - accuracy: 0.7534 - val_loss: 0.5880 - val_accuracy: 0.7771\n",
            "Epoch 380/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5891 - accuracy: 0.7636 - val_loss: 0.6365 - val_accuracy: 0.7578\n",
            "Epoch 381/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5945 - accuracy: 0.7600 - val_loss: 0.6716 - val_accuracy: 0.7333\n",
            "Epoch 382/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5887 - accuracy: 0.7649 - val_loss: 0.6318 - val_accuracy: 0.7589\n",
            "Epoch 383/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5974 - accuracy: 0.7645 - val_loss: 0.5753 - val_accuracy: 0.7810\n",
            "Epoch 384/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5982 - accuracy: 0.7602 - val_loss: 0.6171 - val_accuracy: 0.7633\n",
            "Epoch 385/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5984 - accuracy: 0.7574 - val_loss: 0.5805 - val_accuracy: 0.7768\n",
            "Epoch 386/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5941 - accuracy: 0.7606 - val_loss: 0.6106 - val_accuracy: 0.7771\n",
            "Epoch 387/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6019 - accuracy: 0.7577 - val_loss: 0.7099 - val_accuracy: 0.7250\n",
            "Epoch 388/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5941 - accuracy: 0.7650 - val_loss: 0.6206 - val_accuracy: 0.7677\n",
            "Epoch 389/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5990 - accuracy: 0.7615 - val_loss: 0.6214 - val_accuracy: 0.7568\n",
            "Epoch 390/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6117 - accuracy: 0.7607 - val_loss: 0.5985 - val_accuracy: 0.7677\n",
            "Epoch 391/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5925 - accuracy: 0.7610 - val_loss: 0.5862 - val_accuracy: 0.7797\n",
            "Epoch 392/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.5928 - accuracy: 0.7630 - val_loss: 0.6489 - val_accuracy: 0.7471\n",
            "Epoch 393/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5915 - accuracy: 0.7645 - val_loss: 0.5944 - val_accuracy: 0.7810\n",
            "Epoch 394/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6091 - accuracy: 0.7568 - val_loss: 0.7051 - val_accuracy: 0.7375\n",
            "Epoch 395/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6005 - accuracy: 0.7612 - val_loss: 0.9209 - val_accuracy: 0.6057\n",
            "Epoch 396/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5992 - accuracy: 0.7632 - val_loss: 0.6814 - val_accuracy: 0.7560\n",
            "Epoch 397/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5949 - accuracy: 0.7640 - val_loss: 0.5984 - val_accuracy: 0.7711\n",
            "Epoch 398/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5881 - accuracy: 0.7650 - val_loss: 0.7485 - val_accuracy: 0.6693\n",
            "Epoch 399/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5929 - accuracy: 0.7609 - val_loss: 0.6558 - val_accuracy: 0.7440\n",
            "Epoch 400/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5992 - accuracy: 0.7629 - val_loss: 0.7208 - val_accuracy: 0.7440\n",
            "Epoch 401/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5886 - accuracy: 0.7623 - val_loss: 0.6025 - val_accuracy: 0.7576\n",
            "Epoch 402/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.6007 - accuracy: 0.7596 - val_loss: 0.6130 - val_accuracy: 0.7690\n",
            "Epoch 403/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5813 - accuracy: 0.7677 - val_loss: 0.5839 - val_accuracy: 0.7701\n",
            "Epoch 404/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5940 - accuracy: 0.7626 - val_loss: 0.6425 - val_accuracy: 0.7555\n",
            "Epoch 405/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5937 - accuracy: 0.7584 - val_loss: 0.7420 - val_accuracy: 0.7453\n",
            "Epoch 406/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5890 - accuracy: 0.7650 - val_loss: 0.6120 - val_accuracy: 0.7727\n",
            "Epoch 407/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5896 - accuracy: 0.7634 - val_loss: 0.6280 - val_accuracy: 0.7534\n",
            "Epoch 408/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5944 - accuracy: 0.7620 - val_loss: 0.7398 - val_accuracy: 0.7130\n",
            "Epoch 409/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5869 - accuracy: 0.7633 - val_loss: 0.6301 - val_accuracy: 0.7552\n",
            "Epoch 410/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5958 - accuracy: 0.7613 - val_loss: 0.8480 - val_accuracy: 0.6753\n",
            "Epoch 411/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5895 - accuracy: 0.7643 - val_loss: 0.5963 - val_accuracy: 0.7768\n",
            "Epoch 412/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5908 - accuracy: 0.7637 - val_loss: 0.5854 - val_accuracy: 0.7776\n",
            "Epoch 413/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5805 - accuracy: 0.7684 - val_loss: 0.6400 - val_accuracy: 0.7552\n",
            "Epoch 414/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5995 - accuracy: 0.7583 - val_loss: 0.5977 - val_accuracy: 0.7766\n",
            "Epoch 415/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5940 - accuracy: 0.7637 - val_loss: 0.8069 - val_accuracy: 0.6979\n",
            "Epoch 416/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5948 - accuracy: 0.7613 - val_loss: 0.6526 - val_accuracy: 0.7646\n",
            "Epoch 417/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5938 - accuracy: 0.7614 - val_loss: 0.6037 - val_accuracy: 0.7641\n",
            "Epoch 418/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.5903 - accuracy: 0.7624 - val_loss: 0.6476 - val_accuracy: 0.7378\n",
            "Epoch 419/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.5847 - accuracy: 0.7670 - val_loss: 0.6193 - val_accuracy: 0.7654\n",
            "Epoch 420/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.6014 - accuracy: 0.7596 - val_loss: 0.5972 - val_accuracy: 0.7727\n",
            "Epoch 421/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5816 - accuracy: 0.7697 - val_loss: 0.6337 - val_accuracy: 0.7651\n",
            "Epoch 422/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5863 - accuracy: 0.7635 - val_loss: 0.6931 - val_accuracy: 0.7180\n",
            "Epoch 423/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5933 - accuracy: 0.7598 - val_loss: 0.6537 - val_accuracy: 0.7552\n",
            "Epoch 424/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5840 - accuracy: 0.7649 - val_loss: 0.6175 - val_accuracy: 0.7513\n",
            "Epoch 425/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5807 - accuracy: 0.7700 - val_loss: 0.8800 - val_accuracy: 0.6469\n",
            "Epoch 426/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5875 - accuracy: 0.7652 - val_loss: 0.7397 - val_accuracy: 0.6971\n",
            "Epoch 427/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5790 - accuracy: 0.7710 - val_loss: 0.6780 - val_accuracy: 0.7188\n",
            "Epoch 428/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5971 - accuracy: 0.7628 - val_loss: 0.5902 - val_accuracy: 0.7815\n",
            "Epoch 429/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5745 - accuracy: 0.7702 - val_loss: 0.5796 - val_accuracy: 0.7758\n",
            "Epoch 430/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5841 - accuracy: 0.7672 - val_loss: 0.6016 - val_accuracy: 0.7724\n",
            "Epoch 431/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5899 - accuracy: 0.7670 - val_loss: 0.9447 - val_accuracy: 0.5977\n",
            "Epoch 432/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5800 - accuracy: 0.7683 - val_loss: 0.5965 - val_accuracy: 0.7779\n",
            "Epoch 433/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5764 - accuracy: 0.7689 - val_loss: 0.7475 - val_accuracy: 0.6961\n",
            "Epoch 434/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5745 - accuracy: 0.7718 - val_loss: 0.6248 - val_accuracy: 0.7661\n",
            "Epoch 435/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5781 - accuracy: 0.7665 - val_loss: 0.6853 - val_accuracy: 0.7122\n",
            "Epoch 436/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5678 - accuracy: 0.7726 - val_loss: 0.5996 - val_accuracy: 0.7763\n",
            "Epoch 437/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5867 - accuracy: 0.7659 - val_loss: 0.6116 - val_accuracy: 0.7740\n",
            "Epoch 438/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5841 - accuracy: 0.7686 - val_loss: 0.7449 - val_accuracy: 0.7044\n",
            "Epoch 439/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5812 - accuracy: 0.7661 - val_loss: 0.7217 - val_accuracy: 0.7000\n",
            "Epoch 440/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5832 - accuracy: 0.7659 - val_loss: 0.6036 - val_accuracy: 0.7734\n",
            "Epoch 441/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5798 - accuracy: 0.7676 - val_loss: 0.6919 - val_accuracy: 0.7406\n",
            "Epoch 442/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5761 - accuracy: 0.7680 - val_loss: 0.5934 - val_accuracy: 0.7635\n",
            "Epoch 443/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5791 - accuracy: 0.7662 - val_loss: 0.5917 - val_accuracy: 0.7779\n",
            "Epoch 444/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5780 - accuracy: 0.7696 - val_loss: 0.6291 - val_accuracy: 0.7656\n",
            "Epoch 445/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5687 - accuracy: 0.7735 - val_loss: 0.6857 - val_accuracy: 0.7737\n",
            "Epoch 446/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5853 - accuracy: 0.7646 - val_loss: 0.5968 - val_accuracy: 0.7742\n",
            "Epoch 447/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5848 - accuracy: 0.7665 - val_loss: 0.6153 - val_accuracy: 0.7680\n",
            "Epoch 448/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5694 - accuracy: 0.7734 - val_loss: 0.5949 - val_accuracy: 0.7750\n",
            "Epoch 449/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5785 - accuracy: 0.7682 - val_loss: 0.5889 - val_accuracy: 0.7784\n",
            "Epoch 450/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5786 - accuracy: 0.7718 - val_loss: 0.7442 - val_accuracy: 0.7529\n",
            "Epoch 451/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5747 - accuracy: 0.7701 - val_loss: 0.6165 - val_accuracy: 0.7659\n",
            "Epoch 452/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5844 - accuracy: 0.7660 - val_loss: 0.7029 - val_accuracy: 0.7352\n",
            "Epoch 453/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5800 - accuracy: 0.7679 - val_loss: 0.5763 - val_accuracy: 0.7878\n",
            "Epoch 454/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5799 - accuracy: 0.7684 - val_loss: 0.6405 - val_accuracy: 0.7474\n",
            "Epoch 455/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5819 - accuracy: 0.7708 - val_loss: 0.7734 - val_accuracy: 0.6857\n",
            "Epoch 456/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5792 - accuracy: 0.7660 - val_loss: 0.8947 - val_accuracy: 0.6503\n",
            "Epoch 457/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5717 - accuracy: 0.7716 - val_loss: 0.5788 - val_accuracy: 0.7784\n",
            "Epoch 458/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5803 - accuracy: 0.7701 - val_loss: 0.6400 - val_accuracy: 0.7565\n",
            "Epoch 459/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5690 - accuracy: 0.7753 - val_loss: 0.6323 - val_accuracy: 0.7719\n",
            "Epoch 460/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5711 - accuracy: 0.7704 - val_loss: 0.6441 - val_accuracy: 0.7443\n",
            "Epoch 461/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5780 - accuracy: 0.7665 - val_loss: 0.6707 - val_accuracy: 0.7708\n",
            "Epoch 462/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5732 - accuracy: 0.7716 - val_loss: 0.6494 - val_accuracy: 0.7633\n",
            "Epoch 463/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5843 - accuracy: 0.7685 - val_loss: 0.6100 - val_accuracy: 0.7690\n",
            "Epoch 464/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5746 - accuracy: 0.7701 - val_loss: 0.6112 - val_accuracy: 0.7570\n",
            "Epoch 465/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5769 - accuracy: 0.7711 - val_loss: 0.5893 - val_accuracy: 0.7570\n",
            "Epoch 466/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5728 - accuracy: 0.7708 - val_loss: 0.6966 - val_accuracy: 0.7336\n",
            "Epoch 467/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5831 - accuracy: 0.7696 - val_loss: 0.5833 - val_accuracy: 0.7797\n",
            "Epoch 468/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5728 - accuracy: 0.7716 - val_loss: 0.6120 - val_accuracy: 0.7745\n",
            "Epoch 469/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5759 - accuracy: 0.7723 - val_loss: 0.6058 - val_accuracy: 0.7685\n",
            "Epoch 470/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5770 - accuracy: 0.7710 - val_loss: 0.6188 - val_accuracy: 0.7628\n",
            "Epoch 471/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5816 - accuracy: 0.7688 - val_loss: 0.7132 - val_accuracy: 0.7000\n",
            "Epoch 472/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5755 - accuracy: 0.7716 - val_loss: 0.6657 - val_accuracy: 0.7445\n",
            "Epoch 473/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5743 - accuracy: 0.7696 - val_loss: 0.5909 - val_accuracy: 0.7669\n",
            "Epoch 474/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5999 - accuracy: 0.7622 - val_loss: 0.6024 - val_accuracy: 0.7833\n",
            "Epoch 475/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5817 - accuracy: 0.7671 - val_loss: 0.6404 - val_accuracy: 0.7437\n",
            "Epoch 476/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5630 - accuracy: 0.7765 - val_loss: 0.7003 - val_accuracy: 0.7487\n",
            "Epoch 477/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5705 - accuracy: 0.7728 - val_loss: 0.6867 - val_accuracy: 0.7266\n",
            "Epoch 478/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5727 - accuracy: 0.7682 - val_loss: 0.5889 - val_accuracy: 0.7763\n",
            "Epoch 479/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5603 - accuracy: 0.7798 - val_loss: 0.7906 - val_accuracy: 0.7305\n",
            "Epoch 480/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5867 - accuracy: 0.7657 - val_loss: 0.6400 - val_accuracy: 0.7607\n",
            "Epoch 481/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5749 - accuracy: 0.7747 - val_loss: 0.6067 - val_accuracy: 0.7768\n",
            "Epoch 482/500\n",
            "138/138 [==============================] - 73s 535ms/step - loss: 0.5776 - accuracy: 0.7704 - val_loss: 0.5819 - val_accuracy: 0.7771\n",
            "Epoch 483/500\n",
            "138/138 [==============================] - 73s 536ms/step - loss: 0.5748 - accuracy: 0.7725 - val_loss: 0.5753 - val_accuracy: 0.7810\n",
            "Epoch 484/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5680 - accuracy: 0.7743 - val_loss: 0.7481 - val_accuracy: 0.7229\n",
            "Epoch 485/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5912 - accuracy: 0.7652 - val_loss: 0.6718 - val_accuracy: 0.7260\n",
            "Epoch 486/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5937 - accuracy: 0.7647 - val_loss: 0.5794 - val_accuracy: 0.7857\n",
            "Epoch 487/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5757 - accuracy: 0.7742 - val_loss: 0.5935 - val_accuracy: 0.7786\n",
            "Epoch 488/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5730 - accuracy: 0.7731 - val_loss: 0.5863 - val_accuracy: 0.7792\n",
            "Epoch 489/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5872 - accuracy: 0.7680 - val_loss: 0.6051 - val_accuracy: 0.7620\n",
            "Epoch 490/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5814 - accuracy: 0.7698 - val_loss: 0.5820 - val_accuracy: 0.7820\n",
            "Epoch 491/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5918 - accuracy: 0.7642 - val_loss: 0.6426 - val_accuracy: 0.7669\n",
            "Epoch 492/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5801 - accuracy: 0.7702 - val_loss: 0.6980 - val_accuracy: 0.7523\n",
            "Epoch 493/500\n",
            "138/138 [==============================] - 74s 538ms/step - loss: 0.5872 - accuracy: 0.7680 - val_loss: 0.7307 - val_accuracy: 0.7102\n",
            "Epoch 494/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5707 - accuracy: 0.7743 - val_loss: 0.9433 - val_accuracy: 0.6391\n",
            "Epoch 495/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5882 - accuracy: 0.7689 - val_loss: 0.6260 - val_accuracy: 0.7570\n",
            "Epoch 496/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5794 - accuracy: 0.7704 - val_loss: 0.6449 - val_accuracy: 0.7612\n",
            "Epoch 497/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5846 - accuracy: 0.7672 - val_loss: 0.9681 - val_accuracy: 0.6193\n",
            "Epoch 498/500\n",
            "138/138 [==============================] - 74s 536ms/step - loss: 0.5854 - accuracy: 0.7687 - val_loss: 0.6834 - val_accuracy: 0.7417\n",
            "Epoch 499/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5845 - accuracy: 0.7677 - val_loss: 0.5981 - val_accuracy: 0.7703\n",
            "Epoch 500/500\n",
            "138/138 [==============================] - 74s 537ms/step - loss: 0.5773 - accuracy: 0.7701 - val_loss: 0.6122 - val_accuracy: 0.7758\n"
          ]
        }
      ],
      "source": [
        "def generator(data, labels, batch_size):\n",
        "  while True:\n",
        "    indices = np.random.permutation(len(data))\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      yield data[batch_indices], labels[batch_indices]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(4, input_shape=(393816,), activation='linear'),\n",
        "])\n",
        "\n",
        "for _ in range(4):\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/exp1/'\n",
        "if not os.path.exists(checkpoint_filepath):\n",
        "  os.mkdir(checkpoint_filepath)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'model_weights_epoch{epoch:02d}_val_acc{val_accuracy:.4f}.h5'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback, history_logger_1]\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "X_train = X_final_train_bert[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_final_train_bert[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=500,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=callbacks_list_1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1IP3QcI3pYV"
      },
      "source": [
        "# EXP2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0qDnzXyXNM_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6OYfwZk3n43",
        "outputId": "ba0bf1cb-8646-4459-fd60-f06b1caa1fdc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-b7e99e6f9de4>:49: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "138/138 [==============================] - 109s 768ms/step - loss: 2.8541 - accuracy: 0.3352 - val_loss: 1.3848 - val_accuracy: 0.3411\n",
            "Epoch 2/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 1.3196 - accuracy: 0.3610 - val_loss: 1.0393 - val_accuracy: 0.4688\n",
            "Epoch 3/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 1.2025 - accuracy: 0.3835 - val_loss: 1.0675 - val_accuracy: 0.4135\n",
            "Epoch 4/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 1.0973 - accuracy: 0.4141 - val_loss: 1.0384 - val_accuracy: 0.4503\n",
            "Epoch 5/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 1.0721 - accuracy: 0.4254 - val_loss: 1.0532 - val_accuracy: 0.4396\n",
            "Epoch 6/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 1.0511 - accuracy: 0.4417 - val_loss: 0.9975 - val_accuracy: 0.4734\n",
            "Epoch 7/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 1.0493 - accuracy: 0.4443 - val_loss: 0.9941 - val_accuracy: 0.4945\n",
            "Epoch 8/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 1.0594 - accuracy: 0.4411 - val_loss: 1.0141 - val_accuracy: 0.4982\n",
            "Epoch 9/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 1.0358 - accuracy: 0.4509 - val_loss: 0.9649 - val_accuracy: 0.4549\n",
            "Epoch 10/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 1.0433 - accuracy: 0.4470 - val_loss: 1.0078 - val_accuracy: 0.4703\n",
            "Epoch 11/500\n",
            "138/138 [==============================] - 104s 762ms/step - loss: 1.0378 - accuracy: 0.4585 - val_loss: 0.9918 - val_accuracy: 0.5018\n",
            "Epoch 12/500\n",
            "138/138 [==============================] - 104s 762ms/step - loss: 1.0373 - accuracy: 0.4632 - val_loss: 1.0057 - val_accuracy: 0.4862\n",
            "Epoch 13/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 1.0207 - accuracy: 0.4670 - val_loss: 0.9600 - val_accuracy: 0.5365\n",
            "Epoch 14/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 1.0295 - accuracy: 0.4802 - val_loss: 1.0398 - val_accuracy: 0.4260\n",
            "Epoch 15/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 1.0102 - accuracy: 0.4834 - val_loss: 1.0299 - val_accuracy: 0.4497\n",
            "Epoch 16/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 1.0316 - accuracy: 0.4836 - val_loss: 0.9648 - val_accuracy: 0.5297\n",
            "Epoch 17/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 1.0106 - accuracy: 0.4981 - val_loss: 0.9930 - val_accuracy: 0.5424\n",
            "Epoch 18/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.9954 - accuracy: 0.5070 - val_loss: 0.9973 - val_accuracy: 0.5102\n",
            "Epoch 19/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.9932 - accuracy: 0.5112 - val_loss: 1.4716 - val_accuracy: 0.3315\n",
            "Epoch 20/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.9939 - accuracy: 0.5211 - val_loss: 0.9001 - val_accuracy: 0.5633\n",
            "Epoch 21/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 1.0048 - accuracy: 0.5307 - val_loss: 1.0598 - val_accuracy: 0.5180\n",
            "Epoch 22/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.9771 - accuracy: 0.5280 - val_loss: 0.9084 - val_accuracy: 0.5490\n",
            "Epoch 23/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.9634 - accuracy: 0.5437 - val_loss: 0.8935 - val_accuracy: 0.5891\n",
            "Epoch 24/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.9421 - accuracy: 0.5426 - val_loss: 1.3225 - val_accuracy: 0.3781\n",
            "Epoch 25/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.9451 - accuracy: 0.5529 - val_loss: 0.8501 - val_accuracy: 0.5940\n",
            "Epoch 26/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.9333 - accuracy: 0.5615 - val_loss: 1.1666 - val_accuracy: 0.5081\n",
            "Epoch 27/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.9202 - accuracy: 0.5725 - val_loss: 0.8533 - val_accuracy: 0.6047\n",
            "Epoch 28/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.9200 - accuracy: 0.5650 - val_loss: 0.9172 - val_accuracy: 0.5729\n",
            "Epoch 29/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.9020 - accuracy: 0.5782 - val_loss: 0.8300 - val_accuracy: 0.6318\n",
            "Epoch 30/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.9166 - accuracy: 0.5880 - val_loss: 0.9137 - val_accuracy: 0.5482\n",
            "Epoch 31/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.9069 - accuracy: 0.5844 - val_loss: 0.8188 - val_accuracy: 0.6565\n",
            "Epoch 32/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.9075 - accuracy: 0.5796 - val_loss: 0.8307 - val_accuracy: 0.6589\n",
            "Epoch 33/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.8684 - accuracy: 0.6020 - val_loss: 0.7899 - val_accuracy: 0.6633\n",
            "Epoch 34/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.8926 - accuracy: 0.5926 - val_loss: 0.8160 - val_accuracy: 0.6352\n",
            "Epoch 35/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.8814 - accuracy: 0.6012 - val_loss: 0.8091 - val_accuracy: 0.6409\n",
            "Epoch 36/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.8945 - accuracy: 0.5989 - val_loss: 0.8190 - val_accuracy: 0.6464\n",
            "Epoch 37/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.9009 - accuracy: 0.5807 - val_loss: 0.8433 - val_accuracy: 0.6466\n",
            "Epoch 38/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.8696 - accuracy: 0.6135 - val_loss: 0.7900 - val_accuracy: 0.6773\n",
            "Epoch 39/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.8584 - accuracy: 0.6208 - val_loss: 0.7725 - val_accuracy: 0.6828\n",
            "Epoch 40/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.8448 - accuracy: 0.6245 - val_loss: 0.8942 - val_accuracy: 0.5630\n",
            "Epoch 41/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.8685 - accuracy: 0.6165 - val_loss: 0.9839 - val_accuracy: 0.5599\n",
            "Epoch 42/500\n",
            "138/138 [==============================] - 104s 762ms/step - loss: 0.8486 - accuracy: 0.6236 - val_loss: 0.9090 - val_accuracy: 0.5411\n",
            "Epoch 43/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.8516 - accuracy: 0.6294 - val_loss: 1.0579 - val_accuracy: 0.5448\n",
            "Epoch 44/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.8635 - accuracy: 0.6245 - val_loss: 1.0796 - val_accuracy: 0.5461\n",
            "Epoch 45/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.8592 - accuracy: 0.6200 - val_loss: 0.8417 - val_accuracy: 0.6271\n",
            "Epoch 46/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.8514 - accuracy: 0.6261 - val_loss: 0.7824 - val_accuracy: 0.6703\n",
            "Epoch 47/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.8634 - accuracy: 0.6216 - val_loss: 0.8790 - val_accuracy: 0.5747\n",
            "Epoch 48/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.8410 - accuracy: 0.6221 - val_loss: 0.8399 - val_accuracy: 0.6294\n",
            "Epoch 49/500\n",
            "138/138 [==============================] - 107s 780ms/step - loss: 0.8346 - accuracy: 0.6381 - val_loss: 0.7796 - val_accuracy: 0.6820\n",
            "Epoch 50/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.8224 - accuracy: 0.6417 - val_loss: 0.8366 - val_accuracy: 0.5971\n",
            "Epoch 51/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.8191 - accuracy: 0.6433 - val_loss: 0.7952 - val_accuracy: 0.6487\n",
            "Epoch 52/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.8341 - accuracy: 0.6415 - val_loss: 0.8189 - val_accuracy: 0.6320\n",
            "Epoch 53/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.8155 - accuracy: 0.6439 - val_loss: 0.7837 - val_accuracy: 0.6555\n",
            "Epoch 54/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.8200 - accuracy: 0.6488 - val_loss: 0.8051 - val_accuracy: 0.6510\n",
            "Epoch 55/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.8140 - accuracy: 0.6463 - val_loss: 0.7725 - val_accuracy: 0.6836\n",
            "Epoch 56/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.8104 - accuracy: 0.6487 - val_loss: 0.7591 - val_accuracy: 0.6706\n",
            "Epoch 57/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.8070 - accuracy: 0.6558 - val_loss: 0.7073 - val_accuracy: 0.7159\n",
            "Epoch 58/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.8072 - accuracy: 0.6535 - val_loss: 1.0070 - val_accuracy: 0.5052\n",
            "Epoch 59/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.8029 - accuracy: 0.6583 - val_loss: 0.7141 - val_accuracy: 0.7076\n",
            "Epoch 60/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.7941 - accuracy: 0.6613 - val_loss: 0.8040 - val_accuracy: 0.6365\n",
            "Epoch 61/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.7853 - accuracy: 0.6613 - val_loss: 0.9430 - val_accuracy: 0.5706\n",
            "Epoch 62/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.7898 - accuracy: 0.6613 - val_loss: 1.0400 - val_accuracy: 0.5690\n",
            "Epoch 63/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7919 - accuracy: 0.6596 - val_loss: 0.9125 - val_accuracy: 0.6333\n",
            "Epoch 64/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.7958 - accuracy: 0.6640 - val_loss: 0.7426 - val_accuracy: 0.7055\n",
            "Epoch 65/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.7924 - accuracy: 0.6644 - val_loss: 0.7078 - val_accuracy: 0.7221\n",
            "Epoch 66/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.7821 - accuracy: 0.6644 - val_loss: 0.9996 - val_accuracy: 0.5641\n",
            "Epoch 67/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7710 - accuracy: 0.6717 - val_loss: 0.7142 - val_accuracy: 0.7117\n",
            "Epoch 68/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.7770 - accuracy: 0.6692 - val_loss: 1.2932 - val_accuracy: 0.5174\n",
            "Epoch 69/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7758 - accuracy: 0.6670 - val_loss: 0.7799 - val_accuracy: 0.6659\n",
            "Epoch 70/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7773 - accuracy: 0.6710 - val_loss: 0.7471 - val_accuracy: 0.6969\n",
            "Epoch 71/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.7579 - accuracy: 0.6796 - val_loss: 0.8942 - val_accuracy: 0.6401\n",
            "Epoch 72/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.7715 - accuracy: 0.6704 - val_loss: 1.0617 - val_accuracy: 0.5591\n",
            "Epoch 73/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7686 - accuracy: 0.6795 - val_loss: 0.8159 - val_accuracy: 0.6404\n",
            "Epoch 74/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7676 - accuracy: 0.6725 - val_loss: 0.7222 - val_accuracy: 0.7169\n",
            "Epoch 75/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.7605 - accuracy: 0.6824 - val_loss: 0.6883 - val_accuracy: 0.7214\n",
            "Epoch 76/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7605 - accuracy: 0.6815 - val_loss: 0.7245 - val_accuracy: 0.7044\n",
            "Epoch 77/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.7551 - accuracy: 0.6810 - val_loss: 0.7098 - val_accuracy: 0.7052\n",
            "Epoch 78/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7555 - accuracy: 0.6799 - val_loss: 0.9606 - val_accuracy: 0.5797\n",
            "Epoch 79/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7566 - accuracy: 0.6872 - val_loss: 0.7294 - val_accuracy: 0.7005\n",
            "Epoch 80/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7460 - accuracy: 0.6821 - val_loss: 0.8296 - val_accuracy: 0.6216\n",
            "Epoch 81/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.7580 - accuracy: 0.6807 - val_loss: 0.7082 - val_accuracy: 0.7237\n",
            "Epoch 82/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7512 - accuracy: 0.6901 - val_loss: 0.7372 - val_accuracy: 0.6768\n",
            "Epoch 83/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.7511 - accuracy: 0.6840 - val_loss: 0.7269 - val_accuracy: 0.6779\n",
            "Epoch 84/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.7441 - accuracy: 0.6885 - val_loss: 0.6990 - val_accuracy: 0.7120\n",
            "Epoch 85/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7441 - accuracy: 0.6889 - val_loss: 0.7311 - val_accuracy: 0.6997\n",
            "Epoch 86/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.7385 - accuracy: 0.6920 - val_loss: 0.7287 - val_accuracy: 0.6995\n",
            "Epoch 87/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7459 - accuracy: 0.6937 - val_loss: 0.6967 - val_accuracy: 0.7188\n",
            "Epoch 88/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.7406 - accuracy: 0.6898 - val_loss: 0.8954 - val_accuracy: 0.5898\n",
            "Epoch 89/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7365 - accuracy: 0.6918 - val_loss: 0.6851 - val_accuracy: 0.7292\n",
            "Epoch 90/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.7281 - accuracy: 0.6917 - val_loss: 0.7085 - val_accuracy: 0.7177\n",
            "Epoch 91/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.7207 - accuracy: 0.7000 - val_loss: 0.6543 - val_accuracy: 0.7333\n",
            "Epoch 92/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7297 - accuracy: 0.6932 - val_loss: 0.8489 - val_accuracy: 0.6466\n",
            "Epoch 93/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.7306 - accuracy: 0.6949 - val_loss: 0.6923 - val_accuracy: 0.7122\n",
            "Epoch 94/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.7242 - accuracy: 0.6937 - val_loss: 0.8996 - val_accuracy: 0.5940\n",
            "Epoch 95/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.7252 - accuracy: 0.6946 - val_loss: 0.6794 - val_accuracy: 0.7326\n",
            "Epoch 96/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.7245 - accuracy: 0.6975 - val_loss: 0.7292 - val_accuracy: 0.6849\n",
            "Epoch 97/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7396 - accuracy: 0.7032 - val_loss: 0.6561 - val_accuracy: 0.7336\n",
            "Epoch 98/500\n",
            "138/138 [==============================] - 104s 762ms/step - loss: 0.7209 - accuracy: 0.6966 - val_loss: 0.7223 - val_accuracy: 0.7083\n",
            "Epoch 99/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.7178 - accuracy: 0.7003 - val_loss: 0.6550 - val_accuracy: 0.7393\n",
            "Epoch 100/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.7133 - accuracy: 0.7013 - val_loss: 0.7902 - val_accuracy: 0.6401\n",
            "Epoch 101/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.7174 - accuracy: 0.7019 - val_loss: 0.7293 - val_accuracy: 0.6917\n",
            "Epoch 102/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.7132 - accuracy: 0.7012 - val_loss: 0.8312 - val_accuracy: 0.6409\n",
            "Epoch 103/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.7192 - accuracy: 0.7010 - val_loss: 0.6545 - val_accuracy: 0.7385\n",
            "Epoch 104/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.7124 - accuracy: 0.7039 - val_loss: 0.6861 - val_accuracy: 0.7216\n",
            "Epoch 105/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.7102 - accuracy: 0.7038 - val_loss: 0.8276 - val_accuracy: 0.6154\n",
            "Epoch 106/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.7085 - accuracy: 0.7082 - val_loss: 0.6575 - val_accuracy: 0.7365\n",
            "Epoch 107/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.7033 - accuracy: 0.7057 - val_loss: 0.7215 - val_accuracy: 0.6974\n",
            "Epoch 108/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.7039 - accuracy: 0.7080 - val_loss: 0.6713 - val_accuracy: 0.7323\n",
            "Epoch 109/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.7124 - accuracy: 0.7055 - val_loss: 0.6731 - val_accuracy: 0.7323\n",
            "Epoch 110/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.7091 - accuracy: 0.7053 - val_loss: 0.6705 - val_accuracy: 0.7279\n",
            "Epoch 111/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.7088 - accuracy: 0.7028 - val_loss: 0.9806 - val_accuracy: 0.5792\n",
            "Epoch 112/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6985 - accuracy: 0.7091 - val_loss: 0.7435 - val_accuracy: 0.6805\n",
            "Epoch 113/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.7017 - accuracy: 0.7051 - val_loss: 0.6925 - val_accuracy: 0.7169\n",
            "Epoch 114/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6955 - accuracy: 0.7090 - val_loss: 0.7998 - val_accuracy: 0.6690\n",
            "Epoch 115/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6940 - accuracy: 0.7134 - val_loss: 0.6520 - val_accuracy: 0.7367\n",
            "Epoch 116/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6955 - accuracy: 0.7092 - val_loss: 0.6485 - val_accuracy: 0.7354\n",
            "Epoch 117/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6921 - accuracy: 0.7104 - val_loss: 0.6449 - val_accuracy: 0.7398\n",
            "Epoch 118/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6906 - accuracy: 0.7152 - val_loss: 0.7815 - val_accuracy: 0.6674\n",
            "Epoch 119/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.7029 - accuracy: 0.7103 - val_loss: 0.6417 - val_accuracy: 0.7432\n",
            "Epoch 120/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.7021 - accuracy: 0.7101 - val_loss: 0.6371 - val_accuracy: 0.7466\n",
            "Epoch 121/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.6885 - accuracy: 0.7153 - val_loss: 0.6655 - val_accuracy: 0.7286\n",
            "Epoch 122/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.6920 - accuracy: 0.7110 - val_loss: 0.6705 - val_accuracy: 0.7276\n",
            "Epoch 123/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.7012 - accuracy: 0.7092 - val_loss: 0.6643 - val_accuracy: 0.7341\n",
            "Epoch 124/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6880 - accuracy: 0.7169 - val_loss: 0.6420 - val_accuracy: 0.7422\n",
            "Epoch 125/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.6834 - accuracy: 0.7151 - val_loss: 0.6432 - val_accuracy: 0.7370\n",
            "Epoch 126/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6902 - accuracy: 0.7168 - val_loss: 0.8331 - val_accuracy: 0.6187\n",
            "Epoch 127/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6885 - accuracy: 0.7134 - val_loss: 0.6536 - val_accuracy: 0.7357\n",
            "Epoch 128/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6865 - accuracy: 0.7167 - val_loss: 0.7431 - val_accuracy: 0.6831\n",
            "Epoch 129/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6923 - accuracy: 0.7108 - val_loss: 0.6562 - val_accuracy: 0.7370\n",
            "Epoch 130/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6864 - accuracy: 0.7160 - val_loss: 0.8118 - val_accuracy: 0.6328\n",
            "Epoch 131/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6837 - accuracy: 0.7172 - val_loss: 1.3084 - val_accuracy: 0.5357\n",
            "Epoch 132/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6894 - accuracy: 0.7158 - val_loss: 0.7858 - val_accuracy: 0.6594\n",
            "Epoch 133/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6855 - accuracy: 0.7172 - val_loss: 0.6438 - val_accuracy: 0.7505\n",
            "Epoch 134/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6736 - accuracy: 0.7227 - val_loss: 0.6468 - val_accuracy: 0.7411\n",
            "Epoch 135/500\n",
            "138/138 [==============================] - 105s 769ms/step - loss: 0.6795 - accuracy: 0.7186 - val_loss: 0.7767 - val_accuracy: 0.6805\n",
            "Epoch 136/500\n",
            "138/138 [==============================] - 105s 769ms/step - loss: 0.6794 - accuracy: 0.7194 - val_loss: 0.6476 - val_accuracy: 0.7432\n",
            "Epoch 137/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6691 - accuracy: 0.7242 - val_loss: 0.7506 - val_accuracy: 0.6812\n",
            "Epoch 138/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6834 - accuracy: 0.7208 - val_loss: 0.8019 - val_accuracy: 0.6701\n",
            "Epoch 139/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6813 - accuracy: 0.7221 - val_loss: 0.6344 - val_accuracy: 0.7448\n",
            "Epoch 140/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6795 - accuracy: 0.7192 - val_loss: 0.6787 - val_accuracy: 0.7128\n",
            "Epoch 141/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6801 - accuracy: 0.7189 - val_loss: 0.8723 - val_accuracy: 0.6143\n",
            "Epoch 142/500\n",
            "138/138 [==============================] - 106s 769ms/step - loss: 0.6774 - accuracy: 0.7187 - val_loss: 0.6213 - val_accuracy: 0.7487\n",
            "Epoch 143/500\n",
            "138/138 [==============================] - 105s 769ms/step - loss: 0.6809 - accuracy: 0.7188 - val_loss: 0.7199 - val_accuracy: 0.7036\n",
            "Epoch 144/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6745 - accuracy: 0.7230 - val_loss: 0.7429 - val_accuracy: 0.6870\n",
            "Epoch 145/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6702 - accuracy: 0.7221 - val_loss: 0.8940 - val_accuracy: 0.5961\n",
            "Epoch 146/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6783 - accuracy: 0.7189 - val_loss: 0.8328 - val_accuracy: 0.6440\n",
            "Epoch 147/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6748 - accuracy: 0.7228 - val_loss: 0.8007 - val_accuracy: 0.6729\n",
            "Epoch 148/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6761 - accuracy: 0.7236 - val_loss: 0.7400 - val_accuracy: 0.6878\n",
            "Epoch 149/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6737 - accuracy: 0.7225 - val_loss: 0.6415 - val_accuracy: 0.7461\n",
            "Epoch 150/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.6597 - accuracy: 0.7287 - val_loss: 0.6328 - val_accuracy: 0.7378\n",
            "Epoch 151/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6751 - accuracy: 0.7246 - val_loss: 0.6304 - val_accuracy: 0.7513\n",
            "Epoch 152/500\n",
            "138/138 [==============================] - 105s 766ms/step - loss: 0.6667 - accuracy: 0.7258 - val_loss: 0.6792 - val_accuracy: 0.7305\n",
            "Epoch 153/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6781 - accuracy: 0.7220 - val_loss: 0.6436 - val_accuracy: 0.7385\n",
            "Epoch 154/500\n",
            "138/138 [==============================] - 105s 766ms/step - loss: 0.6713 - accuracy: 0.7238 - val_loss: 0.6340 - val_accuracy: 0.7414\n",
            "Epoch 155/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6598 - accuracy: 0.7279 - val_loss: 0.8872 - val_accuracy: 0.6510\n",
            "Epoch 156/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6653 - accuracy: 0.7283 - val_loss: 0.6284 - val_accuracy: 0.7542\n",
            "Epoch 157/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6662 - accuracy: 0.7268 - val_loss: 1.3242 - val_accuracy: 0.5161\n",
            "Epoch 158/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6739 - accuracy: 0.7245 - val_loss: 0.6629 - val_accuracy: 0.7346\n",
            "Epoch 159/500\n",
            "138/138 [==============================] - 105s 766ms/step - loss: 0.6633 - accuracy: 0.7281 - val_loss: 0.6457 - val_accuracy: 0.7430\n",
            "Epoch 160/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6597 - accuracy: 0.7307 - val_loss: 0.6366 - val_accuracy: 0.7367\n",
            "Epoch 161/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6640 - accuracy: 0.7279 - val_loss: 0.7333 - val_accuracy: 0.6872\n",
            "Epoch 162/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6593 - accuracy: 0.7282 - val_loss: 0.7175 - val_accuracy: 0.6953\n",
            "Epoch 163/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.6702 - accuracy: 0.7261 - val_loss: 0.7259 - val_accuracy: 0.7060\n",
            "Epoch 164/500\n",
            "138/138 [==============================] - 105s 766ms/step - loss: 0.6694 - accuracy: 0.7272 - val_loss: 0.6994 - val_accuracy: 0.7154\n",
            "Epoch 165/500\n",
            "138/138 [==============================] - 104s 762ms/step - loss: 0.6603 - accuracy: 0.7300 - val_loss: 0.6306 - val_accuracy: 0.7503\n",
            "Epoch 166/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6537 - accuracy: 0.7297 - val_loss: 0.7189 - val_accuracy: 0.7052\n",
            "Epoch 167/500\n",
            "138/138 [==============================] - 105s 764ms/step - loss: 0.6602 - accuracy: 0.7290 - val_loss: 0.6248 - val_accuracy: 0.7503\n",
            "Epoch 168/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.6618 - accuracy: 0.7298 - val_loss: 0.8245 - val_accuracy: 0.6253\n",
            "Epoch 169/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6494 - accuracy: 0.7329 - val_loss: 0.6914 - val_accuracy: 0.7180\n",
            "Epoch 170/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6590 - accuracy: 0.7317 - val_loss: 0.6247 - val_accuracy: 0.7469\n",
            "Epoch 171/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6501 - accuracy: 0.7323 - val_loss: 0.6494 - val_accuracy: 0.7372\n",
            "Epoch 172/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6496 - accuracy: 0.7346 - val_loss: 0.7916 - val_accuracy: 0.6466\n",
            "Epoch 173/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.6463 - accuracy: 0.7335 - val_loss: 0.6274 - val_accuracy: 0.7523\n",
            "Epoch 174/500\n",
            "138/138 [==============================] - 104s 762ms/step - loss: 0.6504 - accuracy: 0.7336 - val_loss: 0.6516 - val_accuracy: 0.7406\n",
            "Epoch 175/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.6555 - accuracy: 0.7351 - val_loss: 0.6213 - val_accuracy: 0.7510\n",
            "Epoch 176/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6493 - accuracy: 0.7398 - val_loss: 0.8052 - val_accuracy: 0.6547\n",
            "Epoch 177/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.6480 - accuracy: 0.7341 - val_loss: 0.6298 - val_accuracy: 0.7477\n",
            "Epoch 178/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6475 - accuracy: 0.7341 - val_loss: 0.6653 - val_accuracy: 0.7312\n",
            "Epoch 179/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.6410 - accuracy: 0.7392 - val_loss: 0.9663 - val_accuracy: 0.6258\n",
            "Epoch 180/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6626 - accuracy: 0.7295 - val_loss: 0.6097 - val_accuracy: 0.7576\n",
            "Epoch 181/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6489 - accuracy: 0.7345 - val_loss: 0.6914 - val_accuracy: 0.7156\n",
            "Epoch 182/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6489 - accuracy: 0.7306 - val_loss: 0.6423 - val_accuracy: 0.7341\n",
            "Epoch 183/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6466 - accuracy: 0.7393 - val_loss: 0.6923 - val_accuracy: 0.7154\n",
            "Epoch 184/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6539 - accuracy: 0.7333 - val_loss: 0.6850 - val_accuracy: 0.7234\n",
            "Epoch 185/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.6500 - accuracy: 0.7355 - val_loss: 1.0030 - val_accuracy: 0.6224\n",
            "Epoch 186/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6562 - accuracy: 0.7347 - val_loss: 0.6707 - val_accuracy: 0.7320\n",
            "Epoch 187/500\n",
            "138/138 [==============================] - 107s 779ms/step - loss: 0.6428 - accuracy: 0.7374 - val_loss: 0.6969 - val_accuracy: 0.7294\n",
            "Epoch 188/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6515 - accuracy: 0.7326 - val_loss: 0.6194 - val_accuracy: 0.7583\n",
            "Epoch 189/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6413 - accuracy: 0.7351 - val_loss: 0.7002 - val_accuracy: 0.7214\n",
            "Epoch 190/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.6512 - accuracy: 0.7329 - val_loss: 0.6650 - val_accuracy: 0.7247\n",
            "Epoch 191/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6450 - accuracy: 0.7373 - val_loss: 0.6405 - val_accuracy: 0.7451\n",
            "Epoch 192/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.6426 - accuracy: 0.7363 - val_loss: 0.6176 - val_accuracy: 0.7599\n",
            "Epoch 193/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.6432 - accuracy: 0.7372 - val_loss: 0.6186 - val_accuracy: 0.7510\n",
            "Epoch 194/500\n",
            "138/138 [==============================] - 104s 760ms/step - loss: 0.6438 - accuracy: 0.7376 - val_loss: 0.9150 - val_accuracy: 0.6039\n",
            "Epoch 195/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.6401 - accuracy: 0.7392 - val_loss: 0.6263 - val_accuracy: 0.7505\n",
            "Epoch 196/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.6469 - accuracy: 0.7375 - val_loss: 0.6172 - val_accuracy: 0.7591\n",
            "Epoch 197/500\n",
            "138/138 [==============================] - 104s 759ms/step - loss: 0.6482 - accuracy: 0.7337 - val_loss: 0.6188 - val_accuracy: 0.7625\n",
            "Epoch 198/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6347 - accuracy: 0.7389 - val_loss: 0.6587 - val_accuracy: 0.7284\n",
            "Epoch 199/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6491 - accuracy: 0.7357 - val_loss: 0.6208 - val_accuracy: 0.7643\n",
            "Epoch 200/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.6322 - accuracy: 0.7434 - val_loss: 0.7322 - val_accuracy: 0.7057\n",
            "Epoch 201/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6355 - accuracy: 0.7411 - val_loss: 0.6024 - val_accuracy: 0.7680\n",
            "Epoch 202/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.6491 - accuracy: 0.7376 - val_loss: 0.6883 - val_accuracy: 0.7247\n",
            "Epoch 203/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6365 - accuracy: 0.7387 - val_loss: 0.9535 - val_accuracy: 0.5888\n",
            "Epoch 204/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.6445 - accuracy: 0.7381 - val_loss: 0.6885 - val_accuracy: 0.7247\n",
            "Epoch 205/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6401 - accuracy: 0.7389 - val_loss: 0.6695 - val_accuracy: 0.7263\n",
            "Epoch 206/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6334 - accuracy: 0.7408 - val_loss: 0.6536 - val_accuracy: 0.7401\n",
            "Epoch 207/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6406 - accuracy: 0.7389 - val_loss: 1.2640 - val_accuracy: 0.5406\n",
            "Epoch 208/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6363 - accuracy: 0.7439 - val_loss: 0.6056 - val_accuracy: 0.7646\n",
            "Epoch 209/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6380 - accuracy: 0.7410 - val_loss: 0.6390 - val_accuracy: 0.7331\n",
            "Epoch 210/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6263 - accuracy: 0.7450 - val_loss: 0.6356 - val_accuracy: 0.7443\n",
            "Epoch 211/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6447 - accuracy: 0.7407 - val_loss: 0.6960 - val_accuracy: 0.7133\n",
            "Epoch 212/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6416 - accuracy: 0.7382 - val_loss: 0.6581 - val_accuracy: 0.7268\n",
            "Epoch 213/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6314 - accuracy: 0.7451 - val_loss: 0.7322 - val_accuracy: 0.6844\n",
            "Epoch 214/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6382 - accuracy: 0.7416 - val_loss: 0.6910 - val_accuracy: 0.7164\n",
            "Epoch 215/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6345 - accuracy: 0.7410 - val_loss: 0.6182 - val_accuracy: 0.7620\n",
            "Epoch 216/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6320 - accuracy: 0.7430 - val_loss: 0.6263 - val_accuracy: 0.7542\n",
            "Epoch 217/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6329 - accuracy: 0.7416 - val_loss: 0.6521 - val_accuracy: 0.7286\n",
            "Epoch 218/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6334 - accuracy: 0.7426 - val_loss: 0.6134 - val_accuracy: 0.7568\n",
            "Epoch 219/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6323 - accuracy: 0.7431 - val_loss: 0.6161 - val_accuracy: 0.7594\n",
            "Epoch 220/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6343 - accuracy: 0.7425 - val_loss: 0.6638 - val_accuracy: 0.7227\n",
            "Epoch 221/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6353 - accuracy: 0.7404 - val_loss: 0.6837 - val_accuracy: 0.7221\n",
            "Epoch 222/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6256 - accuracy: 0.7470 - val_loss: 1.1335 - val_accuracy: 0.5818\n",
            "Epoch 223/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6321 - accuracy: 0.7434 - val_loss: 0.6582 - val_accuracy: 0.7326\n",
            "Epoch 224/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6309 - accuracy: 0.7437 - val_loss: 0.6847 - val_accuracy: 0.7104\n",
            "Epoch 225/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6276 - accuracy: 0.7468 - val_loss: 0.6082 - val_accuracy: 0.7539\n",
            "Epoch 226/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6317 - accuracy: 0.7434 - val_loss: 0.6677 - val_accuracy: 0.7372\n",
            "Epoch 227/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6298 - accuracy: 0.7476 - val_loss: 0.6483 - val_accuracy: 0.7495\n",
            "Epoch 228/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6357 - accuracy: 0.7429 - val_loss: 0.8283 - val_accuracy: 0.6539\n",
            "Epoch 229/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6321 - accuracy: 0.7460 - val_loss: 0.6011 - val_accuracy: 0.7667\n",
            "Epoch 230/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6185 - accuracy: 0.7513 - val_loss: 0.6044 - val_accuracy: 0.7635\n",
            "Epoch 231/500\n",
            "138/138 [==============================] - 104s 757ms/step - loss: 0.6285 - accuracy: 0.7439 - val_loss: 0.6441 - val_accuracy: 0.7396\n",
            "Epoch 232/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6301 - accuracy: 0.7470 - val_loss: 0.5963 - val_accuracy: 0.7648\n",
            "Epoch 233/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6248 - accuracy: 0.7443 - val_loss: 0.6577 - val_accuracy: 0.7411\n",
            "Epoch 234/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6214 - accuracy: 0.7504 - val_loss: 0.7239 - val_accuracy: 0.7174\n",
            "Epoch 235/500\n",
            "138/138 [==============================] - 103s 751ms/step - loss: 0.6382 - accuracy: 0.7421 - val_loss: 0.7043 - val_accuracy: 0.7294\n",
            "Epoch 236/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6302 - accuracy: 0.7464 - val_loss: 0.6103 - val_accuracy: 0.7563\n",
            "Epoch 237/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6334 - accuracy: 0.7433 - val_loss: 0.6550 - val_accuracy: 0.7263\n",
            "Epoch 238/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6182 - accuracy: 0.7533 - val_loss: 0.7227 - val_accuracy: 0.7112\n",
            "Epoch 239/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6288 - accuracy: 0.7461 - val_loss: 0.8056 - val_accuracy: 0.6406\n",
            "Epoch 240/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6271 - accuracy: 0.7451 - val_loss: 0.6491 - val_accuracy: 0.7422\n",
            "Epoch 241/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6299 - accuracy: 0.7443 - val_loss: 0.6183 - val_accuracy: 0.7516\n",
            "Epoch 242/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6129 - accuracy: 0.7524 - val_loss: 0.6836 - val_accuracy: 0.7216\n",
            "Epoch 243/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6298 - accuracy: 0.7435 - val_loss: 0.6442 - val_accuracy: 0.7370\n",
            "Epoch 244/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6268 - accuracy: 0.7481 - val_loss: 0.6070 - val_accuracy: 0.7667\n",
            "Epoch 245/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6165 - accuracy: 0.7485 - val_loss: 0.5961 - val_accuracy: 0.7729\n",
            "Epoch 246/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6281 - accuracy: 0.7480 - val_loss: 0.6098 - val_accuracy: 0.7594\n",
            "Epoch 247/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6177 - accuracy: 0.7495 - val_loss: 0.7795 - val_accuracy: 0.6880\n",
            "Epoch 248/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6238 - accuracy: 0.7498 - val_loss: 0.6288 - val_accuracy: 0.7505\n",
            "Epoch 249/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6219 - accuracy: 0.7464 - val_loss: 0.6267 - val_accuracy: 0.7417\n",
            "Epoch 250/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6223 - accuracy: 0.7499 - val_loss: 0.6241 - val_accuracy: 0.7547\n",
            "Epoch 251/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6253 - accuracy: 0.7483 - val_loss: 0.6270 - val_accuracy: 0.7565\n",
            "Epoch 252/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6171 - accuracy: 0.7498 - val_loss: 0.7035 - val_accuracy: 0.7286\n",
            "Epoch 253/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6176 - accuracy: 0.7530 - val_loss: 0.7166 - val_accuracy: 0.7276\n",
            "Epoch 254/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6190 - accuracy: 0.7492 - val_loss: 0.9487 - val_accuracy: 0.6039\n",
            "Epoch 255/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6207 - accuracy: 0.7474 - val_loss: 0.6876 - val_accuracy: 0.7211\n",
            "Epoch 256/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6210 - accuracy: 0.7511 - val_loss: 0.6873 - val_accuracy: 0.7219\n",
            "Epoch 257/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6178 - accuracy: 0.7501 - val_loss: 0.6037 - val_accuracy: 0.7669\n",
            "Epoch 258/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6210 - accuracy: 0.7499 - val_loss: 0.6270 - val_accuracy: 0.7435\n",
            "Epoch 259/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6184 - accuracy: 0.7494 - val_loss: 0.6558 - val_accuracy: 0.7367\n",
            "Epoch 260/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6214 - accuracy: 0.7476 - val_loss: 0.5896 - val_accuracy: 0.7672\n",
            "Epoch 261/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6144 - accuracy: 0.7507 - val_loss: 0.6239 - val_accuracy: 0.7456\n",
            "Epoch 262/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6155 - accuracy: 0.7519 - val_loss: 0.7185 - val_accuracy: 0.7010\n",
            "Epoch 263/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6244 - accuracy: 0.7483 - val_loss: 0.6096 - val_accuracy: 0.7656\n",
            "Epoch 264/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6173 - accuracy: 0.7523 - val_loss: 0.5885 - val_accuracy: 0.7721\n",
            "Epoch 265/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6179 - accuracy: 0.7514 - val_loss: 0.5992 - val_accuracy: 0.7617\n",
            "Epoch 266/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6226 - accuracy: 0.7505 - val_loss: 0.5949 - val_accuracy: 0.7682\n",
            "Epoch 267/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6113 - accuracy: 0.7540 - val_loss: 0.5988 - val_accuracy: 0.7628\n",
            "Epoch 268/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6115 - accuracy: 0.7529 - val_loss: 0.8231 - val_accuracy: 0.7036\n",
            "Epoch 269/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6269 - accuracy: 0.7494 - val_loss: 0.6072 - val_accuracy: 0.7620\n",
            "Epoch 270/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6227 - accuracy: 0.7503 - val_loss: 0.6150 - val_accuracy: 0.7539\n",
            "Epoch 271/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6181 - accuracy: 0.7518 - val_loss: 0.5941 - val_accuracy: 0.7646\n",
            "Epoch 272/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6127 - accuracy: 0.7549 - val_loss: 0.6201 - val_accuracy: 0.7557\n",
            "Epoch 273/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6134 - accuracy: 0.7539 - val_loss: 0.6935 - val_accuracy: 0.7242\n",
            "Epoch 274/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6193 - accuracy: 0.7482 - val_loss: 0.6088 - val_accuracy: 0.7643\n",
            "Epoch 275/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6134 - accuracy: 0.7534 - val_loss: 0.5997 - val_accuracy: 0.7617\n",
            "Epoch 276/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6227 - accuracy: 0.7522 - val_loss: 0.7079 - val_accuracy: 0.7036\n",
            "Epoch 277/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6090 - accuracy: 0.7563 - val_loss: 0.6030 - val_accuracy: 0.7674\n",
            "Epoch 278/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6071 - accuracy: 0.7545 - val_loss: 0.6841 - val_accuracy: 0.7250\n",
            "Epoch 279/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6167 - accuracy: 0.7520 - val_loss: 0.6563 - val_accuracy: 0.7336\n",
            "Epoch 280/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6117 - accuracy: 0.7526 - val_loss: 0.6189 - val_accuracy: 0.7544\n",
            "Epoch 281/500\n",
            "138/138 [==============================] - 103s 755ms/step - loss: 0.6126 - accuracy: 0.7558 - val_loss: 0.6268 - val_accuracy: 0.7490\n",
            "Epoch 282/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6077 - accuracy: 0.7537 - val_loss: 0.7661 - val_accuracy: 0.6794\n",
            "Epoch 283/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6077 - accuracy: 0.7570 - val_loss: 0.6226 - val_accuracy: 0.7560\n",
            "Epoch 284/500\n",
            "138/138 [==============================] - 104s 756ms/step - loss: 0.6157 - accuracy: 0.7523 - val_loss: 0.5961 - val_accuracy: 0.7719\n",
            "Epoch 285/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6080 - accuracy: 0.7549 - val_loss: 0.6162 - val_accuracy: 0.7526\n",
            "Epoch 286/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6076 - accuracy: 0.7557 - val_loss: 0.5936 - val_accuracy: 0.7740\n",
            "Epoch 287/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6119 - accuracy: 0.7523 - val_loss: 0.6033 - val_accuracy: 0.7576\n",
            "Epoch 288/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6045 - accuracy: 0.7579 - val_loss: 0.8940 - val_accuracy: 0.6492\n",
            "Epoch 289/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6065 - accuracy: 0.7568 - val_loss: 0.6608 - val_accuracy: 0.7365\n",
            "Epoch 290/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.6138 - accuracy: 0.7501 - val_loss: 0.5937 - val_accuracy: 0.7625\n",
            "Epoch 291/500\n",
            "138/138 [==============================] - 103s 751ms/step - loss: 0.6165 - accuracy: 0.7516 - val_loss: 0.6050 - val_accuracy: 0.7594\n",
            "Epoch 292/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6048 - accuracy: 0.7558 - val_loss: 0.6066 - val_accuracy: 0.7615\n",
            "Epoch 293/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6025 - accuracy: 0.7567 - val_loss: 0.5878 - val_accuracy: 0.7747\n",
            "Epoch 294/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6108 - accuracy: 0.7552 - val_loss: 0.6689 - val_accuracy: 0.7302\n",
            "Epoch 295/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6112 - accuracy: 0.7571 - val_loss: 0.6839 - val_accuracy: 0.7104\n",
            "Epoch 296/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6054 - accuracy: 0.7548 - val_loss: 0.6329 - val_accuracy: 0.7424\n",
            "Epoch 297/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6052 - accuracy: 0.7559 - val_loss: 0.6366 - val_accuracy: 0.7380\n",
            "Epoch 298/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6006 - accuracy: 0.7589 - val_loss: 0.6246 - val_accuracy: 0.7573\n",
            "Epoch 299/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6059 - accuracy: 0.7593 - val_loss: 0.5948 - val_accuracy: 0.7661\n",
            "Epoch 300/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6065 - accuracy: 0.7530 - val_loss: 0.5974 - val_accuracy: 0.7732\n",
            "Epoch 301/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6014 - accuracy: 0.7589 - val_loss: 0.5965 - val_accuracy: 0.7698\n",
            "Epoch 302/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6072 - accuracy: 0.7604 - val_loss: 0.6297 - val_accuracy: 0.7508\n",
            "Epoch 303/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6033 - accuracy: 0.7562 - val_loss: 0.7002 - val_accuracy: 0.7143\n",
            "Epoch 304/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6045 - accuracy: 0.7561 - val_loss: 0.6181 - val_accuracy: 0.7565\n",
            "Epoch 305/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6053 - accuracy: 0.7573 - val_loss: 0.6063 - val_accuracy: 0.7568\n",
            "Epoch 306/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6111 - accuracy: 0.7557 - val_loss: 0.5963 - val_accuracy: 0.7643\n",
            "Epoch 307/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6019 - accuracy: 0.7587 - val_loss: 0.6300 - val_accuracy: 0.7495\n",
            "Epoch 308/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.5962 - accuracy: 0.7598 - val_loss: 0.8260 - val_accuracy: 0.6529\n",
            "Epoch 309/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6051 - accuracy: 0.7555 - val_loss: 0.5905 - val_accuracy: 0.7758\n",
            "Epoch 310/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.5951 - accuracy: 0.7605 - val_loss: 0.7119 - val_accuracy: 0.7021\n",
            "Epoch 311/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6005 - accuracy: 0.7605 - val_loss: 0.5790 - val_accuracy: 0.7779\n",
            "Epoch 312/500\n",
            "138/138 [==============================] - 103s 751ms/step - loss: 0.6050 - accuracy: 0.7588 - val_loss: 0.6037 - val_accuracy: 0.7539\n",
            "Epoch 313/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6003 - accuracy: 0.7580 - val_loss: 0.6019 - val_accuracy: 0.7659\n",
            "Epoch 314/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5992 - accuracy: 0.7604 - val_loss: 0.7301 - val_accuracy: 0.6982\n",
            "Epoch 315/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6061 - accuracy: 0.7576 - val_loss: 0.6001 - val_accuracy: 0.7701\n",
            "Epoch 316/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5968 - accuracy: 0.7626 - val_loss: 0.5876 - val_accuracy: 0.7721\n",
            "Epoch 317/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.6052 - accuracy: 0.7573 - val_loss: 0.6241 - val_accuracy: 0.7523\n",
            "Epoch 318/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6053 - accuracy: 0.7576 - val_loss: 0.6441 - val_accuracy: 0.7391\n",
            "Epoch 319/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5991 - accuracy: 0.7606 - val_loss: 0.6721 - val_accuracy: 0.7357\n",
            "Epoch 320/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5922 - accuracy: 0.7615 - val_loss: 0.6396 - val_accuracy: 0.7495\n",
            "Epoch 321/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6036 - accuracy: 0.7604 - val_loss: 0.8161 - val_accuracy: 0.6951\n",
            "Epoch 322/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.5967 - accuracy: 0.7627 - val_loss: 0.5916 - val_accuracy: 0.7643\n",
            "Epoch 323/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5966 - accuracy: 0.7623 - val_loss: 0.6806 - val_accuracy: 0.7320\n",
            "Epoch 324/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6026 - accuracy: 0.7615 - val_loss: 0.5953 - val_accuracy: 0.7677\n",
            "Epoch 325/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6056 - accuracy: 0.7571 - val_loss: 0.6237 - val_accuracy: 0.7583\n",
            "Epoch 326/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5958 - accuracy: 0.7622 - val_loss: 0.6787 - val_accuracy: 0.7490\n",
            "Epoch 327/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5936 - accuracy: 0.7625 - val_loss: 0.6083 - val_accuracy: 0.7594\n",
            "Epoch 328/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6038 - accuracy: 0.7573 - val_loss: 0.5961 - val_accuracy: 0.7758\n",
            "Epoch 329/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6004 - accuracy: 0.7605 - val_loss: 0.5870 - val_accuracy: 0.7688\n",
            "Epoch 330/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.5989 - accuracy: 0.7602 - val_loss: 0.8016 - val_accuracy: 0.6927\n",
            "Epoch 331/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6000 - accuracy: 0.7616 - val_loss: 0.5885 - val_accuracy: 0.7711\n",
            "Epoch 332/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5947 - accuracy: 0.7638 - val_loss: 0.6651 - val_accuracy: 0.7391\n",
            "Epoch 333/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5946 - accuracy: 0.7565 - val_loss: 0.5954 - val_accuracy: 0.7667\n",
            "Epoch 334/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6029 - accuracy: 0.7570 - val_loss: 0.5897 - val_accuracy: 0.7688\n",
            "Epoch 335/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5854 - accuracy: 0.7669 - val_loss: 0.7472 - val_accuracy: 0.7133\n",
            "Epoch 336/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5968 - accuracy: 0.7599 - val_loss: 0.6001 - val_accuracy: 0.7698\n",
            "Epoch 337/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5929 - accuracy: 0.7619 - val_loss: 0.8256 - val_accuracy: 0.6719\n",
            "Epoch 338/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5922 - accuracy: 0.7638 - val_loss: 0.5844 - val_accuracy: 0.7766\n",
            "Epoch 339/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5920 - accuracy: 0.7653 - val_loss: 0.6541 - val_accuracy: 0.7273\n",
            "Epoch 340/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5889 - accuracy: 0.7676 - val_loss: 0.6266 - val_accuracy: 0.7464\n",
            "Epoch 341/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.6008 - accuracy: 0.7560 - val_loss: 0.6143 - val_accuracy: 0.7586\n",
            "Epoch 342/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5965 - accuracy: 0.7595 - val_loss: 0.6051 - val_accuracy: 0.7617\n",
            "Epoch 343/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.6036 - accuracy: 0.7596 - val_loss: 0.5979 - val_accuracy: 0.7721\n",
            "Epoch 344/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.5881 - accuracy: 0.7655 - val_loss: 0.5949 - val_accuracy: 0.7612\n",
            "Epoch 345/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5898 - accuracy: 0.7644 - val_loss: 0.6589 - val_accuracy: 0.7417\n",
            "Epoch 346/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5918 - accuracy: 0.7645 - val_loss: 0.7027 - val_accuracy: 0.7099\n",
            "Epoch 347/500\n",
            "138/138 [==============================] - 103s 751ms/step - loss: 0.5918 - accuracy: 0.7628 - val_loss: 0.5894 - val_accuracy: 0.7740\n",
            "Epoch 348/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5923 - accuracy: 0.7636 - val_loss: 0.5978 - val_accuracy: 0.7651\n",
            "Epoch 349/500\n",
            "138/138 [==============================] - 103s 751ms/step - loss: 0.5859 - accuracy: 0.7657 - val_loss: 0.6338 - val_accuracy: 0.7620\n",
            "Epoch 350/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5937 - accuracy: 0.7635 - val_loss: 0.5835 - val_accuracy: 0.7799\n",
            "Epoch 351/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5964 - accuracy: 0.7645 - val_loss: 0.5854 - val_accuracy: 0.7755\n",
            "Epoch 352/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5797 - accuracy: 0.7661 - val_loss: 0.6576 - val_accuracy: 0.7635\n",
            "Epoch 353/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5921 - accuracy: 0.7628 - val_loss: 0.6227 - val_accuracy: 0.7586\n",
            "Epoch 354/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5878 - accuracy: 0.7657 - val_loss: 0.7381 - val_accuracy: 0.7104\n",
            "Epoch 355/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5904 - accuracy: 0.7663 - val_loss: 0.5815 - val_accuracy: 0.7724\n",
            "Epoch 356/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5836 - accuracy: 0.7701 - val_loss: 0.6843 - val_accuracy: 0.7307\n",
            "Epoch 357/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5922 - accuracy: 0.7600 - val_loss: 0.5946 - val_accuracy: 0.7724\n",
            "Epoch 358/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5854 - accuracy: 0.7672 - val_loss: 0.6146 - val_accuracy: 0.7560\n",
            "Epoch 359/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5936 - accuracy: 0.7645 - val_loss: 0.6033 - val_accuracy: 0.7602\n",
            "Epoch 360/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5835 - accuracy: 0.7681 - val_loss: 0.5986 - val_accuracy: 0.7659\n",
            "Epoch 361/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5841 - accuracy: 0.7648 - val_loss: 0.6677 - val_accuracy: 0.7346\n",
            "Epoch 362/500\n",
            "138/138 [==============================] - 103s 755ms/step - loss: 0.5963 - accuracy: 0.7620 - val_loss: 0.5825 - val_accuracy: 0.7711\n",
            "Epoch 363/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5886 - accuracy: 0.7657 - val_loss: 0.5791 - val_accuracy: 0.7745\n",
            "Epoch 364/500\n",
            "138/138 [==============================] - 103s 751ms/step - loss: 0.5920 - accuracy: 0.7664 - val_loss: 0.6000 - val_accuracy: 0.7646\n",
            "Epoch 365/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5925 - accuracy: 0.7641 - val_loss: 0.6323 - val_accuracy: 0.7599\n",
            "Epoch 366/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5836 - accuracy: 0.7659 - val_loss: 0.6392 - val_accuracy: 0.7542\n",
            "Epoch 367/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5863 - accuracy: 0.7641 - val_loss: 0.5987 - val_accuracy: 0.7622\n",
            "Epoch 368/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5862 - accuracy: 0.7676 - val_loss: 0.6258 - val_accuracy: 0.7469\n",
            "Epoch 369/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5896 - accuracy: 0.7659 - val_loss: 0.6652 - val_accuracy: 0.7339\n",
            "Epoch 370/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5878 - accuracy: 0.7687 - val_loss: 0.6475 - val_accuracy: 0.7424\n",
            "Epoch 371/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5855 - accuracy: 0.7668 - val_loss: 0.7183 - val_accuracy: 0.7086\n",
            "Epoch 372/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5844 - accuracy: 0.7672 - val_loss: 0.7425 - val_accuracy: 0.7154\n",
            "Epoch 373/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5854 - accuracy: 0.7653 - val_loss: 0.8018 - val_accuracy: 0.6938\n",
            "Epoch 374/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5810 - accuracy: 0.7668 - val_loss: 0.6191 - val_accuracy: 0.7628\n",
            "Epoch 375/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5848 - accuracy: 0.7651 - val_loss: 0.6183 - val_accuracy: 0.7552\n",
            "Epoch 376/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5943 - accuracy: 0.7650 - val_loss: 0.6811 - val_accuracy: 0.7297\n",
            "Epoch 377/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5772 - accuracy: 0.7677 - val_loss: 0.6202 - val_accuracy: 0.7589\n",
            "Epoch 378/500\n",
            "138/138 [==============================] - 104s 755ms/step - loss: 0.5872 - accuracy: 0.7676 - val_loss: 0.6126 - val_accuracy: 0.7656\n",
            "Epoch 379/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5892 - accuracy: 0.7650 - val_loss: 0.6788 - val_accuracy: 0.7346\n",
            "Epoch 380/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5881 - accuracy: 0.7663 - val_loss: 0.6670 - val_accuracy: 0.7362\n",
            "Epoch 381/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5755 - accuracy: 0.7709 - val_loss: 0.7040 - val_accuracy: 0.7195\n",
            "Epoch 382/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5883 - accuracy: 0.7651 - val_loss: 0.6519 - val_accuracy: 0.7609\n",
            "Epoch 383/500\n",
            "138/138 [==============================] - 103s 751ms/step - loss: 0.5915 - accuracy: 0.7659 - val_loss: 0.6794 - val_accuracy: 0.7484\n",
            "Epoch 384/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5769 - accuracy: 0.7682 - val_loss: 0.5971 - val_accuracy: 0.7612\n",
            "Epoch 385/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5816 - accuracy: 0.7680 - val_loss: 0.6914 - val_accuracy: 0.7227\n",
            "Epoch 386/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5886 - accuracy: 0.7630 - val_loss: 0.6164 - val_accuracy: 0.7638\n",
            "Epoch 387/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5948 - accuracy: 0.7618 - val_loss: 0.5897 - val_accuracy: 0.7747\n",
            "Epoch 388/500\n",
            "138/138 [==============================] - 104s 754ms/step - loss: 0.5869 - accuracy: 0.7667 - val_loss: 0.7473 - val_accuracy: 0.7073\n",
            "Epoch 389/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5827 - accuracy: 0.7698 - val_loss: 0.6926 - val_accuracy: 0.7284\n",
            "Epoch 390/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5863 - accuracy: 0.7690 - val_loss: 1.1752 - val_accuracy: 0.6031\n",
            "Epoch 391/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5847 - accuracy: 0.7701 - val_loss: 0.6084 - val_accuracy: 0.7672\n",
            "Epoch 392/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5856 - accuracy: 0.7665 - val_loss: 0.5902 - val_accuracy: 0.7776\n",
            "Epoch 393/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5821 - accuracy: 0.7697 - val_loss: 0.7176 - val_accuracy: 0.7260\n",
            "Epoch 394/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5842 - accuracy: 0.7654 - val_loss: 0.6644 - val_accuracy: 0.7617\n",
            "Epoch 395/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5989 - accuracy: 0.7624 - val_loss: 0.5990 - val_accuracy: 0.7721\n",
            "Epoch 396/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5792 - accuracy: 0.7699 - val_loss: 0.6419 - val_accuracy: 0.7531\n",
            "Epoch 397/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5826 - accuracy: 0.7701 - val_loss: 0.6780 - val_accuracy: 0.7359\n",
            "Epoch 398/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5976 - accuracy: 0.7629 - val_loss: 0.6197 - val_accuracy: 0.7729\n",
            "Epoch 399/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5807 - accuracy: 0.7698 - val_loss: 0.6033 - val_accuracy: 0.7688\n",
            "Epoch 400/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5862 - accuracy: 0.7691 - val_loss: 0.5948 - val_accuracy: 0.7714\n",
            "Epoch 401/500\n",
            "138/138 [==============================] - 103s 752ms/step - loss: 0.5883 - accuracy: 0.7673 - val_loss: 0.6048 - val_accuracy: 0.7708\n",
            "Epoch 402/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5785 - accuracy: 0.7688 - val_loss: 0.7980 - val_accuracy: 0.6812\n",
            "Epoch 403/500\n",
            "138/138 [==============================] - 103s 754ms/step - loss: 0.5886 - accuracy: 0.7677 - val_loss: 0.6228 - val_accuracy: 0.7565\n",
            "Epoch 404/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5784 - accuracy: 0.7697 - val_loss: 0.6182 - val_accuracy: 0.7682\n",
            "Epoch 405/500\n",
            "138/138 [==============================] - 103s 753ms/step - loss: 0.5808 - accuracy: 0.7694 - val_loss: 0.6350 - val_accuracy: 0.7534\n",
            "Epoch 406/500\n",
            "138/138 [==============================] - 104s 761ms/step - loss: 0.5892 - accuracy: 0.7645 - val_loss: 0.5855 - val_accuracy: 0.7753\n",
            "Epoch 407/500\n",
            "138/138 [==============================] - 104s 758ms/step - loss: 0.5781 - accuracy: 0.7693 - val_loss: 0.5689 - val_accuracy: 0.7753\n",
            "Epoch 408/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.5823 - accuracy: 0.7698 - val_loss: 0.5881 - val_accuracy: 0.7771\n",
            "Epoch 409/500\n",
            "138/138 [==============================] - 105s 762ms/step - loss: 0.5768 - accuracy: 0.7704 - val_loss: 0.5915 - val_accuracy: 0.7732\n",
            "Epoch 410/500\n",
            "138/138 [==============================] - 105s 763ms/step - loss: 0.5723 - accuracy: 0.7705 - val_loss: 0.6902 - val_accuracy: 0.7396\n",
            "Epoch 411/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5774 - accuracy: 0.7696 - val_loss: 0.5996 - val_accuracy: 0.7638\n",
            "Epoch 412/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.5750 - accuracy: 0.7709 - val_loss: 0.6058 - val_accuracy: 0.7654\n",
            "Epoch 413/500\n",
            "138/138 [==============================] - 105s 766ms/step - loss: 0.5855 - accuracy: 0.7682 - val_loss: 0.6414 - val_accuracy: 0.7453\n",
            "Epoch 414/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.5728 - accuracy: 0.7704 - val_loss: 0.6755 - val_accuracy: 0.7365\n",
            "Epoch 415/500\n",
            "138/138 [==============================] - 105s 765ms/step - loss: 0.5795 - accuracy: 0.7713 - val_loss: 0.7142 - val_accuracy: 0.7102\n",
            "Epoch 416/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.5829 - accuracy: 0.7687 - val_loss: 0.6036 - val_accuracy: 0.7638\n",
            "Epoch 417/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5866 - accuracy: 0.7654 - val_loss: 0.7193 - val_accuracy: 0.6826\n",
            "Epoch 418/500\n",
            "138/138 [==============================] - 107s 777ms/step - loss: 0.5934 - accuracy: 0.7645 - val_loss: 0.6123 - val_accuracy: 0.7490\n",
            "Epoch 419/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5773 - accuracy: 0.7674 - val_loss: 0.5930 - val_accuracy: 0.7745\n",
            "Epoch 420/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5778 - accuracy: 0.7712 - val_loss: 0.5797 - val_accuracy: 0.7703\n",
            "Epoch 421/500\n",
            "138/138 [==============================] - 106s 769ms/step - loss: 0.5777 - accuracy: 0.7719 - val_loss: 0.5860 - val_accuracy: 0.7766\n",
            "Epoch 422/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5806 - accuracy: 0.7666 - val_loss: 0.5922 - val_accuracy: 0.7651\n",
            "Epoch 423/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5822 - accuracy: 0.7699 - val_loss: 0.5856 - val_accuracy: 0.7760\n",
            "Epoch 424/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5784 - accuracy: 0.7704 - val_loss: 0.6515 - val_accuracy: 0.7458\n",
            "Epoch 425/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5716 - accuracy: 0.7715 - val_loss: 0.6167 - val_accuracy: 0.7607\n",
            "Epoch 426/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5758 - accuracy: 0.7702 - val_loss: 0.5941 - val_accuracy: 0.7682\n",
            "Epoch 427/500\n",
            "138/138 [==============================] - 105s 769ms/step - loss: 0.5714 - accuracy: 0.7725 - val_loss: 0.6049 - val_accuracy: 0.7701\n",
            "Epoch 428/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5750 - accuracy: 0.7718 - val_loss: 0.5793 - val_accuracy: 0.7805\n",
            "Epoch 429/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.5723 - accuracy: 0.7728 - val_loss: 0.7264 - val_accuracy: 0.7133\n",
            "Epoch 430/500\n",
            "138/138 [==============================] - 105s 769ms/step - loss: 0.5743 - accuracy: 0.7716 - val_loss: 0.6008 - val_accuracy: 0.7716\n",
            "Epoch 431/500\n",
            "138/138 [==============================] - 105s 767ms/step - loss: 0.5752 - accuracy: 0.7720 - val_loss: 0.7863 - val_accuracy: 0.6839\n",
            "Epoch 432/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5760 - accuracy: 0.7715 - val_loss: 0.7785 - val_accuracy: 0.6810\n",
            "Epoch 433/500\n",
            "138/138 [==============================] - 105s 769ms/step - loss: 0.5768 - accuracy: 0.7684 - val_loss: 0.5917 - val_accuracy: 0.7773\n",
            "Epoch 434/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5773 - accuracy: 0.7720 - val_loss: 0.6220 - val_accuracy: 0.7615\n",
            "Epoch 435/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5753 - accuracy: 0.7729 - val_loss: 0.6161 - val_accuracy: 0.7612\n",
            "Epoch 436/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5797 - accuracy: 0.7697 - val_loss: 0.6066 - val_accuracy: 0.7638\n",
            "Epoch 437/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5808 - accuracy: 0.7705 - val_loss: 0.5825 - val_accuracy: 0.7690\n",
            "Epoch 438/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5664 - accuracy: 0.7757 - val_loss: 0.6005 - val_accuracy: 0.7721\n",
            "Epoch 439/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5727 - accuracy: 0.7728 - val_loss: 0.6086 - val_accuracy: 0.7568\n",
            "Epoch 440/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5762 - accuracy: 0.7729 - val_loss: 0.6471 - val_accuracy: 0.7497\n",
            "Epoch 441/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5723 - accuracy: 0.7708 - val_loss: 0.6271 - val_accuracy: 0.7482\n",
            "Epoch 442/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5672 - accuracy: 0.7745 - val_loss: 0.6227 - val_accuracy: 0.7641\n",
            "Epoch 443/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5735 - accuracy: 0.7730 - val_loss: 0.8153 - val_accuracy: 0.6951\n",
            "Epoch 444/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5717 - accuracy: 0.7753 - val_loss: 0.6264 - val_accuracy: 0.7617\n",
            "Epoch 445/500\n",
            "138/138 [==============================] - 106s 775ms/step - loss: 0.5747 - accuracy: 0.7718 - val_loss: 0.5918 - val_accuracy: 0.7727\n",
            "Epoch 446/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.5820 - accuracy: 0.7683 - val_loss: 0.8326 - val_accuracy: 0.6466\n",
            "Epoch 447/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5752 - accuracy: 0.7733 - val_loss: 0.8502 - val_accuracy: 0.6695\n",
            "Epoch 448/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5762 - accuracy: 0.7723 - val_loss: 0.6360 - val_accuracy: 0.7617\n",
            "Epoch 449/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5735 - accuracy: 0.7731 - val_loss: 0.5771 - val_accuracy: 0.7776\n",
            "Epoch 450/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5718 - accuracy: 0.7740 - val_loss: 0.6053 - val_accuracy: 0.7630\n",
            "Epoch 451/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5722 - accuracy: 0.7724 - val_loss: 0.6089 - val_accuracy: 0.7568\n",
            "Epoch 452/500\n",
            "138/138 [==============================] - 106s 776ms/step - loss: 0.5708 - accuracy: 0.7708 - val_loss: 0.5845 - val_accuracy: 0.7745\n",
            "Epoch 453/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.5732 - accuracy: 0.7705 - val_loss: 0.5757 - val_accuracy: 0.7721\n",
            "Epoch 454/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5783 - accuracy: 0.7682 - val_loss: 0.6215 - val_accuracy: 0.7581\n",
            "Epoch 455/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5703 - accuracy: 0.7725 - val_loss: 0.5780 - val_accuracy: 0.7766\n",
            "Epoch 456/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5780 - accuracy: 0.7690 - val_loss: 0.5709 - val_accuracy: 0.7823\n",
            "Epoch 457/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5863 - accuracy: 0.7689 - val_loss: 0.7063 - val_accuracy: 0.6862\n",
            "Epoch 458/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5772 - accuracy: 0.7739 - val_loss: 0.5718 - val_accuracy: 0.7792\n",
            "Epoch 459/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5827 - accuracy: 0.7685 - val_loss: 0.6013 - val_accuracy: 0.7682\n",
            "Epoch 460/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.5735 - accuracy: 0.7723 - val_loss: 0.5985 - val_accuracy: 0.7693\n",
            "Epoch 461/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5707 - accuracy: 0.7724 - val_loss: 0.6478 - val_accuracy: 0.7490\n",
            "Epoch 462/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.5741 - accuracy: 0.7730 - val_loss: 0.9511 - val_accuracy: 0.6500\n",
            "Epoch 463/500\n",
            "138/138 [==============================] - 106s 771ms/step - loss: 0.5772 - accuracy: 0.7675 - val_loss: 0.6291 - val_accuracy: 0.7685\n",
            "Epoch 464/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5593 - accuracy: 0.7782 - val_loss: 0.5932 - val_accuracy: 0.7716\n",
            "Epoch 465/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5659 - accuracy: 0.7763 - val_loss: 0.7043 - val_accuracy: 0.7237\n",
            "Epoch 466/500\n",
            "138/138 [==============================] - 106s 776ms/step - loss: 0.5626 - accuracy: 0.7768 - val_loss: 0.6105 - val_accuracy: 0.7807\n",
            "Epoch 467/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5789 - accuracy: 0.7696 - val_loss: 0.5721 - val_accuracy: 0.7781\n",
            "Epoch 468/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5713 - accuracy: 0.7737 - val_loss: 0.5860 - val_accuracy: 0.7810\n",
            "Epoch 469/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5747 - accuracy: 0.7705 - val_loss: 0.7553 - val_accuracy: 0.7177\n",
            "Epoch 470/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5688 - accuracy: 0.7741 - val_loss: 0.5778 - val_accuracy: 0.7721\n",
            "Epoch 471/500\n",
            "138/138 [==============================] - 106s 776ms/step - loss: 0.5758 - accuracy: 0.7729 - val_loss: 0.6795 - val_accuracy: 0.7229\n",
            "Epoch 472/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5655 - accuracy: 0.7783 - val_loss: 0.7227 - val_accuracy: 0.7263\n",
            "Epoch 473/500\n",
            "138/138 [==============================] - 107s 778ms/step - loss: 0.5751 - accuracy: 0.7710 - val_loss: 0.5952 - val_accuracy: 0.7826\n",
            "Epoch 474/500\n",
            "138/138 [==============================] - 107s 779ms/step - loss: 0.5762 - accuracy: 0.7717 - val_loss: 0.5813 - val_accuracy: 0.7779\n",
            "Epoch 475/500\n",
            "138/138 [==============================] - 106s 775ms/step - loss: 0.5697 - accuracy: 0.7726 - val_loss: 0.6400 - val_accuracy: 0.7615\n",
            "Epoch 476/500\n",
            "138/138 [==============================] - 106s 776ms/step - loss: 0.5719 - accuracy: 0.7757 - val_loss: 0.5750 - val_accuracy: 0.7794\n",
            "Epoch 477/500\n",
            "138/138 [==============================] - 106s 775ms/step - loss: 0.5808 - accuracy: 0.7689 - val_loss: 0.5985 - val_accuracy: 0.7682\n",
            "Epoch 478/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5609 - accuracy: 0.7763 - val_loss: 0.8598 - val_accuracy: 0.6807\n",
            "Epoch 479/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5710 - accuracy: 0.7735 - val_loss: 0.7068 - val_accuracy: 0.7607\n",
            "Epoch 480/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5745 - accuracy: 0.7741 - val_loss: 0.6376 - val_accuracy: 0.7529\n",
            "Epoch 481/500\n",
            "138/138 [==============================] - 106s 772ms/step - loss: 0.5653 - accuracy: 0.7758 - val_loss: 0.6059 - val_accuracy: 0.7812\n",
            "Epoch 482/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.5661 - accuracy: 0.7760 - val_loss: 0.8448 - val_accuracy: 0.6753\n",
            "Epoch 483/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5767 - accuracy: 0.7720 - val_loss: 0.6486 - val_accuracy: 0.7737\n",
            "Epoch 484/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5683 - accuracy: 0.7711 - val_loss: 0.6112 - val_accuracy: 0.7659\n",
            "Epoch 485/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5685 - accuracy: 0.7758 - val_loss: 0.6717 - val_accuracy: 0.7326\n",
            "Epoch 486/500\n",
            "138/138 [==============================] - 106s 773ms/step - loss: 0.5698 - accuracy: 0.7763 - val_loss: 0.5838 - val_accuracy: 0.7805\n",
            "Epoch 487/500\n",
            "138/138 [==============================] - 108s 789ms/step - loss: 0.5667 - accuracy: 0.7765 - val_loss: 0.6080 - val_accuracy: 0.7794\n",
            "Epoch 488/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.5566 - accuracy: 0.7804 - val_loss: 0.6958 - val_accuracy: 0.7258\n",
            "Epoch 489/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.5702 - accuracy: 0.7765 - val_loss: 0.6028 - val_accuracy: 0.7750\n",
            "Epoch 490/500\n",
            "138/138 [==============================] - 106s 774ms/step - loss: 0.5657 - accuracy: 0.7797 - val_loss: 0.6296 - val_accuracy: 0.7599\n",
            "Epoch 491/500\n",
            "138/138 [==============================] - 106s 770ms/step - loss: 0.5597 - accuracy: 0.7775 - val_loss: 0.5857 - val_accuracy: 0.7734\n",
            "Epoch 492/500\n",
            "138/138 [==============================] - 105s 768ms/step - loss: 0.5624 - accuracy: 0.7798 - val_loss: 0.6161 - val_accuracy: 0.7792\n",
            "Epoch 493/500\n",
            "138/138 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.7763"
          ]
        }
      ],
      "source": [
        "def generator(data, labels, batch_size):\n",
        "  while True:\n",
        "    indices = np.random.permutation(len(data))\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      yield data[batch_indices], labels[batch_indices]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(4, input_shape=(393816,), activation='linear'),\n",
        "])\n",
        "\n",
        "for _ in range(6):\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/exp2/'\n",
        "if not os.path.exists(checkpoint_filepath):\n",
        "  os.mkdir(checkpoint_filepath)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'model_weights_epoch{epoch:02d}_val_acc{val_accuracy:.4f}.h5'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "#checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback, history_logger_1]\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "X_train = X_final_train_bert[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_final_train_bert[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=500,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=callbacks_list_1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot4LhujF39Kr"
      },
      "source": [
        "#EXP3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKilr10M3nrQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7068b5d1-4d90-45d1-fbf9-aec1929bc72f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-ee2be7a4d290>:51: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "138/138 [==============================] - 121s 864ms/step - loss: 0.6023 - accuracy: 0.7599 - val_loss: 0.6745 - val_accuracy: 0.7143\n",
            "Epoch 2/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5669 - accuracy: 0.7783 - val_loss: 0.5785 - val_accuracy: 0.7885\n",
            "Epoch 3/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5582 - accuracy: 0.7830 - val_loss: 0.6587 - val_accuracy: 0.7471\n",
            "Epoch 4/500\n",
            "138/138 [==============================] - 121s 886ms/step - loss: 0.5737 - accuracy: 0.7724 - val_loss: 0.8018 - val_accuracy: 0.7135\n",
            "Epoch 5/500\n",
            "138/138 [==============================] - 117s 851ms/step - loss: 0.5812 - accuracy: 0.7699 - val_loss: 0.5609 - val_accuracy: 0.7870\n",
            "Epoch 6/500\n",
            "138/138 [==============================] - 117s 857ms/step - loss: 0.5867 - accuracy: 0.7675 - val_loss: 0.6156 - val_accuracy: 0.7737\n",
            "Epoch 7/500\n",
            "138/138 [==============================] - 117s 857ms/step - loss: 0.5694 - accuracy: 0.7743 - val_loss: 0.6116 - val_accuracy: 0.7646\n",
            "Epoch 8/500\n",
            "138/138 [==============================] - 115s 835ms/step - loss: 0.5529 - accuracy: 0.7831 - val_loss: 0.6448 - val_accuracy: 0.7380\n",
            "Epoch 9/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5712 - accuracy: 0.7748 - val_loss: 0.5646 - val_accuracy: 0.7893\n",
            "Epoch 10/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5641 - accuracy: 0.7813 - val_loss: 0.5947 - val_accuracy: 0.7784\n",
            "Epoch 11/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5598 - accuracy: 0.7822 - val_loss: 0.6464 - val_accuracy: 0.7664\n",
            "Epoch 12/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.6344 - accuracy: 0.7588 - val_loss: 0.6338 - val_accuracy: 0.7471\n",
            "Epoch 13/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5962 - accuracy: 0.7655 - val_loss: 0.5593 - val_accuracy: 0.7859\n",
            "Epoch 14/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5610 - accuracy: 0.7795 - val_loss: 0.6473 - val_accuracy: 0.7799\n",
            "Epoch 15/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5779 - accuracy: 0.7705 - val_loss: 0.6903 - val_accuracy: 0.7471\n",
            "Epoch 16/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5818 - accuracy: 0.7674 - val_loss: 0.7135 - val_accuracy: 0.7471\n",
            "Epoch 17/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5799 - accuracy: 0.7724 - val_loss: 0.5891 - val_accuracy: 0.7812\n",
            "Epoch 18/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5617 - accuracy: 0.7802 - val_loss: 0.6235 - val_accuracy: 0.7734\n",
            "Epoch 19/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5580 - accuracy: 0.7806 - val_loss: 0.5882 - val_accuracy: 0.7799\n",
            "Epoch 20/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5534 - accuracy: 0.7833 - val_loss: 0.6432 - val_accuracy: 0.7391\n",
            "Epoch 21/500\n",
            "138/138 [==============================] - 115s 838ms/step - loss: 0.5575 - accuracy: 0.7810 - val_loss: 0.6678 - val_accuracy: 0.7190\n",
            "Epoch 22/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5594 - accuracy: 0.7802 - val_loss: 0.8364 - val_accuracy: 0.6615\n",
            "Epoch 23/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5595 - accuracy: 0.7776 - val_loss: 0.6021 - val_accuracy: 0.7695\n",
            "Epoch 24/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5690 - accuracy: 0.7788 - val_loss: 0.7297 - val_accuracy: 0.7052\n",
            "Epoch 25/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5738 - accuracy: 0.7797 - val_loss: 0.5865 - val_accuracy: 0.7906\n",
            "Epoch 26/500\n",
            "138/138 [==============================] - 116s 843ms/step - loss: 0.5639 - accuracy: 0.7777 - val_loss: 0.7180 - val_accuracy: 0.7253\n",
            "Epoch 27/500\n",
            "138/138 [==============================] - 120s 875ms/step - loss: 0.5971 - accuracy: 0.7783 - val_loss: 0.6269 - val_accuracy: 0.7396\n",
            "Epoch 28/500\n",
            "138/138 [==============================] - 121s 881ms/step - loss: 0.5716 - accuracy: 0.7826 - val_loss: 0.7453 - val_accuracy: 0.7039\n",
            "Epoch 29/500\n",
            "138/138 [==============================] - 116s 843ms/step - loss: 0.5741 - accuracy: 0.7712 - val_loss: 0.6971 - val_accuracy: 0.7115\n",
            "Epoch 30/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5730 - accuracy: 0.7750 - val_loss: 0.6236 - val_accuracy: 0.7656\n",
            "Epoch 31/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5706 - accuracy: 0.7761 - val_loss: 0.6140 - val_accuracy: 0.7685\n",
            "Epoch 32/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5720 - accuracy: 0.7753 - val_loss: 0.7205 - val_accuracy: 0.7440\n",
            "Epoch 33/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5520 - accuracy: 0.7837 - val_loss: 0.9034 - val_accuracy: 0.6177\n",
            "Epoch 34/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5634 - accuracy: 0.7793 - val_loss: 0.5784 - val_accuracy: 0.7792\n",
            "Epoch 35/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5550 - accuracy: 0.7816 - val_loss: 0.6645 - val_accuracy: 0.7260\n",
            "Epoch 36/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5607 - accuracy: 0.7781 - val_loss: 0.6630 - val_accuracy: 0.7302\n",
            "Epoch 37/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5600 - accuracy: 0.7780 - val_loss: 0.5810 - val_accuracy: 0.7766\n",
            "Epoch 38/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5618 - accuracy: 0.7809 - val_loss: 0.6761 - val_accuracy: 0.7536\n",
            "Epoch 39/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5478 - accuracy: 0.7836 - val_loss: 0.6356 - val_accuracy: 0.7742\n",
            "Epoch 40/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5557 - accuracy: 0.7828 - val_loss: 0.8292 - val_accuracy: 0.6836\n",
            "Epoch 41/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5497 - accuracy: 0.7839 - val_loss: 0.5677 - val_accuracy: 0.7844\n",
            "Epoch 42/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5489 - accuracy: 0.7830 - val_loss: 0.6288 - val_accuracy: 0.7745\n",
            "Epoch 43/500\n",
            "138/138 [==============================] - 115s 838ms/step - loss: 0.5478 - accuracy: 0.7850 - val_loss: 0.5733 - val_accuracy: 0.7911\n",
            "Epoch 44/500\n",
            "138/138 [==============================] - 115s 839ms/step - loss: 0.5625 - accuracy: 0.7786 - val_loss: 0.7045 - val_accuracy: 0.7000\n",
            "Epoch 45/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5611 - accuracy: 0.7795 - val_loss: 0.6336 - val_accuracy: 0.7784\n",
            "Epoch 46/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5525 - accuracy: 0.7856 - val_loss: 0.6781 - val_accuracy: 0.7461\n",
            "Epoch 47/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5962 - accuracy: 0.7766 - val_loss: 0.8534 - val_accuracy: 0.7229\n",
            "Epoch 48/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5445 - accuracy: 0.7869 - val_loss: 0.6444 - val_accuracy: 0.7727\n",
            "Epoch 49/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5535 - accuracy: 0.7810 - val_loss: 0.8318 - val_accuracy: 0.6661\n",
            "Epoch 50/500\n",
            "138/138 [==============================] - 114s 831ms/step - loss: 0.5533 - accuracy: 0.7826 - val_loss: 0.6540 - val_accuracy: 0.7583\n",
            "Epoch 51/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5397 - accuracy: 0.7871 - val_loss: 0.7553 - val_accuracy: 0.7258\n",
            "Epoch 52/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5458 - accuracy: 0.7861 - val_loss: 0.5973 - val_accuracy: 0.7867\n",
            "Epoch 53/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5985 - accuracy: 0.7661 - val_loss: 0.6314 - val_accuracy: 0.7443\n",
            "Epoch 54/500\n",
            "138/138 [==============================] - 120s 878ms/step - loss: 0.5489 - accuracy: 0.7856 - val_loss: 0.5933 - val_accuracy: 0.7859\n",
            "Epoch 55/500\n",
            "138/138 [==============================] - 118s 862ms/step - loss: 0.5597 - accuracy: 0.7799 - val_loss: 0.6689 - val_accuracy: 0.7284\n",
            "Epoch 56/500\n",
            "138/138 [==============================] - 116s 847ms/step - loss: 0.5509 - accuracy: 0.7844 - val_loss: 0.6446 - val_accuracy: 0.7721\n",
            "Epoch 57/500\n",
            "138/138 [==============================] - 117s 855ms/step - loss: 0.5439 - accuracy: 0.7860 - val_loss: 0.6108 - val_accuracy: 0.7805\n",
            "Epoch 58/500\n",
            "138/138 [==============================] - 115s 841ms/step - loss: 0.5483 - accuracy: 0.7863 - val_loss: 0.8456 - val_accuracy: 0.7000\n",
            "Epoch 59/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5488 - accuracy: 0.7844 - val_loss: 0.6752 - val_accuracy: 0.7349\n",
            "Epoch 60/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5497 - accuracy: 0.7885 - val_loss: 0.6027 - val_accuracy: 0.7766\n",
            "Epoch 61/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5506 - accuracy: 0.7847 - val_loss: 0.7715 - val_accuracy: 0.7232\n",
            "Epoch 62/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5336 - accuracy: 0.7921 - val_loss: 0.6056 - val_accuracy: 0.7753\n",
            "Epoch 63/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5650 - accuracy: 0.7764 - val_loss: 0.6051 - val_accuracy: 0.7893\n",
            "Epoch 64/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5500 - accuracy: 0.7838 - val_loss: 0.5935 - val_accuracy: 0.7706\n",
            "Epoch 65/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5369 - accuracy: 0.7906 - val_loss: 0.6536 - val_accuracy: 0.7247\n",
            "Epoch 66/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5607 - accuracy: 0.7810 - val_loss: 0.5810 - val_accuracy: 0.7883\n",
            "Epoch 67/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5483 - accuracy: 0.7856 - val_loss: 0.6714 - val_accuracy: 0.7693\n",
            "Epoch 68/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5776 - accuracy: 0.7857 - val_loss: 0.6333 - val_accuracy: 0.7563\n",
            "Epoch 69/500\n",
            "138/138 [==============================] - 111s 811ms/step - loss: 0.5449 - accuracy: 0.7868 - val_loss: 0.8073 - val_accuracy: 0.7451\n",
            "Epoch 70/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5886 - accuracy: 0.7647 - val_loss: 0.5868 - val_accuracy: 0.7737\n",
            "Epoch 71/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5689 - accuracy: 0.7778 - val_loss: 0.6677 - val_accuracy: 0.7461\n",
            "Epoch 72/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5523 - accuracy: 0.7843 - val_loss: 0.6483 - val_accuracy: 0.7823\n",
            "Epoch 73/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5293 - accuracy: 0.7951 - val_loss: 0.6650 - val_accuracy: 0.7490\n",
            "Epoch 74/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5513 - accuracy: 0.7817 - val_loss: 0.5994 - val_accuracy: 0.7839\n",
            "Epoch 75/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.5606 - accuracy: 0.7816 - val_loss: 0.5985 - val_accuracy: 0.7951\n",
            "Epoch 76/500\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.5414 - accuracy: 0.7884 - val_loss: 0.7571 - val_accuracy: 0.7594\n",
            "Epoch 77/500\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.5525 - accuracy: 0.7862 - val_loss: 0.6309 - val_accuracy: 0.7721\n",
            "Epoch 78/500\n",
            "138/138 [==============================] - 119s 866ms/step - loss: 0.5352 - accuracy: 0.7920 - val_loss: 0.7185 - val_accuracy: 0.7659\n",
            "Epoch 79/500\n",
            "138/138 [==============================] - 118s 864ms/step - loss: 0.5600 - accuracy: 0.7810 - val_loss: 0.6383 - val_accuracy: 0.7617\n",
            "Epoch 80/500\n",
            "138/138 [==============================] - 120s 875ms/step - loss: 0.5762 - accuracy: 0.7713 - val_loss: 0.6468 - val_accuracy: 0.7734\n",
            "Epoch 81/500\n",
            "138/138 [==============================] - 120s 878ms/step - loss: 0.5347 - accuracy: 0.7925 - val_loss: 0.6257 - val_accuracy: 0.7906\n",
            "Epoch 82/500\n",
            "138/138 [==============================] - 117s 850ms/step - loss: 0.5582 - accuracy: 0.7800 - val_loss: 0.7016 - val_accuracy: 0.7815\n",
            "Epoch 83/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5380 - accuracy: 0.7892 - val_loss: 0.7811 - val_accuracy: 0.7430\n",
            "Epoch 84/500\n",
            "138/138 [==============================] - 114s 835ms/step - loss: 0.5303 - accuracy: 0.7925 - val_loss: 0.6423 - val_accuracy: 0.7758\n",
            "Epoch 85/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.6208 - accuracy: 0.7857 - val_loss: 0.5803 - val_accuracy: 0.7750\n",
            "Epoch 86/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5515 - accuracy: 0.7850 - val_loss: 0.5614 - val_accuracy: 0.7893\n",
            "Epoch 87/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5632 - accuracy: 0.7752 - val_loss: 0.5922 - val_accuracy: 0.7792\n",
            "Epoch 88/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5393 - accuracy: 0.7895 - val_loss: 0.5938 - val_accuracy: 0.7836\n",
            "Epoch 89/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5706 - accuracy: 0.7880 - val_loss: 0.6940 - val_accuracy: 0.7180\n",
            "Epoch 90/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5402 - accuracy: 0.7905 - val_loss: 0.5597 - val_accuracy: 0.7909\n",
            "Epoch 91/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5476 - accuracy: 0.7838 - val_loss: 0.7280 - val_accuracy: 0.7471\n",
            "Epoch 92/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5464 - accuracy: 0.7850 - val_loss: 0.6526 - val_accuracy: 0.7591\n",
            "Epoch 93/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5716 - accuracy: 0.7781 - val_loss: 0.6506 - val_accuracy: 0.7440\n",
            "Epoch 94/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5583 - accuracy: 0.7806 - val_loss: 0.7320 - val_accuracy: 0.7344\n",
            "Epoch 95/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5432 - accuracy: 0.7852 - val_loss: 0.9007 - val_accuracy: 0.6716\n",
            "Epoch 96/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5576 - accuracy: 0.7818 - val_loss: 0.9408 - val_accuracy: 0.6544\n",
            "Epoch 97/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5398 - accuracy: 0.7879 - val_loss: 0.6110 - val_accuracy: 0.7924\n",
            "Epoch 98/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5307 - accuracy: 0.7941 - val_loss: 0.6724 - val_accuracy: 0.7721\n",
            "Epoch 99/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5386 - accuracy: 0.7870 - val_loss: 0.6523 - val_accuracy: 0.7763\n",
            "Epoch 100/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5373 - accuracy: 0.7899 - val_loss: 0.6060 - val_accuracy: 0.7875\n",
            "Epoch 101/500\n",
            "138/138 [==============================] - 115s 840ms/step - loss: 0.5404 - accuracy: 0.7872 - val_loss: 0.7125 - val_accuracy: 0.7529\n",
            "Epoch 102/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5660 - accuracy: 0.7789 - val_loss: 0.7477 - val_accuracy: 0.6854\n",
            "Epoch 103/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5567 - accuracy: 0.7802 - val_loss: 0.5828 - val_accuracy: 0.7891\n",
            "Epoch 104/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5394 - accuracy: 0.7893 - val_loss: 0.7797 - val_accuracy: 0.7198\n",
            "Epoch 105/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5966 - accuracy: 0.7668 - val_loss: 0.6029 - val_accuracy: 0.7909\n",
            "Epoch 106/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.5727 - accuracy: 0.7755 - val_loss: 0.6286 - val_accuracy: 0.7786\n",
            "Epoch 107/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5311 - accuracy: 0.7937 - val_loss: 0.6139 - val_accuracy: 0.7807\n",
            "Epoch 108/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5563 - accuracy: 0.7815 - val_loss: 0.5658 - val_accuracy: 0.7878\n",
            "Epoch 109/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5452 - accuracy: 0.7845 - val_loss: 0.6869 - val_accuracy: 0.7328\n",
            "Epoch 110/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5332 - accuracy: 0.7938 - val_loss: 0.6589 - val_accuracy: 0.7484\n",
            "Epoch 111/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5594 - accuracy: 0.7816 - val_loss: 0.6799 - val_accuracy: 0.7229\n",
            "Epoch 112/500\n",
            "138/138 [==============================] - 118s 862ms/step - loss: 0.5484 - accuracy: 0.7833 - val_loss: 0.6489 - val_accuracy: 0.7563\n",
            "Epoch 113/500\n",
            "138/138 [==============================] - 117s 852ms/step - loss: 0.5492 - accuracy: 0.7910 - val_loss: 0.5766 - val_accuracy: 0.7826\n",
            "Epoch 114/500\n",
            "138/138 [==============================] - 117s 851ms/step - loss: 0.5510 - accuracy: 0.7917 - val_loss: 0.7225 - val_accuracy: 0.7104\n",
            "Epoch 115/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5429 - accuracy: 0.7880 - val_loss: 0.6000 - val_accuracy: 0.7862\n",
            "Epoch 116/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5649 - accuracy: 0.7786 - val_loss: 0.5637 - val_accuracy: 0.7828\n",
            "Epoch 117/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.6320 - accuracy: 0.7801 - val_loss: 0.5456 - val_accuracy: 0.7951\n",
            "Epoch 118/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5555 - accuracy: 0.7863 - val_loss: 0.6682 - val_accuracy: 0.7375\n",
            "Epoch 119/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5786 - accuracy: 0.7857 - val_loss: 0.6451 - val_accuracy: 0.7320\n",
            "Epoch 120/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5734 - accuracy: 0.7841 - val_loss: 0.6605 - val_accuracy: 0.7268\n",
            "Epoch 121/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5515 - accuracy: 0.7844 - val_loss: 0.7553 - val_accuracy: 0.6719\n",
            "Epoch 122/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5471 - accuracy: 0.7845 - val_loss: 0.5911 - val_accuracy: 0.7792\n",
            "Epoch 123/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5416 - accuracy: 0.7907 - val_loss: 0.5542 - val_accuracy: 0.7958\n",
            "Epoch 124/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5726 - accuracy: 0.7837 - val_loss: 0.5475 - val_accuracy: 0.7865\n",
            "Epoch 125/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5557 - accuracy: 0.7842 - val_loss: 0.7538 - val_accuracy: 0.6891\n",
            "Epoch 126/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5447 - accuracy: 0.7883 - val_loss: 0.6044 - val_accuracy: 0.7799\n",
            "Epoch 127/500\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.5736 - accuracy: 0.7814 - val_loss: 0.6546 - val_accuracy: 0.7427\n",
            "Epoch 128/500\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 0.5634 - accuracy: 0.7868 - val_loss: 0.6644 - val_accuracy: 0.7549\n",
            "Epoch 129/500\n",
            "138/138 [==============================] - 118s 863ms/step - loss: 0.5301 - accuracy: 0.7945 - val_loss: 0.5701 - val_accuracy: 0.7826\n",
            "Epoch 130/500\n",
            "138/138 [==============================] - 119s 866ms/step - loss: 0.5383 - accuracy: 0.7891 - val_loss: 0.5673 - val_accuracy: 0.7854\n",
            "Epoch 131/500\n",
            "138/138 [==============================] - 118s 862ms/step - loss: 0.5730 - accuracy: 0.7733 - val_loss: 0.6005 - val_accuracy: 0.7760\n",
            "Epoch 132/500\n",
            "138/138 [==============================] - 117s 851ms/step - loss: 0.5358 - accuracy: 0.7921 - val_loss: 0.5547 - val_accuracy: 0.7846\n",
            "Epoch 133/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5541 - accuracy: 0.7824 - val_loss: 0.6053 - val_accuracy: 0.7794\n",
            "Epoch 134/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.5567 - accuracy: 0.7821 - val_loss: 0.5649 - val_accuracy: 0.7951\n",
            "Epoch 135/500\n",
            "138/138 [==============================] - 116s 845ms/step - loss: 0.5442 - accuracy: 0.7904 - val_loss: 0.5466 - val_accuracy: 0.7917\n",
            "Epoch 136/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5292 - accuracy: 0.7948 - val_loss: 0.6855 - val_accuracy: 0.7688\n",
            "Epoch 137/500\n",
            "138/138 [==============================] - 114s 831ms/step - loss: 0.5309 - accuracy: 0.7945 - val_loss: 0.6294 - val_accuracy: 0.7638\n",
            "Epoch 138/500\n",
            "138/138 [==============================] - 115s 839ms/step - loss: 0.5903 - accuracy: 0.7773 - val_loss: 0.6624 - val_accuracy: 0.7521\n",
            "Epoch 139/500\n",
            "138/138 [==============================] - 115s 839ms/step - loss: 0.5307 - accuracy: 0.7913 - val_loss: 0.7010 - val_accuracy: 0.7664\n",
            "Epoch 140/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5707 - accuracy: 0.7784 - val_loss: 0.6281 - val_accuracy: 0.7654\n",
            "Epoch 141/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5368 - accuracy: 0.7914 - val_loss: 0.5788 - val_accuracy: 0.7878\n",
            "Epoch 142/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5295 - accuracy: 0.7937 - val_loss: 0.5875 - val_accuracy: 0.7930\n",
            "Epoch 143/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5502 - accuracy: 0.7872 - val_loss: 0.5973 - val_accuracy: 0.7729\n",
            "Epoch 144/500\n",
            "138/138 [==============================] - 111s 806ms/step - loss: 0.5554 - accuracy: 0.7856 - val_loss: 0.6022 - val_accuracy: 0.7867\n",
            "Epoch 145/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5397 - accuracy: 0.7916 - val_loss: 0.5942 - val_accuracy: 0.7753\n",
            "Epoch 146/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5814 - accuracy: 0.7696 - val_loss: 0.5588 - val_accuracy: 0.7893\n",
            "Epoch 147/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5414 - accuracy: 0.7903 - val_loss: 0.9110 - val_accuracy: 0.6823\n",
            "Epoch 148/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.5935 - accuracy: 0.7777 - val_loss: 0.7108 - val_accuracy: 0.7214\n",
            "Epoch 149/500\n",
            "138/138 [==============================] - 111s 811ms/step - loss: 0.5436 - accuracy: 0.7870 - val_loss: 0.6471 - val_accuracy: 0.7677\n",
            "Epoch 150/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5275 - accuracy: 0.7936 - val_loss: 0.6531 - val_accuracy: 0.7828\n",
            "Epoch 151/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5791 - accuracy: 0.7791 - val_loss: 0.5941 - val_accuracy: 0.7698\n",
            "Epoch 152/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5471 - accuracy: 0.7888 - val_loss: 0.6309 - val_accuracy: 0.7682\n",
            "Epoch 153/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5428 - accuracy: 0.7888 - val_loss: 0.5994 - val_accuracy: 0.7794\n",
            "Epoch 154/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5817 - accuracy: 0.7719 - val_loss: 0.5931 - val_accuracy: 0.7872\n",
            "Epoch 155/500\n",
            "138/138 [==============================] - 111s 807ms/step - loss: 0.5685 - accuracy: 0.7729 - val_loss: 0.6467 - val_accuracy: 0.7599\n",
            "Epoch 156/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5910 - accuracy: 0.7804 - val_loss: 0.6021 - val_accuracy: 0.7789\n",
            "Epoch 157/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5494 - accuracy: 0.7915 - val_loss: 0.5773 - val_accuracy: 0.7805\n",
            "Epoch 158/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5323 - accuracy: 0.7931 - val_loss: 0.5643 - val_accuracy: 0.8000\n",
            "Epoch 159/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5989 - accuracy: 0.7781 - val_loss: 0.7110 - val_accuracy: 0.7138\n",
            "Epoch 160/500\n",
            "138/138 [==============================] - 116s 847ms/step - loss: 0.5471 - accuracy: 0.7882 - val_loss: 0.5818 - val_accuracy: 0.7987\n",
            "Epoch 161/500\n",
            "138/138 [==============================] - 116s 850ms/step - loss: 0.5401 - accuracy: 0.7910 - val_loss: 0.6529 - val_accuracy: 0.7466\n",
            "Epoch 162/500\n",
            "138/138 [==============================] - 116s 847ms/step - loss: 0.5709 - accuracy: 0.7783 - val_loss: 1.0865 - val_accuracy: 0.5388\n",
            "Epoch 163/500\n",
            "138/138 [==============================] - 115s 840ms/step - loss: 0.6272 - accuracy: 0.7626 - val_loss: 0.5446 - val_accuracy: 0.7930\n",
            "Epoch 164/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5622 - accuracy: 0.7857 - val_loss: 0.7033 - val_accuracy: 0.7385\n",
            "Epoch 165/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.6047 - accuracy: 0.7616 - val_loss: 0.6286 - val_accuracy: 0.7672\n",
            "Epoch 166/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5854 - accuracy: 0.7680 - val_loss: 0.6713 - val_accuracy: 0.7430\n",
            "Epoch 167/500\n",
            "138/138 [==============================] - 111s 811ms/step - loss: 0.5503 - accuracy: 0.7837 - val_loss: 0.6760 - val_accuracy: 0.7427\n",
            "Epoch 168/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5848 - accuracy: 0.7798 - val_loss: 0.8025 - val_accuracy: 0.6841\n",
            "Epoch 169/500\n",
            "138/138 [==============================] - 111s 807ms/step - loss: 0.5907 - accuracy: 0.7708 - val_loss: 0.5738 - val_accuracy: 0.7846\n",
            "Epoch 170/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.5394 - accuracy: 0.7922 - val_loss: 0.6384 - val_accuracy: 0.7721\n",
            "Epoch 171/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5364 - accuracy: 0.7905 - val_loss: 0.6245 - val_accuracy: 0.7786\n",
            "Epoch 172/500\n",
            "138/138 [==============================] - 111s 807ms/step - loss: 0.5865 - accuracy: 0.7791 - val_loss: 1.1925 - val_accuracy: 0.5469\n",
            "Epoch 173/500\n",
            "138/138 [==============================] - 111s 808ms/step - loss: 0.5904 - accuracy: 0.7704 - val_loss: 0.8269 - val_accuracy: 0.6984\n",
            "Epoch 174/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5872 - accuracy: 0.7685 - val_loss: 0.8584 - val_accuracy: 0.6328\n",
            "Epoch 175/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5433 - accuracy: 0.7900 - val_loss: 0.6194 - val_accuracy: 0.7792\n",
            "Epoch 176/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5909 - accuracy: 0.7852 - val_loss: 0.7334 - val_accuracy: 0.7010\n",
            "Epoch 177/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.6031 - accuracy: 0.7794 - val_loss: 0.6086 - val_accuracy: 0.7542\n",
            "Epoch 178/500\n",
            "138/138 [==============================] - 116s 844ms/step - loss: 0.5702 - accuracy: 0.7697 - val_loss: 0.6752 - val_accuracy: 0.7555\n",
            "Epoch 179/500\n",
            "138/138 [==============================] - 117s 851ms/step - loss: 0.5583 - accuracy: 0.7823 - val_loss: 0.6731 - val_accuracy: 0.7607\n",
            "Epoch 180/500\n",
            "138/138 [==============================] - 116s 848ms/step - loss: 0.5276 - accuracy: 0.7935 - val_loss: 0.9989 - val_accuracy: 0.7242\n",
            "Epoch 181/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5472 - accuracy: 0.7856 - val_loss: 0.6382 - val_accuracy: 0.7534\n",
            "Epoch 182/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5802 - accuracy: 0.7814 - val_loss: 0.6609 - val_accuracy: 0.7167\n",
            "Epoch 183/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5800 - accuracy: 0.7718 - val_loss: 0.5959 - val_accuracy: 0.7654\n",
            "Epoch 184/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5523 - accuracy: 0.7859 - val_loss: 0.7543 - val_accuracy: 0.7229\n",
            "Epoch 185/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5632 - accuracy: 0.7774 - val_loss: 0.6937 - val_accuracy: 0.7797\n",
            "Epoch 186/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5827 - accuracy: 0.7855 - val_loss: 0.8397 - val_accuracy: 0.7292\n",
            "Epoch 187/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5855 - accuracy: 0.7741 - val_loss: 0.6269 - val_accuracy: 0.7690\n",
            "Epoch 188/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5657 - accuracy: 0.7819 - val_loss: 0.6609 - val_accuracy: 0.7641\n",
            "Epoch 189/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5452 - accuracy: 0.7880 - val_loss: 0.8294 - val_accuracy: 0.7102\n",
            "Epoch 190/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5555 - accuracy: 0.7837 - val_loss: 0.5431 - val_accuracy: 0.7901\n",
            "Epoch 191/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5472 - accuracy: 0.7888 - val_loss: 0.5862 - val_accuracy: 0.7880\n",
            "Epoch 192/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5710 - accuracy: 0.7758 - val_loss: 0.6247 - val_accuracy: 0.7818\n",
            "Epoch 193/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5833 - accuracy: 0.7817 - val_loss: 0.7007 - val_accuracy: 0.7307\n",
            "Epoch 194/500\n",
            "138/138 [==============================] - 117s 856ms/step - loss: 0.5661 - accuracy: 0.7848 - val_loss: 0.6799 - val_accuracy: 0.7516\n",
            "Epoch 195/500\n",
            "138/138 [==============================] - 115s 838ms/step - loss: 0.6843 - accuracy: 0.7792 - val_loss: 0.6313 - val_accuracy: 0.7443\n",
            "Epoch 196/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5298 - accuracy: 0.7949 - val_loss: 0.5993 - val_accuracy: 0.7797\n",
            "Epoch 197/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5478 - accuracy: 0.7853 - val_loss: 0.5877 - val_accuracy: 0.7891\n",
            "Epoch 198/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5426 - accuracy: 0.7886 - val_loss: 0.6128 - val_accuracy: 0.7849\n",
            "Epoch 199/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5587 - accuracy: 0.7815 - val_loss: 0.6830 - val_accuracy: 0.7766\n",
            "Epoch 200/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5378 - accuracy: 0.7972 - val_loss: 0.6694 - val_accuracy: 0.7638\n",
            "Epoch 201/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5656 - accuracy: 0.7782 - val_loss: 0.6140 - val_accuracy: 0.7576\n",
            "Epoch 202/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5525 - accuracy: 0.7865 - val_loss: 0.8797 - val_accuracy: 0.6523\n",
            "Epoch 203/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5593 - accuracy: 0.7808 - val_loss: 0.6676 - val_accuracy: 0.7312\n",
            "Epoch 204/500\n",
            "138/138 [==============================] - 111s 811ms/step - loss: 0.5778 - accuracy: 0.7753 - val_loss: 0.7036 - val_accuracy: 0.7427\n",
            "Epoch 205/500\n",
            "138/138 [==============================] - 110s 803ms/step - loss: 0.5573 - accuracy: 0.7842 - val_loss: 0.6529 - val_accuracy: 0.7784\n",
            "Epoch 206/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5575 - accuracy: 0.7824 - val_loss: 0.7627 - val_accuracy: 0.7201\n",
            "Epoch 207/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5549 - accuracy: 0.7828 - val_loss: 0.6476 - val_accuracy: 0.7531\n",
            "Epoch 208/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5641 - accuracy: 0.7782 - val_loss: 0.8584 - val_accuracy: 0.7232\n",
            "Epoch 209/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5677 - accuracy: 0.7782 - val_loss: 0.5950 - val_accuracy: 0.7716\n",
            "Epoch 210/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5692 - accuracy: 0.7816 - val_loss: 0.6271 - val_accuracy: 0.7607\n",
            "Epoch 211/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5654 - accuracy: 0.7746 - val_loss: 0.7828 - val_accuracy: 0.7034\n",
            "Epoch 212/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5623 - accuracy: 0.7824 - val_loss: 0.7157 - val_accuracy: 0.7690\n",
            "Epoch 213/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5438 - accuracy: 0.7876 - val_loss: 0.7118 - val_accuracy: 0.7224\n",
            "Epoch 214/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5797 - accuracy: 0.7780 - val_loss: 0.6393 - val_accuracy: 0.7464\n",
            "Epoch 215/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5511 - accuracy: 0.7858 - val_loss: 0.7097 - val_accuracy: 0.7615\n",
            "Epoch 216/500\n",
            "138/138 [==============================] - 111s 807ms/step - loss: 0.5532 - accuracy: 0.7839 - val_loss: 0.7210 - val_accuracy: 0.7268\n",
            "Epoch 217/500\n",
            "138/138 [==============================] - 111s 808ms/step - loss: 0.5741 - accuracy: 0.7896 - val_loss: 0.8764 - val_accuracy: 0.6745\n",
            "Epoch 218/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5725 - accuracy: 0.7810 - val_loss: 0.6002 - val_accuracy: 0.7768\n",
            "Epoch 219/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5591 - accuracy: 0.7839 - val_loss: 0.7114 - val_accuracy: 0.7266\n",
            "Epoch 220/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5535 - accuracy: 0.7863 - val_loss: 0.6175 - val_accuracy: 0.7661\n",
            "Epoch 221/500\n",
            "138/138 [==============================] - 110s 805ms/step - loss: 0.6999 - accuracy: 0.7752 - val_loss: 0.8027 - val_accuracy: 0.7750\n",
            "Epoch 222/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5538 - accuracy: 0.7876 - val_loss: 0.8191 - val_accuracy: 0.7495\n",
            "Epoch 223/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5662 - accuracy: 0.7804 - val_loss: 0.6840 - val_accuracy: 0.7880\n",
            "Epoch 224/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5652 - accuracy: 0.7843 - val_loss: 0.7372 - val_accuracy: 0.7263\n",
            "Epoch 225/500\n",
            "138/138 [==============================] - 117s 855ms/step - loss: 0.5525 - accuracy: 0.7859 - val_loss: 0.7375 - val_accuracy: 0.7594\n",
            "Epoch 226/500\n",
            "138/138 [==============================] - 114s 835ms/step - loss: 0.5330 - accuracy: 0.7921 - val_loss: 0.6501 - val_accuracy: 0.7589\n",
            "Epoch 227/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5554 - accuracy: 0.7829 - val_loss: 0.7821 - val_accuracy: 0.7852\n",
            "Epoch 228/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5582 - accuracy: 0.7844 - val_loss: 0.7475 - val_accuracy: 0.7216\n",
            "Epoch 229/500\n",
            "138/138 [==============================] - 115s 838ms/step - loss: 0.5690 - accuracy: 0.7810 - val_loss: 0.6057 - val_accuracy: 0.7698\n",
            "Epoch 230/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5559 - accuracy: 0.7827 - val_loss: 0.6472 - val_accuracy: 0.7594\n",
            "Epoch 231/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5888 - accuracy: 0.7793 - val_loss: 0.8961 - val_accuracy: 0.6727\n",
            "Epoch 232/500\n",
            "138/138 [==============================] - 116s 849ms/step - loss: 0.5558 - accuracy: 0.7839 - val_loss: 0.5966 - val_accuracy: 0.7826\n",
            "Epoch 233/500\n",
            "138/138 [==============================] - 115s 840ms/step - loss: 0.5531 - accuracy: 0.7853 - val_loss: 0.6430 - val_accuracy: 0.7612\n",
            "Epoch 234/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5693 - accuracy: 0.7797 - val_loss: 0.7220 - val_accuracy: 0.7352\n",
            "Epoch 235/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.5635 - accuracy: 0.7767 - val_loss: 0.6952 - val_accuracy: 0.7685\n",
            "Epoch 236/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5678 - accuracy: 0.7784 - val_loss: 0.6457 - val_accuracy: 0.7568\n",
            "Epoch 237/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5662 - accuracy: 0.7782 - val_loss: 0.8035 - val_accuracy: 0.6740\n",
            "Epoch 238/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.5535 - accuracy: 0.7816 - val_loss: 0.5914 - val_accuracy: 0.7701\n",
            "Epoch 239/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5660 - accuracy: 0.7807 - val_loss: 0.6862 - val_accuracy: 0.7570\n",
            "Epoch 240/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.6089 - accuracy: 0.7807 - val_loss: 0.5912 - val_accuracy: 0.7854\n",
            "Epoch 241/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5597 - accuracy: 0.7815 - val_loss: 0.8598 - val_accuracy: 0.7065\n",
            "Epoch 242/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.6412 - accuracy: 0.7797 - val_loss: 0.6153 - val_accuracy: 0.7750\n",
            "Epoch 243/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5418 - accuracy: 0.7913 - val_loss: 0.6986 - val_accuracy: 0.7297\n",
            "Epoch 244/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5791 - accuracy: 0.7728 - val_loss: 0.6108 - val_accuracy: 0.7633\n",
            "Epoch 245/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5678 - accuracy: 0.7766 - val_loss: 0.8675 - val_accuracy: 0.6617\n",
            "Epoch 246/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5702 - accuracy: 0.7779 - val_loss: 0.7066 - val_accuracy: 0.7703\n",
            "Epoch 247/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5697 - accuracy: 0.7762 - val_loss: 0.6595 - val_accuracy: 0.7443\n",
            "Epoch 248/500\n",
            "138/138 [==============================] - 117s 855ms/step - loss: 0.5653 - accuracy: 0.7824 - val_loss: 0.5814 - val_accuracy: 0.7844\n",
            "Epoch 249/500\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.5576 - accuracy: 0.7816 - val_loss: 0.6322 - val_accuracy: 0.7818\n",
            "Epoch 250/500\n",
            "138/138 [==============================] - 116s 844ms/step - loss: 0.5779 - accuracy: 0.7787 - val_loss: 0.6180 - val_accuracy: 0.7549\n",
            "Epoch 251/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5782 - accuracy: 0.7748 - val_loss: 0.5836 - val_accuracy: 0.7758\n",
            "Epoch 252/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5611 - accuracy: 0.7815 - val_loss: 0.7300 - val_accuracy: 0.7273\n",
            "Epoch 253/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5660 - accuracy: 0.7814 - val_loss: 0.6188 - val_accuracy: 0.7721\n",
            "Epoch 254/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5595 - accuracy: 0.7810 - val_loss: 0.7914 - val_accuracy: 0.7651\n",
            "Epoch 255/500\n",
            "138/138 [==============================] - 115s 841ms/step - loss: 0.5586 - accuracy: 0.7805 - val_loss: 0.9175 - val_accuracy: 0.7344\n",
            "Epoch 256/500\n",
            "138/138 [==============================] - 115s 842ms/step - loss: 0.5985 - accuracy: 0.7797 - val_loss: 0.6824 - val_accuracy: 0.7544\n",
            "Epoch 257/500\n",
            "138/138 [==============================] - 112s 821ms/step - loss: 0.5554 - accuracy: 0.7821 - val_loss: 0.5792 - val_accuracy: 0.7799\n",
            "Epoch 258/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5567 - accuracy: 0.7836 - val_loss: 0.6443 - val_accuracy: 0.7667\n",
            "Epoch 259/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5667 - accuracy: 0.7785 - val_loss: 0.5877 - val_accuracy: 0.7721\n",
            "Epoch 260/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5634 - accuracy: 0.7801 - val_loss: 0.5759 - val_accuracy: 0.7846\n",
            "Epoch 261/500\n",
            "138/138 [==============================] - 111s 808ms/step - loss: 0.5570 - accuracy: 0.7860 - val_loss: 0.7022 - val_accuracy: 0.7383\n",
            "Epoch 262/500\n",
            "138/138 [==============================] - 111s 808ms/step - loss: 0.5798 - accuracy: 0.7808 - val_loss: 0.6818 - val_accuracy: 0.7846\n",
            "Epoch 263/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5669 - accuracy: 0.7776 - val_loss: 0.6402 - val_accuracy: 0.7513\n",
            "Epoch 264/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5661 - accuracy: 0.7796 - val_loss: 0.6109 - val_accuracy: 0.7802\n",
            "Epoch 265/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5745 - accuracy: 0.7794 - val_loss: 0.5937 - val_accuracy: 0.7789\n",
            "Epoch 266/500\n",
            "138/138 [==============================] - 110s 804ms/step - loss: 0.5642 - accuracy: 0.7797 - val_loss: 0.6605 - val_accuracy: 0.7383\n",
            "Epoch 267/500\n",
            "138/138 [==============================] - 110s 805ms/step - loss: 0.5772 - accuracy: 0.7773 - val_loss: 0.8806 - val_accuracy: 0.7521\n",
            "Epoch 268/500\n",
            "138/138 [==============================] - 110s 803ms/step - loss: 0.5964 - accuracy: 0.7794 - val_loss: 0.6204 - val_accuracy: 0.7568\n",
            "Epoch 269/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.7896 - accuracy: 0.7763 - val_loss: 0.5629 - val_accuracy: 0.7859\n",
            "Epoch 270/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5812 - accuracy: 0.7728 - val_loss: 0.5819 - val_accuracy: 0.7820\n",
            "Epoch 271/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5736 - accuracy: 0.7819 - val_loss: 0.6060 - val_accuracy: 0.7667\n",
            "Epoch 272/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5785 - accuracy: 0.7794 - val_loss: 0.6698 - val_accuracy: 0.7414\n",
            "Epoch 273/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5753 - accuracy: 0.7757 - val_loss: 0.6204 - val_accuracy: 0.7622\n",
            "Epoch 274/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5776 - accuracy: 0.7746 - val_loss: 0.6006 - val_accuracy: 0.7667\n",
            "Epoch 275/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5837 - accuracy: 0.7750 - val_loss: 0.6624 - val_accuracy: 0.7628\n",
            "Epoch 276/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5795 - accuracy: 0.7743 - val_loss: 0.6479 - val_accuracy: 0.7690\n",
            "Epoch 277/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5772 - accuracy: 0.7760 - val_loss: 0.6648 - val_accuracy: 0.7266\n",
            "Epoch 278/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5746 - accuracy: 0.7776 - val_loss: 0.6947 - val_accuracy: 0.7422\n",
            "Epoch 279/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5833 - accuracy: 0.7723 - val_loss: 0.7812 - val_accuracy: 0.7346\n",
            "Epoch 280/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5864 - accuracy: 0.7694 - val_loss: 0.7392 - val_accuracy: 0.7708\n",
            "Epoch 281/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5721 - accuracy: 0.7788 - val_loss: 0.6194 - val_accuracy: 0.7581\n",
            "Epoch 282/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5837 - accuracy: 0.7735 - val_loss: 0.6885 - val_accuracy: 0.7307\n",
            "Epoch 283/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.6411 - accuracy: 0.7697 - val_loss: 0.6877 - val_accuracy: 0.7648\n",
            "Epoch 284/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5791 - accuracy: 0.7756 - val_loss: 0.7354 - val_accuracy: 0.7078\n",
            "Epoch 285/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5743 - accuracy: 0.7760 - val_loss: 0.6206 - val_accuracy: 0.7656\n",
            "Epoch 286/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5956 - accuracy: 0.7763 - val_loss: 0.6316 - val_accuracy: 0.7513\n",
            "Epoch 287/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5935 - accuracy: 0.7705 - val_loss: 0.5766 - val_accuracy: 0.7862\n",
            "Epoch 288/500\n",
            "138/138 [==============================] - 114s 835ms/step - loss: 0.5742 - accuracy: 0.7788 - val_loss: 0.6550 - val_accuracy: 0.7602\n",
            "Epoch 289/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.6051 - accuracy: 0.7732 - val_loss: 0.5831 - val_accuracy: 0.7768\n",
            "Epoch 290/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5693 - accuracy: 0.7774 - val_loss: 0.7620 - val_accuracy: 0.7615\n",
            "Epoch 291/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5796 - accuracy: 0.7776 - val_loss: 0.6130 - val_accuracy: 0.7773\n",
            "Epoch 292/500\n",
            "138/138 [==============================] - 118s 864ms/step - loss: 0.5748 - accuracy: 0.7759 - val_loss: 0.6751 - val_accuracy: 0.7674\n",
            "Epoch 293/500\n",
            "138/138 [==============================] - 120s 874ms/step - loss: 0.5784 - accuracy: 0.7728 - val_loss: 0.7771 - val_accuracy: 0.6859\n",
            "Epoch 294/500\n",
            "138/138 [==============================] - 120s 872ms/step - loss: 0.5732 - accuracy: 0.7762 - val_loss: 0.5750 - val_accuracy: 0.7789\n",
            "Epoch 295/500\n",
            "138/138 [==============================] - 119s 872ms/step - loss: 0.5714 - accuracy: 0.7771 - val_loss: 0.5717 - val_accuracy: 0.7818\n",
            "Epoch 296/500\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 0.9130 - accuracy: 0.7795 - val_loss: 0.7227 - val_accuracy: 0.7346\n",
            "Epoch 297/500\n",
            "138/138 [==============================] - 119s 872ms/step - loss: 0.5703 - accuracy: 0.7801 - val_loss: 0.6191 - val_accuracy: 0.7635\n",
            "Epoch 298/500\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.5922 - accuracy: 0.7700 - val_loss: 0.6072 - val_accuracy: 0.7690\n",
            "Epoch 299/500\n",
            "138/138 [==============================] - 116s 843ms/step - loss: 0.5987 - accuracy: 0.7727 - val_loss: 0.7676 - val_accuracy: 0.6815\n",
            "Epoch 300/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5667 - accuracy: 0.7791 - val_loss: 0.6897 - val_accuracy: 0.7458\n",
            "Epoch 301/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.5817 - accuracy: 0.7755 - val_loss: 0.6453 - val_accuracy: 0.7565\n",
            "Epoch 302/500\n",
            "138/138 [==============================] - 116s 850ms/step - loss: 0.5987 - accuracy: 0.7686 - val_loss: 0.6432 - val_accuracy: 0.7453\n",
            "Epoch 303/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5705 - accuracy: 0.7769 - val_loss: 0.6568 - val_accuracy: 0.7591\n",
            "Epoch 304/500\n",
            "138/138 [==============================] - 116s 843ms/step - loss: 0.5633 - accuracy: 0.7797 - val_loss: 0.8465 - val_accuracy: 0.6565\n",
            "Epoch 305/500\n",
            "138/138 [==============================] - 115s 841ms/step - loss: 0.5671 - accuracy: 0.7792 - val_loss: 0.5677 - val_accuracy: 0.7839\n",
            "Epoch 306/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5537 - accuracy: 0.7875 - val_loss: 0.6341 - val_accuracy: 0.7630\n",
            "Epoch 307/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.6071 - accuracy: 0.7737 - val_loss: 0.6359 - val_accuracy: 0.7753\n",
            "Epoch 308/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5624 - accuracy: 0.7817 - val_loss: 0.6879 - val_accuracy: 0.7372\n",
            "Epoch 309/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.6400 - accuracy: 0.7619 - val_loss: 0.6101 - val_accuracy: 0.7805\n",
            "Epoch 310/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.6232 - accuracy: 0.7652 - val_loss: 0.5688 - val_accuracy: 0.7794\n",
            "Epoch 311/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5755 - accuracy: 0.7744 - val_loss: 0.6747 - val_accuracy: 0.7320\n",
            "Epoch 312/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5925 - accuracy: 0.7715 - val_loss: 0.6851 - val_accuracy: 0.7016\n",
            "Epoch 313/500\n",
            "138/138 [==============================] - 112s 813ms/step - loss: 0.5774 - accuracy: 0.7741 - val_loss: 0.6602 - val_accuracy: 0.7375\n",
            "Epoch 314/500\n",
            "138/138 [==============================] - 111s 808ms/step - loss: 0.5864 - accuracy: 0.7711 - val_loss: 0.5950 - val_accuracy: 0.7706\n",
            "Epoch 315/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.5950 - accuracy: 0.7702 - val_loss: 0.5709 - val_accuracy: 0.7839\n",
            "Epoch 316/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5697 - accuracy: 0.7783 - val_loss: 0.7008 - val_accuracy: 0.7469\n",
            "Epoch 317/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5810 - accuracy: 0.7734 - val_loss: 0.6300 - val_accuracy: 0.7724\n",
            "Epoch 318/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5720 - accuracy: 0.7744 - val_loss: 0.6519 - val_accuracy: 0.7393\n",
            "Epoch 319/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5683 - accuracy: 0.7780 - val_loss: 0.7668 - val_accuracy: 0.7318\n",
            "Epoch 320/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.6654 - accuracy: 0.7695 - val_loss: 0.7352 - val_accuracy: 0.7135\n",
            "Epoch 321/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5988 - accuracy: 0.7781 - val_loss: 0.7979 - val_accuracy: 0.6714\n",
            "Epoch 322/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5708 - accuracy: 0.7763 - val_loss: 0.7311 - val_accuracy: 0.7034\n",
            "Epoch 323/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.6156 - accuracy: 0.7649 - val_loss: 0.6347 - val_accuracy: 0.7602\n",
            "Epoch 324/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5700 - accuracy: 0.7775 - val_loss: 0.7592 - val_accuracy: 0.7000\n",
            "Epoch 325/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.6358 - accuracy: 0.7561 - val_loss: 0.6956 - val_accuracy: 0.7161\n",
            "Epoch 326/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5893 - accuracy: 0.7726 - val_loss: 0.5892 - val_accuracy: 0.7742\n",
            "Epoch 327/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5897 - accuracy: 0.7701 - val_loss: 0.5954 - val_accuracy: 0.7763\n",
            "Epoch 328/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5726 - accuracy: 0.7784 - val_loss: 0.5879 - val_accuracy: 0.7758\n",
            "Epoch 329/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5812 - accuracy: 0.7715 - val_loss: 0.5927 - val_accuracy: 0.7732\n",
            "Epoch 330/500\n",
            "138/138 [==============================] - 118s 859ms/step - loss: 0.5784 - accuracy: 0.7726 - val_loss: 0.5731 - val_accuracy: 0.7849\n",
            "Epoch 331/500\n",
            "138/138 [==============================] - 118s 861ms/step - loss: 0.5568 - accuracy: 0.7842 - val_loss: 0.7311 - val_accuracy: 0.7214\n",
            "Epoch 332/500\n",
            "138/138 [==============================] - 118s 862ms/step - loss: 0.5609 - accuracy: 0.7841 - val_loss: 0.7519 - val_accuracy: 0.7167\n",
            "Epoch 333/500\n",
            "138/138 [==============================] - 118s 859ms/step - loss: 0.5644 - accuracy: 0.7818 - val_loss: 0.6315 - val_accuracy: 0.7799\n",
            "Epoch 334/500\n",
            "138/138 [==============================] - 118s 863ms/step - loss: 0.5659 - accuracy: 0.7826 - val_loss: 0.6846 - val_accuracy: 0.7521\n",
            "Epoch 335/500\n",
            "138/138 [==============================] - 118s 863ms/step - loss: 0.5779 - accuracy: 0.7760 - val_loss: 0.6400 - val_accuracy: 0.7445\n",
            "Epoch 336/500\n",
            "138/138 [==============================] - 118s 859ms/step - loss: 0.5757 - accuracy: 0.7773 - val_loss: 0.5927 - val_accuracy: 0.7706\n",
            "Epoch 337/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5834 - accuracy: 0.7783 - val_loss: 0.5700 - val_accuracy: 0.7799\n",
            "Epoch 338/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5759 - accuracy: 0.7766 - val_loss: 0.7512 - val_accuracy: 0.7227\n",
            "Epoch 339/500\n",
            "138/138 [==============================] - 115s 839ms/step - loss: 0.5753 - accuracy: 0.7776 - val_loss: 0.7226 - val_accuracy: 0.7201\n",
            "Epoch 340/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5990 - accuracy: 0.7785 - val_loss: 0.6934 - val_accuracy: 0.7240\n",
            "Epoch 341/500\n",
            "138/138 [==============================] - 114s 831ms/step - loss: 0.5772 - accuracy: 0.7769 - val_loss: 0.6939 - val_accuracy: 0.7440\n",
            "Epoch 342/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.5982 - accuracy: 0.7755 - val_loss: 0.6340 - val_accuracy: 0.7578\n",
            "Epoch 343/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5621 - accuracy: 0.7844 - val_loss: 0.6364 - val_accuracy: 0.7440\n",
            "Epoch 344/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5651 - accuracy: 0.7832 - val_loss: 0.7773 - val_accuracy: 0.7273\n",
            "Epoch 345/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5745 - accuracy: 0.7755 - val_loss: 0.6022 - val_accuracy: 0.7729\n",
            "Epoch 346/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5552 - accuracy: 0.7877 - val_loss: 0.9425 - val_accuracy: 0.7216\n",
            "Epoch 347/500\n",
            "138/138 [==============================] - 111s 811ms/step - loss: 0.5600 - accuracy: 0.7832 - val_loss: 0.7308 - val_accuracy: 0.7148\n",
            "Epoch 348/500\n",
            "138/138 [==============================] - 111s 808ms/step - loss: 0.5793 - accuracy: 0.7771 - val_loss: 0.8625 - val_accuracy: 0.6794\n",
            "Epoch 349/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5718 - accuracy: 0.7832 - val_loss: 0.6070 - val_accuracy: 0.7742\n",
            "Epoch 350/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5609 - accuracy: 0.7809 - val_loss: 0.6088 - val_accuracy: 0.7753\n",
            "Epoch 351/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5843 - accuracy: 0.7765 - val_loss: 0.5913 - val_accuracy: 0.7745\n",
            "Epoch 352/500\n",
            "138/138 [==============================] - 111s 811ms/step - loss: 0.5727 - accuracy: 0.7789 - val_loss: 0.7341 - val_accuracy: 0.6990\n",
            "Epoch 353/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5510 - accuracy: 0.7850 - val_loss: 0.6634 - val_accuracy: 0.7409\n",
            "Epoch 354/500\n",
            "138/138 [==============================] - 110s 806ms/step - loss: 0.5662 - accuracy: 0.7814 - val_loss: 0.5994 - val_accuracy: 0.7734\n",
            "Epoch 355/500\n",
            "138/138 [==============================] - 111s 812ms/step - loss: 0.5617 - accuracy: 0.7818 - val_loss: 0.6115 - val_accuracy: 0.7789\n",
            "Epoch 356/500\n",
            "138/138 [==============================] - 115s 839ms/step - loss: 0.5551 - accuracy: 0.7858 - val_loss: 0.6913 - val_accuracy: 0.7568\n",
            "Epoch 357/500\n",
            "138/138 [==============================] - 115s 840ms/step - loss: 0.5525 - accuracy: 0.7869 - val_loss: 0.6904 - val_accuracy: 0.7443\n",
            "Epoch 358/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5551 - accuracy: 0.7831 - val_loss: 0.6162 - val_accuracy: 0.7719\n",
            "Epoch 359/500\n",
            "138/138 [==============================] - 117s 857ms/step - loss: 0.5639 - accuracy: 0.7851 - val_loss: 0.5928 - val_accuracy: 0.7695\n",
            "Epoch 360/500\n",
            "138/138 [==============================] - 115s 839ms/step - loss: 0.5540 - accuracy: 0.7847 - val_loss: 0.7150 - val_accuracy: 0.7549\n",
            "Epoch 361/500\n",
            "138/138 [==============================] - 116s 847ms/step - loss: 0.5547 - accuracy: 0.7859 - val_loss: 0.6244 - val_accuracy: 0.7682\n",
            "Epoch 362/500\n",
            "138/138 [==============================] - 117s 852ms/step - loss: 0.5443 - accuracy: 0.7917 - val_loss: 0.6404 - val_accuracy: 0.7560\n",
            "Epoch 363/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5516 - accuracy: 0.7891 - val_loss: 0.8236 - val_accuracy: 0.7599\n",
            "Epoch 364/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5492 - accuracy: 0.7872 - val_loss: 0.6058 - val_accuracy: 0.7805\n",
            "Epoch 365/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5428 - accuracy: 0.7903 - val_loss: 0.7376 - val_accuracy: 0.7732\n",
            "Epoch 366/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.7141 - accuracy: 0.7578 - val_loss: 0.6566 - val_accuracy: 0.7698\n",
            "Epoch 367/500\n",
            "138/138 [==============================] - 112s 815ms/step - loss: 0.5657 - accuracy: 0.7835 - val_loss: 0.8734 - val_accuracy: 0.6698\n",
            "Epoch 368/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5449 - accuracy: 0.7893 - val_loss: 0.8338 - val_accuracy: 0.7336\n",
            "Epoch 369/500\n",
            "138/138 [==============================] - 111s 808ms/step - loss: 0.5559 - accuracy: 0.7836 - val_loss: 0.7076 - val_accuracy: 0.7823\n",
            "Epoch 370/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5787 - accuracy: 0.7862 - val_loss: 0.8095 - val_accuracy: 0.7018\n",
            "Epoch 371/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.6915 - accuracy: 0.7800 - val_loss: 0.7003 - val_accuracy: 0.7232\n",
            "Epoch 372/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5591 - accuracy: 0.7838 - val_loss: 0.6264 - val_accuracy: 0.7628\n",
            "Epoch 373/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5583 - accuracy: 0.7874 - val_loss: 0.6701 - val_accuracy: 0.7339\n",
            "Epoch 374/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.5499 - accuracy: 0.7859 - val_loss: 0.6387 - val_accuracy: 0.7810\n",
            "Epoch 375/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5648 - accuracy: 0.7812 - val_loss: 0.6344 - val_accuracy: 0.7841\n",
            "Epoch 376/500\n",
            "138/138 [==============================] - 117s 852ms/step - loss: 0.5406 - accuracy: 0.7898 - val_loss: 0.6701 - val_accuracy: 0.7768\n",
            "Epoch 377/500\n",
            "138/138 [==============================] - 118s 861ms/step - loss: 0.5424 - accuracy: 0.7885 - val_loss: 0.6672 - val_accuracy: 0.7828\n",
            "Epoch 378/500\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.5487 - accuracy: 0.7877 - val_loss: 0.6938 - val_accuracy: 0.7424\n",
            "Epoch 379/500\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 0.5930 - accuracy: 0.7816 - val_loss: 0.6890 - val_accuracy: 0.7633\n",
            "Epoch 380/500\n",
            "138/138 [==============================] - 118s 863ms/step - loss: 0.5641 - accuracy: 0.7807 - val_loss: 0.7825 - val_accuracy: 0.7388\n",
            "Epoch 381/500\n",
            "138/138 [==============================] - 115s 838ms/step - loss: 0.5590 - accuracy: 0.7840 - val_loss: 0.7015 - val_accuracy: 0.7393\n",
            "Epoch 382/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5624 - accuracy: 0.7864 - val_loss: 0.9513 - val_accuracy: 0.7495\n",
            "Epoch 383/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.6123 - accuracy: 0.7806 - val_loss: 0.6057 - val_accuracy: 0.7654\n",
            "Epoch 384/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5758 - accuracy: 0.7814 - val_loss: 0.7172 - val_accuracy: 0.7448\n",
            "Epoch 385/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5842 - accuracy: 0.7826 - val_loss: 0.7290 - val_accuracy: 0.7141\n",
            "Epoch 386/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5636 - accuracy: 0.7853 - val_loss: 0.8077 - val_accuracy: 0.6956\n",
            "Epoch 387/500\n",
            "138/138 [==============================] - 111s 810ms/step - loss: 0.5563 - accuracy: 0.7838 - val_loss: 0.6332 - val_accuracy: 0.7688\n",
            "Epoch 388/500\n",
            "138/138 [==============================] - 111s 811ms/step - loss: 0.5965 - accuracy: 0.7843 - val_loss: 0.5768 - val_accuracy: 0.7820\n",
            "Epoch 389/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5400 - accuracy: 0.7918 - val_loss: 0.6340 - val_accuracy: 0.7753\n",
            "Epoch 390/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.5538 - accuracy: 0.7861 - val_loss: 0.7069 - val_accuracy: 0.7406\n",
            "Epoch 391/500\n",
            "138/138 [==============================] - 111s 809ms/step - loss: 0.5707 - accuracy: 0.7842 - val_loss: 0.6069 - val_accuracy: 0.7841\n",
            "Epoch 392/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5689 - accuracy: 0.7839 - val_loss: 0.5982 - val_accuracy: 0.7857\n",
            "Epoch 393/500\n",
            "138/138 [==============================] - 111s 813ms/step - loss: 0.6105 - accuracy: 0.7761 - val_loss: 0.6532 - val_accuracy: 0.7807\n",
            "Epoch 394/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5621 - accuracy: 0.7858 - val_loss: 0.6224 - val_accuracy: 0.7701\n",
            "Epoch 395/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5445 - accuracy: 0.7884 - val_loss: 0.7816 - val_accuracy: 0.7721\n",
            "Epoch 396/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5722 - accuracy: 0.7801 - val_loss: 0.6711 - val_accuracy: 0.7802\n",
            "Epoch 397/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5448 - accuracy: 0.7910 - val_loss: 0.7893 - val_accuracy: 0.7016\n",
            "Epoch 398/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.5552 - accuracy: 0.7916 - val_loss: 0.5716 - val_accuracy: 0.7815\n",
            "Epoch 399/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5428 - accuracy: 0.7908 - val_loss: 0.7947 - val_accuracy: 0.7466\n",
            "Epoch 400/500\n",
            "138/138 [==============================] - 112s 821ms/step - loss: 0.5392 - accuracy: 0.7926 - val_loss: 0.8634 - val_accuracy: 0.7185\n",
            "Epoch 401/500\n",
            "138/138 [==============================] - 113s 821ms/step - loss: 0.5574 - accuracy: 0.7885 - val_loss: 0.5845 - val_accuracy: 0.7766\n",
            "Epoch 402/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5439 - accuracy: 0.7939 - val_loss: 0.7566 - val_accuracy: 0.7336\n",
            "Epoch 403/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5459 - accuracy: 0.7928 - val_loss: 0.6830 - val_accuracy: 0.7620\n",
            "Epoch 404/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5871 - accuracy: 0.7913 - val_loss: 0.5781 - val_accuracy: 0.7833\n",
            "Epoch 405/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5444 - accuracy: 0.7895 - val_loss: 0.7596 - val_accuracy: 0.7068\n",
            "Epoch 406/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5402 - accuracy: 0.7926 - val_loss: 0.5757 - val_accuracy: 0.7893\n",
            "Epoch 407/500\n",
            "138/138 [==============================] - 114s 835ms/step - loss: 0.5595 - accuracy: 0.7923 - val_loss: 0.5846 - val_accuracy: 0.7844\n",
            "Epoch 408/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5569 - accuracy: 0.7895 - val_loss: 0.5907 - val_accuracy: 0.7706\n",
            "Epoch 409/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5471 - accuracy: 0.7901 - val_loss: 0.6473 - val_accuracy: 0.7466\n",
            "Epoch 410/500\n",
            "138/138 [==============================] - 115s 835ms/step - loss: 0.5467 - accuracy: 0.7892 - val_loss: 0.6681 - val_accuracy: 0.7526\n",
            "Epoch 411/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5658 - accuracy: 0.7833 - val_loss: 0.5996 - val_accuracy: 0.7745\n",
            "Epoch 412/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5369 - accuracy: 0.7933 - val_loss: 0.6479 - val_accuracy: 0.7591\n",
            "Epoch 413/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5576 - accuracy: 0.7868 - val_loss: 0.6412 - val_accuracy: 0.7557\n",
            "Epoch 414/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5609 - accuracy: 0.7838 - val_loss: 0.6568 - val_accuracy: 0.7750\n",
            "Epoch 415/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5725 - accuracy: 0.7832 - val_loss: 0.7763 - val_accuracy: 0.7354\n",
            "Epoch 416/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5890 - accuracy: 0.7846 - val_loss: 0.6956 - val_accuracy: 0.7469\n",
            "Epoch 417/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5504 - accuracy: 0.7874 - val_loss: 0.7046 - val_accuracy: 0.7346\n",
            "Epoch 418/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5377 - accuracy: 0.7915 - val_loss: 0.6840 - val_accuracy: 0.7552\n",
            "Epoch 419/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.5650 - accuracy: 0.7865 - val_loss: 0.5892 - val_accuracy: 0.7833\n",
            "Epoch 420/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5393 - accuracy: 0.7922 - val_loss: 0.5765 - val_accuracy: 0.7805\n",
            "Epoch 421/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.5487 - accuracy: 0.7876 - val_loss: 0.6282 - val_accuracy: 0.7768\n",
            "Epoch 422/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.6871 - accuracy: 0.7817 - val_loss: 0.6396 - val_accuracy: 0.7630\n",
            "Epoch 423/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5600 - accuracy: 0.7950 - val_loss: 0.6125 - val_accuracy: 0.7820\n",
            "Epoch 424/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5448 - accuracy: 0.7918 - val_loss: 0.5895 - val_accuracy: 0.7826\n",
            "Epoch 425/500\n",
            "138/138 [==============================] - 114s 831ms/step - loss: 0.5397 - accuracy: 0.7939 - val_loss: 0.6581 - val_accuracy: 0.7625\n",
            "Epoch 426/500\n",
            "138/138 [==============================] - 115s 836ms/step - loss: 0.5486 - accuracy: 0.7891 - val_loss: 0.5982 - val_accuracy: 0.7818\n",
            "Epoch 427/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.6735 - accuracy: 0.7771 - val_loss: 0.6202 - val_accuracy: 0.7740\n",
            "Epoch 428/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5709 - accuracy: 0.7816 - val_loss: 0.6390 - val_accuracy: 0.7708\n",
            "Epoch 429/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5682 - accuracy: 0.7876 - val_loss: 0.6016 - val_accuracy: 0.7812\n",
            "Epoch 430/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5519 - accuracy: 0.7870 - val_loss: 1.0393 - val_accuracy: 0.6375\n",
            "Epoch 431/500\n",
            "138/138 [==============================] - 112s 816ms/step - loss: 0.5436 - accuracy: 0.7907 - val_loss: 0.6541 - val_accuracy: 0.7732\n",
            "Epoch 432/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5485 - accuracy: 0.7875 - val_loss: 0.6714 - val_accuracy: 0.7581\n",
            "Epoch 433/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5462 - accuracy: 0.7906 - val_loss: 0.7084 - val_accuracy: 0.7552\n",
            "Epoch 434/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5584 - accuracy: 0.7837 - val_loss: 0.6317 - val_accuracy: 0.7802\n",
            "Epoch 435/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5487 - accuracy: 0.7890 - val_loss: 0.7063 - val_accuracy: 0.7388\n",
            "Epoch 436/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.6873 - accuracy: 0.7707 - val_loss: 0.7114 - val_accuracy: 0.6971\n",
            "Epoch 437/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.5438 - accuracy: 0.7934 - val_loss: 0.6239 - val_accuracy: 0.7602\n",
            "Epoch 438/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5397 - accuracy: 0.7905 - val_loss: 0.6446 - val_accuracy: 0.7385\n",
            "Epoch 439/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5502 - accuracy: 0.7926 - val_loss: 0.6363 - val_accuracy: 0.7745\n",
            "Epoch 440/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5473 - accuracy: 0.7881 - val_loss: 1.0418 - val_accuracy: 0.6255\n",
            "Epoch 441/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5631 - accuracy: 0.7827 - val_loss: 0.8955 - val_accuracy: 0.7117\n",
            "Epoch 442/500\n",
            "138/138 [==============================] - 115s 839ms/step - loss: 0.5447 - accuracy: 0.7907 - val_loss: 0.6386 - val_accuracy: 0.7784\n",
            "Epoch 443/500\n",
            "138/138 [==============================] - 115s 838ms/step - loss: 0.5530 - accuracy: 0.7873 - val_loss: 0.6342 - val_accuracy: 0.7695\n",
            "Epoch 444/500\n",
            "138/138 [==============================] - 115s 841ms/step - loss: 0.5664 - accuracy: 0.7849 - val_loss: 0.6296 - val_accuracy: 0.7729\n",
            "Epoch 445/500\n",
            "138/138 [==============================] - 115s 842ms/step - loss: 0.5761 - accuracy: 0.7878 - val_loss: 0.7016 - val_accuracy: 0.7404\n",
            "Epoch 446/500\n",
            "138/138 [==============================] - 116s 846ms/step - loss: 0.5478 - accuracy: 0.7878 - val_loss: 1.3354 - val_accuracy: 0.6586\n",
            "Epoch 447/500\n",
            "138/138 [==============================] - 117s 850ms/step - loss: 0.5545 - accuracy: 0.7861 - val_loss: 0.6338 - val_accuracy: 0.7781\n",
            "Epoch 448/500\n",
            "138/138 [==============================] - 115s 841ms/step - loss: 0.5427 - accuracy: 0.7904 - val_loss: 0.7242 - val_accuracy: 0.7531\n",
            "Epoch 449/500\n",
            "138/138 [==============================] - 114s 831ms/step - loss: 0.5390 - accuracy: 0.7930 - val_loss: 0.7460 - val_accuracy: 0.7258\n",
            "Epoch 450/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5477 - accuracy: 0.7887 - val_loss: 0.6665 - val_accuracy: 0.7714\n",
            "Epoch 451/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5396 - accuracy: 0.7910 - val_loss: 0.6480 - val_accuracy: 0.7766\n",
            "Epoch 452/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5412 - accuracy: 0.7972 - val_loss: 0.8591 - val_accuracy: 0.6615\n",
            "Epoch 453/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5411 - accuracy: 0.7935 - val_loss: 0.7909 - val_accuracy: 0.7547\n",
            "Epoch 454/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.6378 - accuracy: 0.7886 - val_loss: 0.5838 - val_accuracy: 0.7833\n",
            "Epoch 455/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5526 - accuracy: 0.7895 - val_loss: 0.6911 - val_accuracy: 0.7542\n",
            "Epoch 456/500\n",
            "138/138 [==============================] - 113s 828ms/step - loss: 0.5319 - accuracy: 0.7940 - val_loss: 0.6492 - val_accuracy: 0.7680\n",
            "Epoch 457/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5749 - accuracy: 0.7863 - val_loss: 0.5813 - val_accuracy: 0.7859\n",
            "Epoch 458/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5740 - accuracy: 0.7790 - val_loss: 0.6444 - val_accuracy: 0.7698\n",
            "Epoch 459/500\n",
            "138/138 [==============================] - 114s 828ms/step - loss: 0.6176 - accuracy: 0.7684 - val_loss: 0.9874 - val_accuracy: 0.6896\n",
            "Epoch 460/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5665 - accuracy: 0.7868 - val_loss: 0.5922 - val_accuracy: 0.7661\n",
            "Epoch 461/500\n",
            "138/138 [==============================] - 112s 821ms/step - loss: 0.5643 - accuracy: 0.7858 - val_loss: 0.5716 - val_accuracy: 0.7846\n",
            "Epoch 462/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.6024 - accuracy: 0.7741 - val_loss: 0.6713 - val_accuracy: 0.7661\n",
            "Epoch 463/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5548 - accuracy: 0.7868 - val_loss: 0.6042 - val_accuracy: 0.7836\n",
            "Epoch 464/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5832 - accuracy: 0.7775 - val_loss: 0.6387 - val_accuracy: 0.7786\n",
            "Epoch 465/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5513 - accuracy: 0.7858 - val_loss: 0.7271 - val_accuracy: 0.7466\n",
            "Epoch 466/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.6124 - accuracy: 0.7546 - val_loss: 0.8112 - val_accuracy: 0.6990\n",
            "Epoch 467/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.6018 - accuracy: 0.7726 - val_loss: 0.6188 - val_accuracy: 0.7505\n",
            "Epoch 468/500\n",
            "138/138 [==============================] - 115s 838ms/step - loss: 0.7376 - accuracy: 0.7458 - val_loss: 0.9574 - val_accuracy: 0.6526\n",
            "Epoch 469/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5670 - accuracy: 0.7831 - val_loss: 0.6478 - val_accuracy: 0.7729\n",
            "Epoch 470/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.7018 - accuracy: 0.7805 - val_loss: 0.6240 - val_accuracy: 0.7849\n",
            "Epoch 471/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5496 - accuracy: 0.7897 - val_loss: 1.3832 - val_accuracy: 0.6932\n",
            "Epoch 472/500\n",
            "138/138 [==============================] - 115s 837ms/step - loss: 0.5592 - accuracy: 0.7874 - val_loss: 0.6788 - val_accuracy: 0.7638\n",
            "Epoch 473/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5755 - accuracy: 0.7902 - val_loss: 0.5737 - val_accuracy: 0.7826\n",
            "Epoch 474/500\n",
            "138/138 [==============================] - 113s 822ms/step - loss: 0.6019 - accuracy: 0.7751 - val_loss: 0.6900 - val_accuracy: 0.7206\n",
            "Epoch 475/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5529 - accuracy: 0.7830 - val_loss: 0.8641 - val_accuracy: 0.6617\n",
            "Epoch 476/500\n",
            "138/138 [==============================] - 112s 820ms/step - loss: 0.5645 - accuracy: 0.7852 - val_loss: 0.7461 - val_accuracy: 0.7185\n",
            "Epoch 477/500\n",
            "138/138 [==============================] - 114s 829ms/step - loss: 0.5827 - accuracy: 0.7879 - val_loss: 0.6241 - val_accuracy: 0.7620\n",
            "Epoch 478/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5416 - accuracy: 0.7923 - val_loss: 0.6495 - val_accuracy: 0.7875\n",
            "Epoch 479/500\n",
            "138/138 [==============================] - 114s 834ms/step - loss: 0.5555 - accuracy: 0.7873 - val_loss: 0.6694 - val_accuracy: 0.7721\n",
            "Epoch 480/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5504 - accuracy: 0.7844 - val_loss: 0.6739 - val_accuracy: 0.7404\n",
            "Epoch 481/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5504 - accuracy: 0.7899 - val_loss: 0.6522 - val_accuracy: 0.7703\n",
            "Epoch 482/500\n",
            "138/138 [==============================] - 113s 826ms/step - loss: 0.5697 - accuracy: 0.7856 - val_loss: 0.6083 - val_accuracy: 0.7779\n",
            "Epoch 483/500\n",
            "138/138 [==============================] - 113s 824ms/step - loss: 0.5623 - accuracy: 0.7845 - val_loss: 0.6590 - val_accuracy: 0.7680\n",
            "Epoch 484/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5441 - accuracy: 0.7899 - val_loss: 0.5811 - val_accuracy: 0.7857\n",
            "Epoch 485/500\n",
            "138/138 [==============================] - 113s 823ms/step - loss: 0.5431 - accuracy: 0.7894 - val_loss: 0.7089 - val_accuracy: 0.7740\n",
            "Epoch 486/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5486 - accuracy: 0.7871 - val_loss: 0.7845 - val_accuracy: 0.7201\n",
            "Epoch 487/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5496 - accuracy: 0.7911 - val_loss: 0.6107 - val_accuracy: 0.7794\n",
            "Epoch 488/500\n",
            "138/138 [==============================] - 113s 827ms/step - loss: 0.5426 - accuracy: 0.7931 - val_loss: 0.7137 - val_accuracy: 0.7521\n",
            "Epoch 489/500\n",
            "138/138 [==============================] - 114s 833ms/step - loss: 0.5547 - accuracy: 0.7841 - val_loss: 0.8079 - val_accuracy: 0.7029\n",
            "Epoch 490/500\n",
            "138/138 [==============================] - 114s 832ms/step - loss: 0.5849 - accuracy: 0.7860 - val_loss: 0.5775 - val_accuracy: 0.7839\n",
            "Epoch 491/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5326 - accuracy: 0.7943 - val_loss: 0.7147 - val_accuracy: 0.7703\n",
            "Epoch 492/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5444 - accuracy: 0.7910 - val_loss: 0.6744 - val_accuracy: 0.7563\n",
            "Epoch 493/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5366 - accuracy: 0.7910 - val_loss: 0.7389 - val_accuracy: 0.7130\n",
            "Epoch 494/500\n",
            "138/138 [==============================] - 114s 830ms/step - loss: 0.5484 - accuracy: 0.7905 - val_loss: 0.8764 - val_accuracy: 0.6310\n",
            "Epoch 495/500\n",
            "138/138 [==============================] - 112s 814ms/step - loss: 0.5457 - accuracy: 0.7906 - val_loss: 0.6819 - val_accuracy: 0.7435\n",
            "Epoch 496/500\n",
            "138/138 [==============================] - 112s 819ms/step - loss: 0.5891 - accuracy: 0.7930 - val_loss: 0.5891 - val_accuracy: 0.7755\n",
            "Epoch 497/500\n",
            "138/138 [==============================] - 112s 818ms/step - loss: 0.5398 - accuracy: 0.7938 - val_loss: 0.6740 - val_accuracy: 0.7461\n",
            "Epoch 498/500\n",
            "138/138 [==============================] - 112s 817ms/step - loss: 0.6489 - accuracy: 0.7973 - val_loss: 0.5655 - val_accuracy: 0.7818\n",
            "Epoch 499/500\n",
            "138/138 [==============================] - 114s 835ms/step - loss: 0.5460 - accuracy: 0.7905 - val_loss: 0.6645 - val_accuracy: 0.7620\n",
            "Epoch 500/500\n",
            "138/138 [==============================] - 113s 825ms/step - loss: 0.5391 - accuracy: 0.7928 - val_loss: 0.7144 - val_accuracy: 0.7492\n"
          ]
        }
      ],
      "source": [
        "def generator(data, labels, batch_size):\n",
        "  while True:\n",
        "    indices = np.random.permutation(len(data))\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      yield data[batch_indices], labels[batch_indices]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(4, input_shape=(393816,), activation='linear'),\n",
        "])\n",
        "\n",
        "for _ in range(10):\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "\n",
        "model.compile( loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "model.load_weights(os.path.join('/content/drive/MyDrive/ASC/results/models/exp3/', 'f_model_weights_epoch170_val_acc0.7909.h5'))\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/exp3/'\n",
        "if not os.path.exists(checkpoint_filepath):\n",
        "  os.mkdir(checkpoint_filepath)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'z_model_weights_epoch{epoch:02d}_val_acc{val_accuracy:.4f}.h5'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback, history_logger_1]\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "X_train = X_final_train_bert[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_final_train_bert[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=500,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=callbacks_list_1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXP4"
      ],
      "metadata": {
        "id": "21wGqCraz6Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(data, labels, batch_size):\n",
        "  while True:\n",
        "    indices = np.random.permutation(len(data))\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      yield data[batch_indices], labels[batch_indices]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(4, input_shape=(393816,), activation='linear'),\n",
        "])\n",
        "\n",
        "for _ in range(15):\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "\n",
        "model.compile( loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "# model.load_weights(os.path.join('/content/drive/MyDrive/ASC/results/models/exp3/', 'f_model_weights_epoch170_val_acc0.7909.h5'))\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/exp4/'\n",
        "if not os.path.exists(checkpoint_filepath):\n",
        "  os.mkdir(checkpoint_filepath)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'z_model_weights_epoch{epoch:02d}_val_acc{val_accuracy:.4f}.h5'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback, history_logger_1]\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "X_train = X_final_train_bert[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_final_train_bert[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=1000,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=callbacks_list_1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_pBjeTSzoIZ",
        "outputId": "4a5aefd6-8bc5-4ac9-dcf5-32eb0651307d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-507b1e3a1ba6>:51: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "138/138 [==============================] - 125s 873ms/step - loss: 1.0983 - accuracy: 0.3428 - val_loss: 1.0948 - val_accuracy: 0.3547\n",
            "Epoch 2/1000\n",
            "138/138 [==============================] - 119s 866ms/step - loss: 1.0826 - accuracy: 0.3938 - val_loss: 1.0782 - val_accuracy: 0.3292\n",
            "Epoch 3/1000\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 1.0530 - accuracy: 0.4272 - val_loss: 1.1327 - val_accuracy: 0.3208\n",
            "Epoch 4/1000\n",
            "138/138 [==============================] - 120s 872ms/step - loss: 1.0507 - accuracy: 0.4355 - val_loss: 1.0182 - val_accuracy: 0.4815\n",
            "Epoch 5/1000\n",
            "138/138 [==============================] - 119s 865ms/step - loss: 1.0364 - accuracy: 0.4465 - val_loss: 1.3076 - val_accuracy: 0.3260\n",
            "Epoch 6/1000\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 1.0395 - accuracy: 0.4467 - val_loss: 1.0217 - val_accuracy: 0.4781\n",
            "Epoch 7/1000\n",
            "138/138 [==============================] - 119s 865ms/step - loss: 1.0405 - accuracy: 0.4432 - val_loss: 1.0839 - val_accuracy: 0.4448\n",
            "Epoch 8/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 1.0374 - accuracy: 0.4562 - val_loss: 1.0022 - val_accuracy: 0.5102\n",
            "Epoch 9/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 1.0375 - accuracy: 0.4528 - val_loss: 0.9825 - val_accuracy: 0.5005\n",
            "Epoch 10/1000\n",
            "138/138 [==============================] - 120s 873ms/step - loss: 1.0313 - accuracy: 0.4666 - val_loss: 2.3860 - val_accuracy: 0.4346\n",
            "Epoch 11/1000\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 1.0347 - accuracy: 0.4635 - val_loss: 1.0002 - val_accuracy: 0.4641\n",
            "Epoch 12/1000\n",
            "138/138 [==============================] - 120s 871ms/step - loss: 1.0160 - accuracy: 0.4708 - val_loss: 0.9762 - val_accuracy: 0.4951\n",
            "Epoch 13/1000\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.9954 - accuracy: 0.4817 - val_loss: 1.0158 - val_accuracy: 0.4495\n",
            "Epoch 14/1000\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 0.9881 - accuracy: 0.4910 - val_loss: 0.9512 - val_accuracy: 0.5005\n",
            "Epoch 15/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.9728 - accuracy: 0.5011 - val_loss: 0.8894 - val_accuracy: 0.5549\n",
            "Epoch 16/1000\n",
            "138/138 [==============================] - 120s 874ms/step - loss: 0.9700 - accuracy: 0.5045 - val_loss: 0.9170 - val_accuracy: 0.5508\n",
            "Epoch 17/1000\n",
            "138/138 [==============================] - 118s 863ms/step - loss: 0.9660 - accuracy: 0.5104 - val_loss: 0.9353 - val_accuracy: 0.5190\n",
            "Epoch 18/1000\n",
            "138/138 [==============================] - 120s 872ms/step - loss: 0.9600 - accuracy: 0.5088 - val_loss: 0.8921 - val_accuracy: 0.5794\n",
            "Epoch 19/1000\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 0.9592 - accuracy: 0.5090 - val_loss: 0.9066 - val_accuracy: 0.5542\n",
            "Epoch 20/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.9481 - accuracy: 0.5215 - val_loss: 0.9346 - val_accuracy: 0.5273\n",
            "Epoch 21/1000\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.9420 - accuracy: 0.5254 - val_loss: 0.9331 - val_accuracy: 0.5112\n",
            "Epoch 22/1000\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 0.9404 - accuracy: 0.5299 - val_loss: 0.9133 - val_accuracy: 0.5688\n",
            "Epoch 23/1000\n",
            "138/138 [==============================] - 119s 865ms/step - loss: 0.9264 - accuracy: 0.5435 - val_loss: 0.8766 - val_accuracy: 0.5885\n",
            "Epoch 24/1000\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.9334 - accuracy: 0.5287 - val_loss: 0.9805 - val_accuracy: 0.4818\n",
            "Epoch 25/1000\n",
            "138/138 [==============================] - 119s 865ms/step - loss: 0.9278 - accuracy: 0.5338 - val_loss: 0.9278 - val_accuracy: 0.5513\n",
            "Epoch 26/1000\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 0.9176 - accuracy: 0.5486 - val_loss: 0.8564 - val_accuracy: 0.5943\n",
            "Epoch 27/1000\n",
            "138/138 [==============================] - 118s 863ms/step - loss: 0.9164 - accuracy: 0.5495 - val_loss: 0.8637 - val_accuracy: 0.5904\n",
            "Epoch 28/1000\n",
            "138/138 [==============================] - 119s 865ms/step - loss: 0.9162 - accuracy: 0.5437 - val_loss: 0.8697 - val_accuracy: 0.5786\n",
            "Epoch 29/1000\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 0.9220 - accuracy: 0.5358 - val_loss: 0.8593 - val_accuracy: 0.5865\n",
            "Epoch 30/1000\n",
            "138/138 [==============================] - 119s 867ms/step - loss: 0.9116 - accuracy: 0.5529 - val_loss: 0.8987 - val_accuracy: 0.5375\n",
            "Epoch 31/1000\n",
            "138/138 [==============================] - 120s 871ms/step - loss: 0.9089 - accuracy: 0.5515 - val_loss: 0.9983 - val_accuracy: 0.4742\n",
            "Epoch 32/1000\n",
            "138/138 [==============================] - 120s 871ms/step - loss: 0.9004 - accuracy: 0.5591 - val_loss: 0.8919 - val_accuracy: 0.5750\n",
            "Epoch 33/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.8967 - accuracy: 0.5556 - val_loss: 0.9000 - val_accuracy: 0.5380\n",
            "Epoch 34/1000\n",
            "138/138 [==============================] - 120s 874ms/step - loss: 0.8899 - accuracy: 0.5666 - val_loss: 0.8515 - val_accuracy: 0.6044\n",
            "Epoch 35/1000\n",
            "138/138 [==============================] - 119s 865ms/step - loss: 0.8892 - accuracy: 0.5650 - val_loss: 0.8961 - val_accuracy: 0.5719\n",
            "Epoch 36/1000\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 0.8842 - accuracy: 0.5721 - val_loss: 0.9543 - val_accuracy: 0.5458\n",
            "Epoch 37/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.8803 - accuracy: 0.5728 - val_loss: 0.8408 - val_accuracy: 0.5940\n",
            "Epoch 38/1000\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 0.8808 - accuracy: 0.5709 - val_loss: 0.8205 - val_accuracy: 0.6141\n",
            "Epoch 39/1000\n",
            "138/138 [==============================] - 119s 871ms/step - loss: 0.8859 - accuracy: 0.5701 - val_loss: 0.8337 - val_accuracy: 0.6068\n",
            "Epoch 40/1000\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 0.8739 - accuracy: 0.5760 - val_loss: 0.8701 - val_accuracy: 0.5906\n",
            "Epoch 41/1000\n",
            "138/138 [==============================] - 120s 873ms/step - loss: 0.8868 - accuracy: 0.5600 - val_loss: 0.8426 - val_accuracy: 0.6023\n",
            "Epoch 42/1000\n",
            "138/138 [==============================] - 119s 871ms/step - loss: 0.8744 - accuracy: 0.5768 - val_loss: 0.8441 - val_accuracy: 0.6091\n",
            "Epoch 43/1000\n",
            "138/138 [==============================] - 120s 874ms/step - loss: 0.8764 - accuracy: 0.5760 - val_loss: 0.8234 - val_accuracy: 0.6133\n",
            "Epoch 44/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.8715 - accuracy: 0.5776 - val_loss: 0.9297 - val_accuracy: 0.5581\n",
            "Epoch 45/1000\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.8682 - accuracy: 0.5825 - val_loss: 0.9692 - val_accuracy: 0.4648\n",
            "Epoch 46/1000\n",
            "138/138 [==============================] - 120s 872ms/step - loss: 0.8707 - accuracy: 0.5759 - val_loss: 0.9852 - val_accuracy: 0.5357\n",
            "Epoch 47/1000\n",
            "138/138 [==============================] - 119s 871ms/step - loss: 0.8611 - accuracy: 0.5849 - val_loss: 0.8431 - val_accuracy: 0.6052\n",
            "Epoch 48/1000\n",
            "138/138 [==============================] - 119s 868ms/step - loss: 0.8617 - accuracy: 0.5854 - val_loss: 0.8805 - val_accuracy: 0.5815\n",
            "Epoch 49/1000\n",
            "138/138 [==============================] - 119s 869ms/step - loss: 0.8609 - accuracy: 0.5854 - val_loss: 0.8369 - val_accuracy: 0.5971\n",
            "Epoch 50/1000\n",
            "138/138 [==============================] - 120s 872ms/step - loss: 0.8525 - accuracy: 0.5917 - val_loss: 0.8165 - val_accuracy: 0.6135\n",
            "Epoch 51/1000\n",
            "138/138 [==============================] - 119s 870ms/step - loss: 0.8577 - accuracy: 0.5856 - val_loss: 0.8135 - val_accuracy: 0.6185\n",
            "Epoch 52/1000\n",
            " 81/138 [================>.............] - ETA: 43s - loss: 0.8547 - accuracy: 0.5896"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0cNqEY53nk6",
        "outputId": "3dae15ea-1148-4715-fa7b-f7fc16cd99e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(3940, 3)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wRteO0W3ngB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YS6A6LCc3nbN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnSMr-i-CM4h",
        "outputId": "1abc4779-83d7-4696-8d12-4e605bc4f196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-43268f0e32e1>:95: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0990 - accuracy: 0.3363\n",
            "Epoch 1: val_accuracy improved from -inf to 0.33003, saving model to /content/drive/MyDrive/ASC/results/models/DLA1/model_weights_epoch01.h5\n",
            "4432/4432 [==============================] - 74s 16ms/step - loss: 1.0990 - accuracy: 0.3362 - val_loss: 1.0995 - val_accuracy: 0.3300\n",
            "Epoch 2/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3339\n",
            "Epoch 2: val_accuracy improved from 0.33003 to 0.34257, saving model to /content/drive/MyDrive/ASC/results/models/DLA1/model_weights_epoch02.h5\n",
            "4432/4432 [==============================] - 67s 15ms/step - loss: 1.0988 - accuracy: 0.3339 - val_loss: 1.0986 - val_accuracy: 0.3426\n",
            "Epoch 3/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0997 - accuracy: 0.3321\n",
            "Epoch 3: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 68s 15ms/step - loss: 1.0997 - accuracy: 0.3321 - val_loss: 1.0988 - val_accuracy: 0.3301\n",
            "Epoch 4/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3306\n",
            "Epoch 4: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 67s 15ms/step - loss: 1.0988 - accuracy: 0.3307 - val_loss: 1.0977 - val_accuracy: 0.3281\n",
            "Epoch 5/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.1016 - accuracy: 0.3325\n",
            "Epoch 5: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 66s 15ms/step - loss: 1.1016 - accuracy: 0.3325 - val_loss: 1.0986 - val_accuracy: 0.3296\n",
            "Epoch 6/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3370\n",
            "Epoch 6: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 67s 15ms/step - loss: 1.0987 - accuracy: 0.3370 - val_loss: 1.0991 - val_accuracy: 0.3299\n",
            "Epoch 7/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0991 - accuracy: 0.3343\n",
            "Epoch 7: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 66s 15ms/step - loss: 1.0991 - accuracy: 0.3343 - val_loss: 1.0987 - val_accuracy: 0.3296\n",
            "Epoch 8/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3287\n",
            "Epoch 8: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 67s 15ms/step - loss: 1.0988 - accuracy: 0.3288 - val_loss: 1.0987 - val_accuracy: 0.3278\n",
            "Epoch 9/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3304\n",
            "Epoch 9: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3304 - val_loss: 1.0984 - val_accuracy: 0.3299\n",
            "Epoch 10/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3370\n",
            "Epoch 10: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 62s 14ms/step - loss: 1.0987 - accuracy: 0.3370 - val_loss: 1.0985 - val_accuracy: 0.3301\n",
            "Epoch 11/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3341\n",
            "Epoch 11: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0981 - val_accuracy: 0.3423\n",
            "Epoch 12/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3311\n",
            "Epoch 12: val_accuracy did not improve from 0.34257\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3311 - val_loss: 1.0984 - val_accuracy: 0.3423\n",
            "Epoch 13/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3363\n",
            "Epoch 13: val_accuracy improved from 0.34257 to 0.34283, saving model to /content/drive/MyDrive/ASC/results/models/DLA1/model_weights_epoch13.h5\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3363 - val_loss: 1.0986 - val_accuracy: 0.3428\n",
            "Epoch 14/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3325\n",
            "Epoch 14: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3325 - val_loss: 1.0986 - val_accuracy: 0.3428\n",
            "Epoch 15/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3324\n",
            "Epoch 15: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3326 - val_loss: 1.0984 - val_accuracy: 0.3428\n",
            "Epoch 16/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3349\n",
            "Epoch 16: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3349 - val_loss: 1.0991 - val_accuracy: 0.3296\n",
            "Epoch 17/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3339\n",
            "Epoch 17: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3339 - val_loss: 1.0986 - val_accuracy: 0.3428\n",
            "Epoch 18/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3371\n",
            "Epoch 18: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3371 - val_loss: 1.0992 - val_accuracy: 0.3296\n",
            "Epoch 19/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3393\n",
            "Epoch 19: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3393 - val_loss: 1.0990 - val_accuracy: 0.3296\n",
            "Epoch 20/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3346\n",
            "Epoch 20: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3346 - val_loss: 1.0982 - val_accuracy: 0.3296\n",
            "Epoch 21/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 21: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0986 - val_accuracy: 0.3421\n",
            "Epoch 22/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3358\n",
            "Epoch 22: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3357 - val_loss: 1.0998 - val_accuracy: 0.3299\n",
            "Epoch 23/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3367\n",
            "Epoch 23: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3367 - val_loss: 1.0988 - val_accuracy: 0.3299\n",
            "Epoch 24/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3325\n",
            "Epoch 24: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0988 - accuracy: 0.3325 - val_loss: 1.0986 - val_accuracy: 0.3301\n",
            "Epoch 25/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3326\n",
            "Epoch 25: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3325 - val_loss: 1.0987 - val_accuracy: 0.3291\n",
            "Epoch 26/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3355\n",
            "Epoch 26: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3356 - val_loss: 1.0986 - val_accuracy: 0.3299\n",
            "Epoch 27/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3345\n",
            "Epoch 27: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3345 - val_loss: 1.0986 - val_accuracy: 0.3304\n",
            "Epoch 28/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3358\n",
            "Epoch 28: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3359 - val_loss: 1.0987 - val_accuracy: 0.3291\n",
            "Epoch 29/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3360\n",
            "Epoch 29: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3360 - val_loss: 1.0993 - val_accuracy: 0.3293\n",
            "Epoch 30/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 30: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3343 - val_loss: 1.0986 - val_accuracy: 0.3423\n",
            "Epoch 31/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3388\n",
            "Epoch 31: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3388 - val_loss: 1.0981 - val_accuracy: 0.3426\n",
            "Epoch 32/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3331\n",
            "Epoch 32: val_accuracy did not improve from 0.34283\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3331 - val_loss: 1.0988 - val_accuracy: 0.3301\n",
            "Epoch 33/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3363\n",
            "Epoch 33: val_accuracy improved from 0.34283 to 0.34308, saving model to /content/drive/MyDrive/ASC/results/models/DLA1/model_weights_epoch33.h5\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3363 - val_loss: 1.0986 - val_accuracy: 0.3431\n",
            "Epoch 34/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3354\n",
            "Epoch 34: val_accuracy improved from 0.34308 to 0.34334, saving model to /content/drive/MyDrive/ASC/results/models/DLA1/model_weights_epoch34.h5\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3354 - val_loss: 1.0985 - val_accuracy: 0.3433\n",
            "Epoch 35/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3314\n",
            "Epoch 35: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0988 - accuracy: 0.3314 - val_loss: 1.0983 - val_accuracy: 0.3296\n",
            "Epoch 36/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3321\n",
            "Epoch 36: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3321 - val_loss: 1.0990 - val_accuracy: 0.3293\n",
            "Epoch 37/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3353\n",
            "Epoch 37: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0980 - val_accuracy: 0.3426\n",
            "Epoch 38/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3322\n",
            "Epoch 38: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3322 - val_loss: 1.0991 - val_accuracy: 0.3293\n",
            "Epoch 39/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3346\n",
            "Epoch 39: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3347 - val_loss: 1.0986 - val_accuracy: 0.3426\n",
            "Epoch 40/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3369\n",
            "Epoch 40: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3368 - val_loss: 1.0988 - val_accuracy: 0.3278\n",
            "Epoch 41/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3329\n",
            "Epoch 41: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0988 - accuracy: 0.3329 - val_loss: 1.0990 - val_accuracy: 0.3299\n",
            "Epoch 42/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3345\n",
            "Epoch 42: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3345 - val_loss: 1.0987 - val_accuracy: 0.3293\n",
            "Epoch 43/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3347\n",
            "Epoch 43: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3347 - val_loss: 1.0987 - val_accuracy: 0.3301\n",
            "Epoch 44/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3333\n",
            "Epoch 44: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0985 - val_accuracy: 0.3296\n",
            "Epoch 45/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3367\n",
            "Epoch 45: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3367 - val_loss: 1.0990 - val_accuracy: 0.3433\n",
            "Epoch 46/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3360\n",
            "Epoch 46: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3360 - val_loss: 1.0987 - val_accuracy: 0.3299\n",
            "Epoch 47/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 47: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3340 - val_loss: 1.0990 - val_accuracy: 0.3301\n",
            "Epoch 48/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3370\n",
            "Epoch 48: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3370 - val_loss: 1.0992 - val_accuracy: 0.3421\n",
            "Epoch 49/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3331\n",
            "Epoch 49: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3332 - val_loss: 1.0985 - val_accuracy: 0.3428\n",
            "Epoch 50/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 50: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3343 - val_loss: 1.0983 - val_accuracy: 0.3418\n",
            "Epoch 51/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 51: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0987 - val_accuracy: 0.3299\n",
            "Epoch 52/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3323\n",
            "Epoch 52: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3323 - val_loss: 1.0984 - val_accuracy: 0.3278\n",
            "Epoch 53/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3320\n",
            "Epoch 53: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0988 - accuracy: 0.3320 - val_loss: 1.0984 - val_accuracy: 0.3304\n",
            "Epoch 54/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3329\n",
            "Epoch 54: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3330 - val_loss: 1.0988 - val_accuracy: 0.3296\n",
            "Epoch 55/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3305\n",
            "Epoch 55: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3305 - val_loss: 1.0987 - val_accuracy: 0.3293\n",
            "Epoch 56/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 56: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0989 - val_accuracy: 0.3299\n",
            "Epoch 57/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 57: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3343 - val_loss: 1.0988 - val_accuracy: 0.3299\n",
            "Epoch 58/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3395\n",
            "Epoch 58: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3394 - val_loss: 1.0987 - val_accuracy: 0.3428\n",
            "Epoch 59/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3319\n",
            "Epoch 59: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3319 - val_loss: 1.0984 - val_accuracy: 0.3431\n",
            "Epoch 60/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 60: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0994 - val_accuracy: 0.3299\n",
            "Epoch 61/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3337\n",
            "Epoch 61: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3337 - val_loss: 1.0986 - val_accuracy: 0.3428\n",
            "Epoch 62/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3358\n",
            "Epoch 62: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3359 - val_loss: 1.0982 - val_accuracy: 0.3426\n",
            "Epoch 63/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3328\n",
            "Epoch 63: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3328 - val_loss: 1.0983 - val_accuracy: 0.3278\n",
            "Epoch 64/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3346\n",
            "Epoch 64: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3346 - val_loss: 1.0988 - val_accuracy: 0.3273\n",
            "Epoch 65/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3311\n",
            "Epoch 65: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3311 - val_loss: 1.0987 - val_accuracy: 0.3426\n",
            "Epoch 66/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3346\n",
            "Epoch 66: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3346 - val_loss: 1.0986 - val_accuracy: 0.3293\n",
            "Epoch 67/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3330\n",
            "Epoch 67: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3330 - val_loss: 1.0984 - val_accuracy: 0.3426\n",
            "Epoch 68/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3292\n",
            "Epoch 68: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3292 - val_loss: 1.0983 - val_accuracy: 0.3431\n",
            "Epoch 69/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3313\n",
            "Epoch 69: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3313 - val_loss: 1.0987 - val_accuracy: 0.3299\n",
            "Epoch 70/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3356\n",
            "Epoch 70: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3356 - val_loss: 1.0990 - val_accuracy: 0.3296\n",
            "Epoch 71/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3366\n",
            "Epoch 71: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3366 - val_loss: 1.0986 - val_accuracy: 0.3418\n",
            "Epoch 72/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3377\n",
            "Epoch 72: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3376 - val_loss: 1.0982 - val_accuracy: 0.3423\n",
            "Epoch 73/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3340\n",
            "Epoch 73: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3340 - val_loss: 1.0983 - val_accuracy: 0.3423\n",
            "Epoch 74/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 74: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0990 - val_accuracy: 0.3299\n",
            "Epoch 75/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3369\n",
            "Epoch 75: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3369 - val_loss: 1.0986 - val_accuracy: 0.3431\n",
            "Epoch 76/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3304\n",
            "Epoch 76: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3304 - val_loss: 1.0988 - val_accuracy: 0.3296\n",
            "Epoch 77/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3313\n",
            "Epoch 77: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3313 - val_loss: 1.0986 - val_accuracy: 0.3304\n",
            "Epoch 78/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3332\n",
            "Epoch 78: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3332 - val_loss: 1.0986 - val_accuracy: 0.3426\n",
            "Epoch 79/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 79: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3340 - val_loss: 1.0990 - val_accuracy: 0.3293\n",
            "Epoch 80/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3304\n",
            "Epoch 80: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3304 - val_loss: 1.0983 - val_accuracy: 0.3299\n",
            "Epoch 81/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3342\n",
            "Epoch 81: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3343 - val_loss: 1.0989 - val_accuracy: 0.3296\n",
            "Epoch 82/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 82: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0985 - val_accuracy: 0.3423\n",
            "Epoch 83/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3360\n",
            "Epoch 83: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3359 - val_loss: 1.0986 - val_accuracy: 0.3276\n",
            "Epoch 84/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3319\n",
            "Epoch 84: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0988 - accuracy: 0.3320 - val_loss: 1.0988 - val_accuracy: 0.3296\n",
            "Epoch 85/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3337\n",
            "Epoch 85: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3338 - val_loss: 1.0981 - val_accuracy: 0.3304\n",
            "Epoch 86/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 86: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0989 - val_accuracy: 0.3301\n",
            "Epoch 87/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 87: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0988 - val_accuracy: 0.3276\n",
            "Epoch 88/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3315\n",
            "Epoch 88: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3315 - val_loss: 1.0990 - val_accuracy: 0.3276\n",
            "Epoch 89/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3323\n",
            "Epoch 89: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3323 - val_loss: 1.0987 - val_accuracy: 0.3291\n",
            "Epoch 90/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3334\n",
            "Epoch 90: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3334 - val_loss: 1.0987 - val_accuracy: 0.3426\n",
            "Epoch 91/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3366\n",
            "Epoch 91: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3367 - val_loss: 1.0982 - val_accuracy: 0.3428\n",
            "Epoch 92/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3347\n",
            "Epoch 92: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3347 - val_loss: 1.0991 - val_accuracy: 0.3299\n",
            "Epoch 93/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 93: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3339 - val_loss: 1.0988 - val_accuracy: 0.3296\n",
            "Epoch 94/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3329\n",
            "Epoch 94: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3330 - val_loss: 1.0988 - val_accuracy: 0.3273\n",
            "Epoch 95/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3324\n",
            "Epoch 95: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3323 - val_loss: 1.0985 - val_accuracy: 0.3301\n",
            "Epoch 96/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3329\n",
            "Epoch 96: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3328 - val_loss: 1.0986 - val_accuracy: 0.3293\n",
            "Epoch 97/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3327\n",
            "Epoch 97: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3327 - val_loss: 1.0985 - val_accuracy: 0.3431\n",
            "Epoch 98/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3356\n",
            "Epoch 98: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3356 - val_loss: 1.0990 - val_accuracy: 0.3293\n",
            "Epoch 99/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3327\n",
            "Epoch 99: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3327 - val_loss: 1.0992 - val_accuracy: 0.3301\n",
            "Epoch 100/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 100: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3340 - val_loss: 1.0983 - val_accuracy: 0.3293\n",
            "Epoch 101/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3332\n",
            "Epoch 101: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3332 - val_loss: 1.0982 - val_accuracy: 0.3299\n",
            "Epoch 102/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3316\n",
            "Epoch 102: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3315 - val_loss: 1.0984 - val_accuracy: 0.3293\n",
            "Epoch 103/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3373\n",
            "Epoch 103: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3373 - val_loss: 1.0988 - val_accuracy: 0.3299\n",
            "Epoch 104/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3377\n",
            "Epoch 104: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3376 - val_loss: 1.0985 - val_accuracy: 0.3293\n",
            "Epoch 105/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3353\n",
            "Epoch 105: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3353 - val_loss: 1.0982 - val_accuracy: 0.3421\n",
            "Epoch 106/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3354\n",
            "Epoch 106: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3354 - val_loss: 1.0997 - val_accuracy: 0.3291\n",
            "Epoch 107/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3361\n",
            "Epoch 107: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3361 - val_loss: 1.0984 - val_accuracy: 0.3291\n",
            "Epoch 108/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3364\n",
            "Epoch 108: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3364 - val_loss: 1.0986 - val_accuracy: 0.3433\n",
            "Epoch 109/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3325\n",
            "Epoch 109: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3325 - val_loss: 1.0986 - val_accuracy: 0.3299\n",
            "Epoch 110/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3301\n",
            "Epoch 110: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3302 - val_loss: 1.0988 - val_accuracy: 0.3426\n",
            "Epoch 111/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3375\n",
            "Epoch 111: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3375 - val_loss: 1.0991 - val_accuracy: 0.3301\n",
            "Epoch 112/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3341\n",
            "Epoch 112: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0988 - val_accuracy: 0.3426\n",
            "Epoch 113/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3358\n",
            "Epoch 113: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0988 - accuracy: 0.3357 - val_loss: 1.0986 - val_accuracy: 0.3276\n",
            "Epoch 114/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3342\n",
            "Epoch 114: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3342 - val_loss: 1.0988 - val_accuracy: 0.3291\n",
            "Epoch 115/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3373\n",
            "Epoch 115: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3372 - val_loss: 1.0986 - val_accuracy: 0.3421\n",
            "Epoch 116/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3313\n",
            "Epoch 116: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3313 - val_loss: 1.0982 - val_accuracy: 0.3426\n",
            "Epoch 117/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3388\n",
            "Epoch 117: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3388 - val_loss: 1.0988 - val_accuracy: 0.3278\n",
            "Epoch 118/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3345\n",
            "Epoch 118: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0983 - val_accuracy: 0.3426\n",
            "Epoch 119/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3390\n",
            "Epoch 119: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3390 - val_loss: 1.0993 - val_accuracy: 0.3293\n",
            "Epoch 120/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3382\n",
            "Epoch 120: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3382 - val_loss: 1.0985 - val_accuracy: 0.3431\n",
            "Epoch 121/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3341\n",
            "Epoch 121: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0983 - val_accuracy: 0.3426\n",
            "Epoch 122/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 122: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0985 - val_accuracy: 0.3288\n",
            "Epoch 123/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 123: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0992 - val_accuracy: 0.3421\n",
            "Epoch 124/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3306\n",
            "Epoch 124: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3306 - val_loss: 1.0987 - val_accuracy: 0.3421\n",
            "Epoch 125/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3327\n",
            "Epoch 125: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3327 - val_loss: 1.0979 - val_accuracy: 0.3299\n",
            "Epoch 126/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 126: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0988 - val_accuracy: 0.3293\n",
            "Epoch 127/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3310\n",
            "Epoch 127: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3310 - val_loss: 1.0986 - val_accuracy: 0.3296\n",
            "Epoch 128/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 128: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3343 - val_loss: 1.0991 - val_accuracy: 0.3296\n",
            "Epoch 129/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3355\n",
            "Epoch 129: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3355 - val_loss: 1.0985 - val_accuracy: 0.3431\n",
            "Epoch 130/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3387\n",
            "Epoch 130: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3387 - val_loss: 1.0991 - val_accuracy: 0.3299\n",
            "Epoch 131/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 131: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0988 - val_accuracy: 0.3301\n",
            "Epoch 132/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3336\n",
            "Epoch 132: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3336 - val_loss: 1.0982 - val_accuracy: 0.3426\n",
            "Epoch 133/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3329\n",
            "Epoch 133: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3330 - val_loss: 1.0984 - val_accuracy: 0.3423\n",
            "Epoch 134/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3347\n",
            "Epoch 134: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0985 - val_accuracy: 0.3431\n",
            "Epoch 135/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3337\n",
            "Epoch 135: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3337 - val_loss: 1.0988 - val_accuracy: 0.3293\n",
            "Epoch 136/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3363\n",
            "Epoch 136: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3363 - val_loss: 1.0995 - val_accuracy: 0.3296\n",
            "Epoch 137/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3342\n",
            "Epoch 137: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3342 - val_loss: 1.0989 - val_accuracy: 0.3301\n",
            "Epoch 138/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3378\n",
            "Epoch 138: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3378 - val_loss: 1.0985 - val_accuracy: 0.3423\n",
            "Epoch 139/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3312\n",
            "Epoch 139: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3312 - val_loss: 1.0982 - val_accuracy: 0.3299\n",
            "Epoch 140/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3354\n",
            "Epoch 140: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3354 - val_loss: 1.0989 - val_accuracy: 0.3293\n",
            "Epoch 141/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3319\n",
            "Epoch 141: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3319 - val_loss: 1.0985 - val_accuracy: 0.3426\n",
            "Epoch 142/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 142: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0988 - val_accuracy: 0.3423\n",
            "Epoch 143/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3319\n",
            "Epoch 143: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3319 - val_loss: 1.0984 - val_accuracy: 0.3428\n",
            "Epoch 144/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3335\n",
            "Epoch 144: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3335 - val_loss: 1.0991 - val_accuracy: 0.3276\n",
            "Epoch 145/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3316\n",
            "Epoch 145: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3316 - val_loss: 1.0984 - val_accuracy: 0.3428\n",
            "Epoch 146/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3376\n",
            "Epoch 146: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3376 - val_loss: 1.0988 - val_accuracy: 0.3276\n",
            "Epoch 147/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3331\n",
            "Epoch 147: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3330 - val_loss: 1.0986 - val_accuracy: 0.3423\n",
            "Epoch 148/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3298\n",
            "Epoch 148: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3298 - val_loss: 1.0986 - val_accuracy: 0.3418\n",
            "Epoch 149/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3350\n",
            "Epoch 149: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3350 - val_loss: 1.0987 - val_accuracy: 0.3299\n",
            "Epoch 150/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3345\n",
            "Epoch 150: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3346 - val_loss: 1.0987 - val_accuracy: 0.3299\n",
            "Epoch 151/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3322\n",
            "Epoch 151: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3322 - val_loss: 1.0990 - val_accuracy: 0.3293\n",
            "Epoch 152/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3350\n",
            "Epoch 152: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3350 - val_loss: 1.0985 - val_accuracy: 0.3426\n",
            "Epoch 153/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 153: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0989 - val_accuracy: 0.3296\n",
            "Epoch 154/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3354\n",
            "Epoch 154: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3354 - val_loss: 1.0985 - val_accuracy: 0.3291\n",
            "Epoch 155/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3321\n",
            "Epoch 155: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0989 - val_accuracy: 0.3296\n",
            "Epoch 156/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3376\n",
            "Epoch 156: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3376 - val_loss: 1.0986 - val_accuracy: 0.3428\n",
            "Epoch 157/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3358\n",
            "Epoch 157: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3358 - val_loss: 1.0989 - val_accuracy: 0.3276\n",
            "Epoch 158/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 158: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 63s 14ms/step - loss: 1.0987 - accuracy: 0.3338 - val_loss: 1.0990 - val_accuracy: 0.3293\n",
            "Epoch 159/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3333\n",
            "Epoch 159: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3334 - val_loss: 1.0980 - val_accuracy: 0.3418\n",
            "Epoch 160/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3350\n",
            "Epoch 160: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3349 - val_loss: 1.0986 - val_accuracy: 0.3291\n",
            "Epoch 161/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3296\n",
            "Epoch 161: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3296 - val_loss: 1.0987 - val_accuracy: 0.3299\n",
            "Epoch 162/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3323\n",
            "Epoch 162: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3323 - val_loss: 1.0992 - val_accuracy: 0.3304\n",
            "Epoch 163/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 163: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0990 - val_accuracy: 0.3301\n",
            "Epoch 164/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 164: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0989 - val_accuracy: 0.3296\n",
            "Epoch 165/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3372\n",
            "Epoch 165: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3372 - val_loss: 1.0985 - val_accuracy: 0.3426\n",
            "Epoch 166/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3327\n",
            "Epoch 166: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3327 - val_loss: 1.0985 - val_accuracy: 0.3293\n",
            "Epoch 167/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3333\n",
            "Epoch 167: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0981 - val_accuracy: 0.3428\n",
            "Epoch 168/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3374\n",
            "Epoch 168: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3375 - val_loss: 1.0987 - val_accuracy: 0.3426\n",
            "Epoch 169/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3374\n",
            "Epoch 169: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3374 - val_loss: 1.0991 - val_accuracy: 0.3296\n",
            "Epoch 170/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3365\n",
            "Epoch 170: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3366 - val_loss: 1.0986 - val_accuracy: 0.3301\n",
            "Epoch 171/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 171: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3339 - val_loss: 1.0987 - val_accuracy: 0.3433\n",
            "Epoch 172/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3365\n",
            "Epoch 172: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3365 - val_loss: 1.0985 - val_accuracy: 0.3296\n",
            "Epoch 173/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3321\n",
            "Epoch 173: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3320 - val_loss: 1.0985 - val_accuracy: 0.3426\n",
            "Epoch 174/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3357\n",
            "Epoch 174: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3356 - val_loss: 1.0981 - val_accuracy: 0.3426\n",
            "Epoch 175/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3312\n",
            "Epoch 175: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3313 - val_loss: 1.0988 - val_accuracy: 0.3423\n",
            "Epoch 176/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3371\n",
            "Epoch 176: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3372 - val_loss: 1.0991 - val_accuracy: 0.3296\n",
            "Epoch 177/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3335\n",
            "Epoch 177: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3334 - val_loss: 1.0989 - val_accuracy: 0.3296\n",
            "Epoch 178/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3321\n",
            "Epoch 178: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0985 - val_accuracy: 0.3428\n",
            "Epoch 179/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 179: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0988 - val_accuracy: 0.3301\n",
            "Epoch 180/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3396\n",
            "Epoch 180: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0986 - accuracy: 0.3396 - val_loss: 1.0988 - val_accuracy: 0.3276\n",
            "Epoch 181/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 181: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0987 - val_accuracy: 0.3291\n",
            "Epoch 182/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3331\n",
            "Epoch 182: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3331 - val_loss: 1.0990 - val_accuracy: 0.3296\n",
            "Epoch 183/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3307\n",
            "Epoch 183: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3307 - val_loss: 1.0986 - val_accuracy: 0.3299\n",
            "Epoch 184/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3321\n",
            "Epoch 184: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3323 - val_loss: 1.0988 - val_accuracy: 0.3293\n",
            "Epoch 185/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3318\n",
            "Epoch 185: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3318 - val_loss: 1.0985 - val_accuracy: 0.3421\n",
            "Epoch 186/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3354\n",
            "Epoch 186: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3354 - val_loss: 1.0989 - val_accuracy: 0.3293\n",
            "Epoch 187/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3334\n",
            "Epoch 187: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3334 - val_loss: 1.0987 - val_accuracy: 0.3296\n",
            "Epoch 188/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 188: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0988 - accuracy: 0.3343 - val_loss: 1.0992 - val_accuracy: 0.3426\n",
            "Epoch 189/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 189: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0996 - val_accuracy: 0.3299\n",
            "Epoch 190/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3335\n",
            "Epoch 190: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3335 - val_loss: 1.0989 - val_accuracy: 0.3299\n",
            "Epoch 191/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3336\n",
            "Epoch 191: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3336 - val_loss: 1.0989 - val_accuracy: 0.3293\n",
            "Epoch 192/200\n",
            "4432/4432 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3328\n",
            "Epoch 192: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3328 - val_loss: 1.0984 - val_accuracy: 0.3301\n",
            "Epoch 193/200\n",
            "4430/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3323\n",
            "Epoch 193: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 15ms/step - loss: 1.0988 - accuracy: 0.3323 - val_loss: 1.0992 - val_accuracy: 0.3296\n",
            "Epoch 194/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 194: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3340 - val_loss: 1.0982 - val_accuracy: 0.3296\n",
            "Epoch 195/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3349\n",
            "Epoch 195: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3349 - val_loss: 1.0984 - val_accuracy: 0.3296\n",
            "Epoch 196/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3310\n",
            "Epoch 196: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3310 - val_loss: 1.0984 - val_accuracy: 0.3296\n",
            "Epoch 197/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3310\n",
            "Epoch 197: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3310 - val_loss: 1.0984 - val_accuracy: 0.3423\n",
            "Epoch 198/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3320\n",
            "Epoch 198: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3319 - val_loss: 1.0987 - val_accuracy: 0.3296\n",
            "Epoch 199/200\n",
            "4431/4432 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3307\n",
            "Epoch 199: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0988 - accuracy: 0.3307 - val_loss: 1.0989 - val_accuracy: 0.3296\n",
            "Epoch 200/200\n",
            "4429/4432 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3378\n",
            "Epoch 200: val_accuracy did not improve from 0.34334\n",
            "4432/4432 [==============================] - 64s 14ms/step - loss: 1.0987 - accuracy: 0.3378 - val_loss: 1.0983 - val_accuracy: 0.3421\n"
          ]
        }
      ],
      "source": [
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "# Clear GPU memory\n",
        "K.clear_session()\n",
        "def generator(data, labels, batch_size):\n",
        "    while True:\n",
        "        indices = np.random.permutation(len(data))\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            yield data[batch_indices].astype('float16'), labels[batch_indices].astype('float16')\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, GlobalAveragePooling1D, Dropout, Reshape, Conv1D, MaxPooling1D\n",
        "\n",
        "# Create the Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(Dense(4, input_shape=(393816,), activation='linear'))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Reshape((128, 1)))  # Add Reshape layer\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(GlobalAveragePooling1D())\n",
        "\n",
        "for _ in range(5):  # Reduced the number of dropout layers\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=4, activation='relu'))\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with mixed precision and appropriate loss and metrics\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int64)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    epsilon = 1e-10\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    cross_entropy_loss = -tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.math.log(y_pred))\n",
        "\n",
        "    total_loss = cross_entropy_loss\n",
        "\n",
        "    return total_loss\n",
        "# Compile the model with appropriate loss and metrics\n",
        "#model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "\n",
        "#optimizer = Adam(learning_rate=0.001)  # Adjust learning rate as needed\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/DLA1/'\n",
        "if not os.path.exists(checkpoint_filepath):\n",
        "  os.mkdir(checkpoint_filepath)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'model_weights_epoch{epoch:02d}.h5'),\n",
        "    save_weights_only=True,  # Save only the model weights\n",
        "    save_best_only=True,      # Save only the best model\n",
        "    monitor='val_accuracy',      # Monitor a specific metric (e.g., validation loss)\n",
        "    mode='max',              # Mode for monitoring (minimize the monitored quantity)\n",
        "    verbose=1                 # Verbosity level (1: display messages)\n",
        ")\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "#es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback,history_logger_1]\n",
        "\n",
        "\n",
        "X_train = X_final_train_bert[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_final_train_bert[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "\n",
        "batch_size = 8\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=200,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=callbacks_list_1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aPetOPRCHNr",
        "outputId": "e199aa4e-2837-4277-ef2c-7bc7931b7581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-25-6529f6d0af41>:84: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  history = model.fit_generator(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.4422 - accuracy: 0.3502\n",
            "Epoch 1: val_loss improved from inf to 1.07662, saving model to /content/drive/MyDrive/ASC/results/models/DLA3/model_weights_epoch01.h5\n",
            "2216/2216 [==============================] - 74s 32ms/step - loss: 1.4420 - accuracy: 0.3502 - val_loss: 1.0766 - val_accuracy: 0.4116\n",
            "Epoch 2/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0938 - accuracy: 0.3595\n",
            "Epoch 2: val_loss improved from 1.07662 to 1.06528, saving model to /content/drive/MyDrive/ASC/results/models/DLA3/model_weights_epoch02.h5\n",
            "2216/2216 [==============================] - 68s 31ms/step - loss: 1.0938 - accuracy: 0.3595 - val_loss: 1.0653 - val_accuracy: 0.4220\n",
            "Epoch 3/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0943 - accuracy: 0.3485\n",
            "Epoch 3: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 66s 30ms/step - loss: 1.0943 - accuracy: 0.3485 - val_loss: 1.0993 - val_accuracy: 0.3298\n",
            "Epoch 4/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0999 - accuracy: 0.3341\n",
            "Epoch 4: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 66s 30ms/step - loss: 1.0999 - accuracy: 0.3341 - val_loss: 1.0989 - val_accuracy: 0.3282\n",
            "Epoch 5/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.1004 - accuracy: 0.3343\n",
            "Epoch 5: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 66s 30ms/step - loss: 1.1004 - accuracy: 0.3343 - val_loss: 1.0986 - val_accuracy: 0.3293\n",
            "Epoch 6/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.1057 - accuracy: 0.3372\n",
            "Epoch 6: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 66s 30ms/step - loss: 1.1057 - accuracy: 0.3372 - val_loss: 1.0989 - val_accuracy: 0.3295\n",
            "Epoch 7/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0989 - accuracy: 0.3367\n",
            "Epoch 7: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 65s 29ms/step - loss: 1.0989 - accuracy: 0.3367 - val_loss: 1.0981 - val_accuracy: 0.3428\n",
            "Epoch 8/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0993 - accuracy: 0.3326\n",
            "Epoch 8: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 64s 29ms/step - loss: 1.0993 - accuracy: 0.3326 - val_loss: 1.0984 - val_accuracy: 0.3423\n",
            "Epoch 9/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.1126 - accuracy: 0.3335\n",
            "Epoch 9: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.1126 - accuracy: 0.3335 - val_loss: 1.0989 - val_accuracy: 0.3295\n",
            "Epoch 10/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0989 - accuracy: 0.3355\n",
            "Epoch 10: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0989 - accuracy: 0.3355 - val_loss: 1.0989 - val_accuracy: 0.3300\n",
            "Epoch 11/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0989 - accuracy: 0.3340\n",
            "Epoch 11: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0989 - accuracy: 0.3341 - val_loss: 1.0982 - val_accuracy: 0.3423\n",
            "Epoch 12/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3370\n",
            "Epoch 12: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3370 - val_loss: 1.0992 - val_accuracy: 0.3293\n",
            "Epoch 13/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3359\n",
            "Epoch 13: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3360 - val_loss: 1.0982 - val_accuracy: 0.3433\n",
            "Epoch 14/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3333\n",
            "Epoch 14: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0988 - accuracy: 0.3333 - val_loss: 1.0987 - val_accuracy: 0.3295\n",
            "Epoch 15/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3374\n",
            "Epoch 15: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3374 - val_loss: 1.0986 - val_accuracy: 0.3293\n",
            "Epoch 16/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3351\n",
            "Epoch 16: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0988 - accuracy: 0.3351 - val_loss: 1.0983 - val_accuracy: 0.3277\n",
            "Epoch 17/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3309\n",
            "Epoch 17: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0988 - accuracy: 0.3309 - val_loss: 1.0986 - val_accuracy: 0.3298\n",
            "Epoch 18/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3356\n",
            "Epoch 18: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0986 - accuracy: 0.3356 - val_loss: 1.0994 - val_accuracy: 0.3425\n",
            "Epoch 19/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3312\n",
            "Epoch 19: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0988 - accuracy: 0.3312 - val_loss: 1.0989 - val_accuracy: 0.3290\n",
            "Epoch 20/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3389\n",
            "Epoch 20: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3390 - val_loss: 1.0982 - val_accuracy: 0.3298\n",
            "Epoch 21/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 21: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0984 - val_accuracy: 0.3433\n",
            "Epoch 22/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3335\n",
            "Epoch 22: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0987 - val_accuracy: 0.3298\n",
            "Epoch 23/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3377\n",
            "Epoch 23: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3377 - val_loss: 1.0981 - val_accuracy: 0.3435\n",
            "Epoch 24/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3345\n",
            "Epoch 24: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3345 - val_loss: 1.0988 - val_accuracy: 0.3295\n",
            "Epoch 25/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3321\n",
            "Epoch 25: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0982 - val_accuracy: 0.3428\n",
            "Epoch 26/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3342\n",
            "Epoch 26: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0987 - val_accuracy: 0.3423\n",
            "Epoch 27/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3323\n",
            "Epoch 27: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0988 - accuracy: 0.3323 - val_loss: 1.0985 - val_accuracy: 0.3285\n",
            "Epoch 28/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3384\n",
            "Epoch 28: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0986 - accuracy: 0.3384 - val_loss: 1.0989 - val_accuracy: 0.3425\n",
            "Epoch 29/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3385\n",
            "Epoch 29: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0986 - accuracy: 0.3385 - val_loss: 1.0987 - val_accuracy: 0.3270\n",
            "Epoch 30/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3338\n",
            "Epoch 30: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3338 - val_loss: 1.0988 - val_accuracy: 0.3300\n",
            "Epoch 31/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 31: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0985 - val_accuracy: 0.3420\n",
            "Epoch 32/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 32: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3340 - val_loss: 1.0989 - val_accuracy: 0.3298\n",
            "Epoch 33/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3357\n",
            "Epoch 33: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0986 - accuracy: 0.3357 - val_loss: 1.0989 - val_accuracy: 0.3290\n",
            "Epoch 34/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3392\n",
            "Epoch 34: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3392 - val_loss: 1.0988 - val_accuracy: 0.3428\n",
            "Epoch 35/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 35: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0984 - val_accuracy: 0.3300\n",
            "Epoch 36/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3351\n",
            "Epoch 36: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0983 - val_accuracy: 0.3293\n",
            "Epoch 37/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3367\n",
            "Epoch 37: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0986 - accuracy: 0.3367 - val_loss: 1.0984 - val_accuracy: 0.3290\n",
            "Epoch 38/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3332\n",
            "Epoch 38: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3331 - val_loss: 1.0984 - val_accuracy: 0.3300\n",
            "Epoch 39/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3335\n",
            "Epoch 39: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0986 - accuracy: 0.3336 - val_loss: 1.0982 - val_accuracy: 0.3277\n",
            "Epoch 40/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3356\n",
            "Epoch 40: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3355 - val_loss: 1.0988 - val_accuracy: 0.3293\n",
            "Epoch 41/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 41: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3343 - val_loss: 1.0992 - val_accuracy: 0.3300\n",
            "Epoch 42/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3371\n",
            "Epoch 42: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0986 - accuracy: 0.3372 - val_loss: 1.0987 - val_accuracy: 0.3428\n",
            "Epoch 43/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3364\n",
            "Epoch 43: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3364 - val_loss: 1.0981 - val_accuracy: 0.3430\n",
            "Epoch 44/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3351\n",
            "Epoch 44: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0988 - val_accuracy: 0.3420\n",
            "Epoch 45/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3313\n",
            "Epoch 45: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3313 - val_loss: 1.0984 - val_accuracy: 0.3300\n",
            "Epoch 46/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3341\n",
            "Epoch 46: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0988 - val_accuracy: 0.3308\n",
            "Epoch 47/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3383\n",
            "Epoch 47: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3383 - val_loss: 1.0987 - val_accuracy: 0.3295\n",
            "Epoch 48/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3323\n",
            "Epoch 48: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3323 - val_loss: 1.0986 - val_accuracy: 0.3293\n",
            "Epoch 49/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 49: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0988 - val_accuracy: 0.3300\n",
            "Epoch 50/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3381\n",
            "Epoch 50: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3381 - val_loss: 1.0987 - val_accuracy: 0.3272\n",
            "Epoch 51/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3333\n",
            "Epoch 51: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0989 - val_accuracy: 0.3300\n",
            "Epoch 52/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3351\n",
            "Epoch 52: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0986 - val_accuracy: 0.3300\n",
            "Epoch 53/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3364\n",
            "Epoch 53: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3364 - val_loss: 1.0984 - val_accuracy: 0.3428\n",
            "Epoch 54/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3338\n",
            "Epoch 54: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3338 - val_loss: 1.0982 - val_accuracy: 0.3300\n",
            "Epoch 55/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3374\n",
            "Epoch 55: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0986 - accuracy: 0.3374 - val_loss: 1.0989 - val_accuracy: 0.3298\n",
            "Epoch 56/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3350\n",
            "Epoch 56: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3349 - val_loss: 1.0988 - val_accuracy: 0.3425\n",
            "Epoch 57/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3334\n",
            "Epoch 57: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3334 - val_loss: 1.0984 - val_accuracy: 0.3420\n",
            "Epoch 58/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3298\n",
            "Epoch 58: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0986 - accuracy: 0.3298 - val_loss: 1.0990 - val_accuracy: 0.3303\n",
            "Epoch 59/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3338\n",
            "Epoch 59: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0986 - accuracy: 0.3338 - val_loss: 1.0987 - val_accuracy: 0.3300\n",
            "Epoch 60/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3353\n",
            "Epoch 60: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0986 - accuracy: 0.3353 - val_loss: 1.0990 - val_accuracy: 0.3293\n",
            "Epoch 61/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3321\n",
            "Epoch 61: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0984 - val_accuracy: 0.3293\n",
            "Epoch 62/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3361\n",
            "Epoch 62: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0986 - accuracy: 0.3361 - val_loss: 1.0983 - val_accuracy: 0.3423\n",
            "Epoch 63/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3313\n",
            "Epoch 63: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3313 - val_loss: 1.0984 - val_accuracy: 0.3282\n",
            "Epoch 64/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3326\n",
            "Epoch 64: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3326 - val_loss: 1.0991 - val_accuracy: 0.3300\n",
            "Epoch 65/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3328\n",
            "Epoch 65: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3328 - val_loss: 1.0989 - val_accuracy: 0.3423\n",
            "Epoch 66/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3323\n",
            "Epoch 66: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0987 - val_accuracy: 0.3303\n",
            "Epoch 67/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3330\n",
            "Epoch 67: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3330 - val_loss: 1.0985 - val_accuracy: 0.3303\n",
            "Epoch 68/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3365\n",
            "Epoch 68: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0986 - accuracy: 0.3365 - val_loss: 1.0983 - val_accuracy: 0.3430\n",
            "Epoch 69/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3332\n",
            "Epoch 69: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0981 - val_accuracy: 0.3428\n",
            "Epoch 70/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3330\n",
            "Epoch 70: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3330 - val_loss: 1.0985 - val_accuracy: 0.3428\n",
            "Epoch 71/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3353\n",
            "Epoch 71: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0988 - accuracy: 0.3353 - val_loss: 1.0989 - val_accuracy: 0.3293\n",
            "Epoch 72/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 72: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0986 - val_accuracy: 0.3300\n",
            "Epoch 73/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3371\n",
            "Epoch 73: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3370 - val_loss: 1.0983 - val_accuracy: 0.3433\n",
            "Epoch 74/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3322\n",
            "Epoch 74: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3323 - val_loss: 1.0987 - val_accuracy: 0.3282\n",
            "Epoch 75/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3330\n",
            "Epoch 75: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0987 - accuracy: 0.3330 - val_loss: 1.0987 - val_accuracy: 0.3295\n",
            "Epoch 76/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3378\n",
            "Epoch 76: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3379 - val_loss: 1.0987 - val_accuracy: 0.3298\n",
            "Epoch 77/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3385\n",
            "Epoch 77: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3384 - val_loss: 1.0991 - val_accuracy: 0.3420\n",
            "Epoch 78/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 78: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0987 - val_accuracy: 0.3298\n",
            "Epoch 79/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3338\n",
            "Epoch 79: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3338 - val_loss: 1.0981 - val_accuracy: 0.3293\n",
            "Epoch 80/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3313\n",
            "Epoch 80: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3313 - val_loss: 1.0989 - val_accuracy: 0.3298\n",
            "Epoch 81/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3359\n",
            "Epoch 81: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3359 - val_loss: 1.0984 - val_accuracy: 0.3295\n",
            "Epoch 82/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3325\n",
            "Epoch 82: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3325 - val_loss: 1.0988 - val_accuracy: 0.3300\n",
            "Epoch 83/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3362\n",
            "Epoch 83: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3362 - val_loss: 1.0989 - val_accuracy: 0.3303\n",
            "Epoch 84/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3305\n",
            "Epoch 84: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3305 - val_loss: 1.0982 - val_accuracy: 0.3420\n",
            "Epoch 85/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3347\n",
            "Epoch 85: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3347 - val_loss: 1.0985 - val_accuracy: 0.3428\n",
            "Epoch 86/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3390\n",
            "Epoch 86: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3390 - val_loss: 1.0991 - val_accuracy: 0.3295\n",
            "Epoch 87/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3349\n",
            "Epoch 87: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3349 - val_loss: 1.0988 - val_accuracy: 0.3303\n",
            "Epoch 88/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 88: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0988 - val_accuracy: 0.3303\n",
            "Epoch 89/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 89: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3349 - val_loss: 1.0992 - val_accuracy: 0.3305\n",
            "Epoch 90/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3314\n",
            "Epoch 90: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3315 - val_loss: 1.0982 - val_accuracy: 0.3295\n",
            "Epoch 91/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3337\n",
            "Epoch 91: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3337 - val_loss: 1.0986 - val_accuracy: 0.3275\n",
            "Epoch 92/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 92: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3343 - val_loss: 1.0987 - val_accuracy: 0.3293\n",
            "Epoch 93/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 93: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0986 - val_accuracy: 0.3300\n",
            "Epoch 94/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3359\n",
            "Epoch 94: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3359 - val_loss: 1.0992 - val_accuracy: 0.3303\n",
            "Epoch 95/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3349\n",
            "Epoch 95: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3349 - val_loss: 1.0987 - val_accuracy: 0.3298\n",
            "Epoch 96/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3325\n",
            "Epoch 96: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3325 - val_loss: 1.0986 - val_accuracy: 0.3295\n",
            "Epoch 97/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3364\n",
            "Epoch 97: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0987 - accuracy: 0.3364 - val_loss: 1.0991 - val_accuracy: 0.3295\n",
            "Epoch 98/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3332\n",
            "Epoch 98: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3332 - val_loss: 1.0984 - val_accuracy: 0.3298\n",
            "Epoch 99/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3351\n",
            "Epoch 99: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0989 - val_accuracy: 0.3303\n",
            "Epoch 100/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3350\n",
            "Epoch 100: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3350 - val_loss: 1.0985 - val_accuracy: 0.3425\n",
            "Epoch 101/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 101: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0985 - val_accuracy: 0.3293\n",
            "Epoch 102/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3349\n",
            "Epoch 102: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0990 - val_accuracy: 0.3275\n",
            "Epoch 103/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3320\n",
            "Epoch 103: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3319 - val_loss: 1.0988 - val_accuracy: 0.3298\n",
            "Epoch 104/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3324\n",
            "Epoch 104: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 55s 25ms/step - loss: 1.0986 - accuracy: 0.3323 - val_loss: 1.0988 - val_accuracy: 0.3290\n",
            "Epoch 105/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3363\n",
            "Epoch 105: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0986 - accuracy: 0.3363 - val_loss: 1.0983 - val_accuracy: 0.3433\n",
            "Epoch 106/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3358\n",
            "Epoch 106: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3358 - val_loss: 1.0987 - val_accuracy: 0.3293\n",
            "Epoch 107/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3336\n",
            "Epoch 107: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3336 - val_loss: 1.0984 - val_accuracy: 0.3423\n",
            "Epoch 108/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 108: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3342 - val_loss: 1.0995 - val_accuracy: 0.3295\n",
            "Epoch 109/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3324\n",
            "Epoch 109: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3324 - val_loss: 1.0983 - val_accuracy: 0.3428\n",
            "Epoch 110/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3338\n",
            "Epoch 110: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3339 - val_loss: 1.0986 - val_accuracy: 0.3417\n",
            "Epoch 111/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3322\n",
            "Epoch 111: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0986 - accuracy: 0.3322 - val_loss: 1.0986 - val_accuracy: 0.3303\n",
            "Epoch 112/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 112: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0985 - val_accuracy: 0.3435\n",
            "Epoch 113/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3322\n",
            "Epoch 113: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3321 - val_loss: 1.0988 - val_accuracy: 0.3433\n",
            "Epoch 114/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 114: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3345 - val_loss: 1.0986 - val_accuracy: 0.3298\n",
            "Epoch 115/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3388\n",
            "Epoch 115: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3389 - val_loss: 1.0980 - val_accuracy: 0.3423\n",
            "Epoch 116/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 116: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0986 - val_accuracy: 0.3300\n",
            "Epoch 117/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3304\n",
            "Epoch 117: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3304 - val_loss: 1.0985 - val_accuracy: 0.3282\n",
            "Epoch 118/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3326\n",
            "Epoch 118: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0986 - accuracy: 0.3326 - val_loss: 1.0986 - val_accuracy: 0.3290\n",
            "Epoch 119/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3340\n",
            "Epoch 119: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0988 - accuracy: 0.3340 - val_loss: 1.0989 - val_accuracy: 0.3303\n",
            "Epoch 120/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3377\n",
            "Epoch 120: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3377 - val_loss: 1.0983 - val_accuracy: 0.3425\n",
            "Epoch 121/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 121: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0985 - val_accuracy: 0.3300\n",
            "Epoch 122/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3329\n",
            "Epoch 122: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3329 - val_loss: 1.0989 - val_accuracy: 0.3298\n",
            "Epoch 123/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3363\n",
            "Epoch 123: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3363 - val_loss: 1.0987 - val_accuracy: 0.3423\n",
            "Epoch 124/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3338\n",
            "Epoch 124: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3338 - val_loss: 1.0986 - val_accuracy: 0.3300\n",
            "Epoch 125/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3346\n",
            "Epoch 125: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3346 - val_loss: 1.0990 - val_accuracy: 0.3293\n",
            "Epoch 126/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3339\n",
            "Epoch 126: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0987 - accuracy: 0.3339 - val_loss: 1.0982 - val_accuracy: 0.3423\n",
            "Epoch 127/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3350\n",
            "Epoch 127: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3350 - val_loss: 1.0987 - val_accuracy: 0.3295\n",
            "Epoch 128/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3342\n",
            "Epoch 128: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3342 - val_loss: 1.0990 - val_accuracy: 0.3428\n",
            "Epoch 129/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3268\n",
            "Epoch 129: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3268 - val_loss: 1.0984 - val_accuracy: 0.3430\n",
            "Epoch 130/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3327\n",
            "Epoch 130: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3327 - val_loss: 1.0984 - val_accuracy: 0.3295\n",
            "Epoch 131/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3365\n",
            "Epoch 131: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3365 - val_loss: 1.0989 - val_accuracy: 0.3425\n",
            "Epoch 132/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3360\n",
            "Epoch 132: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0986 - accuracy: 0.3359 - val_loss: 1.0988 - val_accuracy: 0.3295\n",
            "Epoch 133/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3378\n",
            "Epoch 133: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3378 - val_loss: 1.0987 - val_accuracy: 0.3298\n",
            "Epoch 134/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3343\n",
            "Epoch 134: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3342 - val_loss: 1.0981 - val_accuracy: 0.3428\n",
            "Epoch 135/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3373\n",
            "Epoch 135: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3373 - val_loss: 1.0991 - val_accuracy: 0.3295\n",
            "Epoch 136/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3347\n",
            "Epoch 136: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0986 - accuracy: 0.3348 - val_loss: 1.0986 - val_accuracy: 0.3425\n",
            "Epoch 137/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3333\n",
            "Epoch 137: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0984 - val_accuracy: 0.3303\n",
            "Epoch 138/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3344\n",
            "Epoch 138: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3344 - val_loss: 1.0990 - val_accuracy: 0.3303\n",
            "Epoch 139/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3368\n",
            "Epoch 139: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3368 - val_loss: 1.0986 - val_accuracy: 0.3293\n",
            "Epoch 140/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3381\n",
            "Epoch 140: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3381 - val_loss: 1.0982 - val_accuracy: 0.3428\n",
            "Epoch 141/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3351\n",
            "Epoch 141: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0987 - val_accuracy: 0.3295\n",
            "Epoch 142/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3359\n",
            "Epoch 142: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0986 - accuracy: 0.3360 - val_loss: 1.0992 - val_accuracy: 0.3300\n",
            "Epoch 143/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3327\n",
            "Epoch 143: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3327 - val_loss: 1.0985 - val_accuracy: 0.3300\n",
            "Epoch 144/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3356\n",
            "Epoch 144: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3356 - val_loss: 1.0986 - val_accuracy: 0.3430\n",
            "Epoch 145/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3366\n",
            "Epoch 145: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3366 - val_loss: 1.0988 - val_accuracy: 0.3293\n",
            "Epoch 146/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3324\n",
            "Epoch 146: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3323 - val_loss: 1.0982 - val_accuracy: 0.3300\n",
            "Epoch 147/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3340\n",
            "Epoch 147: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3339 - val_loss: 1.0987 - val_accuracy: 0.3303\n",
            "Epoch 148/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3354\n",
            "Epoch 148: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0987 - accuracy: 0.3354 - val_loss: 1.0985 - val_accuracy: 0.3430\n",
            "Epoch 149/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3351\n",
            "Epoch 149: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0988 - val_accuracy: 0.3293\n",
            "Epoch 150/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3345\n",
            "Epoch 150: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3345 - val_loss: 1.0989 - val_accuracy: 0.3298\n",
            "Epoch 151/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3334\n",
            "Epoch 151: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3334 - val_loss: 1.0987 - val_accuracy: 0.3287\n",
            "Epoch 152/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3357\n",
            "Epoch 152: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3357 - val_loss: 1.0986 - val_accuracy: 0.3430\n",
            "Epoch 153/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3345\n",
            "Epoch 153: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3345 - val_loss: 1.0987 - val_accuracy: 0.3305\n",
            "Epoch 154/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3334\n",
            "Epoch 154: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3333 - val_loss: 1.0982 - val_accuracy: 0.3303\n",
            "Epoch 155/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3341\n",
            "Epoch 155: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0985 - val_accuracy: 0.3305\n",
            "Epoch 156/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3343\n",
            "Epoch 156: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3342 - val_loss: 1.0987 - val_accuracy: 0.3428\n",
            "Epoch 157/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3375\n",
            "Epoch 157: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0987 - accuracy: 0.3375 - val_loss: 1.0986 - val_accuracy: 0.3303\n",
            "Epoch 158/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 158: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0989 - val_accuracy: 0.3295\n",
            "Epoch 159/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 159: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0982 - val_accuracy: 0.3428\n",
            "Epoch 160/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3402\n",
            "Epoch 160: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 60s 27ms/step - loss: 1.0986 - accuracy: 0.3402 - val_loss: 1.0985 - val_accuracy: 0.3420\n",
            "Epoch 161/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3359\n",
            "Epoch 161: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0986 - accuracy: 0.3359 - val_loss: 1.0984 - val_accuracy: 0.3430\n",
            "Epoch 162/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 162: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0992 - val_accuracy: 0.3295\n",
            "Epoch 163/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 163: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0982 - val_accuracy: 0.3303\n",
            "Epoch 164/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3345\n",
            "Epoch 164: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3346 - val_loss: 1.0988 - val_accuracy: 0.3293\n",
            "Epoch 165/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3341\n",
            "Epoch 165: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0985 - val_accuracy: 0.3293\n",
            "Epoch 166/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3351\n",
            "Epoch 166: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0983 - val_accuracy: 0.3290\n",
            "Epoch 167/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3356\n",
            "Epoch 167: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 62s 28ms/step - loss: 1.0986 - accuracy: 0.3356 - val_loss: 1.0987 - val_accuracy: 0.3428\n",
            "Epoch 168/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3381\n",
            "Epoch 168: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3381 - val_loss: 1.0981 - val_accuracy: 0.3423\n",
            "Epoch 169/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3347\n",
            "Epoch 169: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 27ms/step - loss: 1.0986 - accuracy: 0.3347 - val_loss: 1.0986 - val_accuracy: 0.3430\n",
            "Epoch 170/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3304\n",
            "Epoch 170: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 61s 28ms/step - loss: 1.0987 - accuracy: 0.3304 - val_loss: 1.0986 - val_accuracy: 0.3293\n",
            "Epoch 171/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3378\n",
            "Epoch 171: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3378 - val_loss: 1.0985 - val_accuracy: 0.3298\n",
            "Epoch 172/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3359\n",
            "Epoch 172: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0986 - accuracy: 0.3359 - val_loss: 1.0985 - val_accuracy: 0.3300\n",
            "Epoch 173/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0988 - accuracy: 0.3291\n",
            "Epoch 173: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0988 - accuracy: 0.3291 - val_loss: 1.0982 - val_accuracy: 0.3305\n",
            "Epoch 174/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3349\n",
            "Epoch 174: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3348 - val_loss: 1.0986 - val_accuracy: 0.3423\n",
            "Epoch 175/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3328\n",
            "Epoch 175: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3328 - val_loss: 1.0987 - val_accuracy: 0.3303\n",
            "Epoch 176/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3323\n",
            "Epoch 176: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3322 - val_loss: 1.0986 - val_accuracy: 0.3298\n",
            "Epoch 177/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0985 - accuracy: 0.3352\n",
            "Epoch 177: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0985 - accuracy: 0.3352 - val_loss: 1.0988 - val_accuracy: 0.3277\n",
            "Epoch 178/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3319\n",
            "Epoch 178: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3320 - val_loss: 1.0990 - val_accuracy: 0.3300\n",
            "Epoch 179/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3386\n",
            "Epoch 179: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3386 - val_loss: 1.0984 - val_accuracy: 0.3435\n",
            "Epoch 180/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3316\n",
            "Epoch 180: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0987 - accuracy: 0.3316 - val_loss: 1.0986 - val_accuracy: 0.3295\n",
            "Epoch 181/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3330\n",
            "Epoch 181: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3331 - val_loss: 1.0988 - val_accuracy: 0.3300\n",
            "Epoch 182/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3375\n",
            "Epoch 182: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3375 - val_loss: 1.0987 - val_accuracy: 0.3295\n",
            "Epoch 183/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3335\n",
            "Epoch 183: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3335 - val_loss: 1.0985 - val_accuracy: 0.3300\n",
            "Epoch 184/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3322\n",
            "Epoch 184: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3322 - val_loss: 1.0984 - val_accuracy: 0.3295\n",
            "Epoch 185/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 185: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0985 - val_accuracy: 0.3298\n",
            "Epoch 186/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3359\n",
            "Epoch 186: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0986 - accuracy: 0.3359 - val_loss: 1.0984 - val_accuracy: 0.3430\n",
            "Epoch 187/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 187: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3351 - val_loss: 1.0986 - val_accuracy: 0.3298\n",
            "Epoch 188/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3349\n",
            "Epoch 188: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3349 - val_loss: 1.0985 - val_accuracy: 0.3298\n",
            "Epoch 189/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3348\n",
            "Epoch 189: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3347 - val_loss: 1.0987 - val_accuracy: 0.3425\n",
            "Epoch 190/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3337\n",
            "Epoch 190: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 26ms/step - loss: 1.0987 - accuracy: 0.3337 - val_loss: 1.0984 - val_accuracy: 0.3425\n",
            "Epoch 191/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3341\n",
            "Epoch 191: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3341 - val_loss: 1.0984 - val_accuracy: 0.3425\n",
            "Epoch 192/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0986 - accuracy: 0.3351\n",
            "Epoch 192: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0986 - accuracy: 0.3352 - val_loss: 1.0989 - val_accuracy: 0.3293\n",
            "Epoch 193/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3353\n",
            "Epoch 193: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0990 - val_accuracy: 0.3290\n",
            "Epoch 194/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.3373\n",
            "Epoch 194: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0986 - accuracy: 0.3373 - val_loss: 1.0987 - val_accuracy: 0.3428\n",
            "Epoch 195/200\n",
            "2214/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3327\n",
            "Epoch 195: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3327 - val_loss: 1.0987 - val_accuracy: 0.3425\n",
            "Epoch 196/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3329\n",
            "Epoch 196: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 59s 27ms/step - loss: 1.0987 - accuracy: 0.3329 - val_loss: 1.0989 - val_accuracy: 0.3430\n",
            "Epoch 197/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3332\n",
            "Epoch 197: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3331 - val_loss: 1.0983 - val_accuracy: 0.3425\n",
            "Epoch 198/200\n",
            "2215/2216 [============================>.] - ETA: 0s - loss: 1.0987 - accuracy: 0.3347\n",
            "Epoch 198: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3348 - val_loss: 1.0987 - val_accuracy: 0.3293\n",
            "Epoch 199/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3354\n",
            "Epoch 199: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 58s 26ms/step - loss: 1.0987 - accuracy: 0.3354 - val_loss: 1.0985 - val_accuracy: 0.3275\n",
            "Epoch 200/200\n",
            "2216/2216 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3352\n",
            "Epoch 200: val_loss did not improve from 1.06528\n",
            "2216/2216 [==============================] - 57s 26ms/step - loss: 1.0987 - accuracy: 0.3352 - val_loss: 1.0987 - val_accuracy: 0.3298\n"
          ]
        }
      ],
      "source": [
        "def generator(data, labels, batch_size):\n",
        "  while True:\n",
        "    indices = np.random.permutation(len(data))  # Shuffle indices\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      yield data[batch_indices], labels[batch_indices]\n",
        "\n",
        "# model = Sequential([\n",
        "#     Dense(4, input_shape=(393816,), activation='linear'),\n",
        "# ])\n",
        "\n",
        "# # Adding Batch Normalization for stability\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "# # Adding ten additional Dense layers with ReLU activation and Dropout\n",
        "# for _ in range(10):\n",
        "#     model.add(Dense(units=64, activation='relu'))\n",
        "#     model.add(Dropout(0.2))  # Adjust dropout rate as needed\n",
        "\n",
        "# model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Updated input shape based on PCA components\n",
        "model.add(Dense(64, input_shape=(393816,), activation='linear'))  # Increased units and changed activation\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "for _ in range(8):  # Reduced to 8 additional layers for simplicity\n",
        "    model.add(Dense(units=128, activation='relu'))  # Increased units\n",
        "    model.add(Dropout(0.2))  # Adjusted dropout rate\n",
        "\n",
        "# Reduced final layer units based on problem requirements\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Optimized hyperparameters\n",
        "optimizer = Adam(learning_rate=0.001)  #\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int64)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    epsilon = 1e-10\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    cross_entropy_loss = -tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.math.log(y_pred))\n",
        "\n",
        "    total_loss = cross_entropy_loss\n",
        "\n",
        "    return total_loss\n",
        "# Compile the model with appropriate loss and metrics\n",
        "#model.compile(optimizer='adam', loss=custom_loss, metrics=['accuracy'])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)  # Adjust learning rate as needed\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/DLA3/'\n",
        "if not os.path.exists(checkpoint_filepath):\n",
        "  os.mkdir(checkpoint_filepath)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'model_weights_epoch{epoch:02d}.h5'),\n",
        "    save_weights_only=True,  # Save only the model weights\n",
        "    save_best_only=True,      # Save only the best model\n",
        "    monitor='val_loss',      # Monitor a specific metric (e.g., validation loss)\n",
        "    mode='min',              # Mode for monitoring (minimize the monitored quantity)\n",
        "    verbose=1                 # Verbosity level (1: display messages)\n",
        ")\n",
        "\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "#es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback, history_logger_1]\n",
        "\n",
        "\n",
        "X_train = X_final_train_bert[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_final_train_bert[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=200,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=callbacks_list_1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "wgzYIsoaEQte",
        "outputId": "18bef196-4686-4a56-c1fd-678019067b4c"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'StandardScaler' object has no attribute 'partial_fit_transform'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6097dd43d60d>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_final_train_bert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_final_train_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size_pca\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mX_batch_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Incremental scaling and transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mX_batch_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincremental_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mX_train_pca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'StandardScaler' object has no attribute 'partial_fit_transform'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import os\n",
        "\n",
        "def generator(data, labels, batch_size):\n",
        "    while True:\n",
        "        indices = np.random.permutation(len(data))  # Shuffle indices\n",
        "        for i in range(0, len(indices), batch_size):\n",
        "            batch_indices = indices[i:i + batch_size]\n",
        "            yield data[batch_indices], labels[batch_indices]\n",
        "def custom_loss(y_true, y_pred):\n",
        "    y_true = tf.cast(y_true, tf.int64)\n",
        "    y_pred = tf.cast(y_pred, tf.float32)\n",
        "\n",
        "    epsilon = 1e-10\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    cross_entropy_loss = -tf.reduce_sum(tf.cast(y_true, tf.float32) * tf.math.log(y_pred))\n",
        "\n",
        "    total_loss = cross_entropy_loss\n",
        "\n",
        "\n",
        "# Incremental PCA and scaling in batches\n",
        "batch_size_pca = 500  # Adjust as needed\n",
        "incremental_pca = IncrementalPCA(n_components=100, batch_size=batch_size_pca)\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_pca = []\n",
        "# ... (previous code unchanged)\n",
        "\n",
        "for i in range(0, len(X_final_train_bert), batch_size_pca):\n",
        "    X_batch = X_final_train_bert[i:i + batch_size_pca]\n",
        "    X_batch_scaled = scaler.partial_fit_transform(X_batch)  # Incremental scaling and transformation\n",
        "    X_batch_pca = incremental_pca.partial_fit(X_batch_scaled)\n",
        "    X_train_pca.append(X_batch_pca)\n",
        "\n",
        "# ... (remaining code unchanged)\n",
        "\n",
        "X_train_pca = np.concatenate(X_train_pca)  # Combine transformed batches\n",
        "\n",
        "# ... (model definition and training as before, using generators)\n",
        "model = Sequential()\n",
        "\n",
        "# Updated input shape based on PCA components\n",
        "model.add(Dense(64, input_shape=(100,), activation='relu'))  # Increased units and changed activation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "for _ in range(8):  # Reduced to 8 additional layers for simplicity\n",
        "    model.add(Dense(units=128, activation='relu'))  # Increased units\n",
        "    model.add(Dropout(0.3))  # Adjusted dropout rate\n",
        "\n",
        "# Reduced final layer units based on problem requirements\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Optimized hyperparameters\n",
        "optimizer = Adam(learning_rate=0.001)  # Adjust learning rate as needed\n",
        "model.compile( loss=custom_loss , metrics=['accuracy'])\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/PCA/'\n",
        "os.makedirs(checkpoint_filepath, exist_ok=True)\n",
        "\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'model_weights_epoch{epoch:02d}.h5'),\n",
        "    save_weights_only=True,  # Save only the model weights\n",
        "    save_best_only=True,      # Save only the best model\n",
        "    monitor='val_loss',      # Monitor a specific metric (e.g., validation loss)\n",
        "    mode='min',              # Mode for monitoring (minimize the monitored quantity)\n",
        "    verbose=1                 # Verbosity level (1: display messages)\n",
        ")\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "X_train = X_train_pca[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_train_pca[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "batch_size = 16\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=200,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=[model_checkpoint_callback],\n",
        "    verbose=2  # Set verbose to 2 for a more concise output\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ9VOkzfJx5j"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKD4nMQdJnOi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri2uH05ltT9C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SR4qtGXumg8"
      },
      "outputs": [],
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrY686e1uikK"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 4191634,
          "sourceId": 7277404,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30626,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03785cd2135945348d9bfb000eead73e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1419ebdd11ad459d8457a7a403f911ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1622398cd29a43608dcad4e38acf8ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c72258392f540a3bd7873e11a056e41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d8a46406404615939dc08a271acdb1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0eee1f3b0c4c9ea7acd838110b3af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e8536677b3d469fb430d4b1f78be004",
            "max": 77,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dccbb9b9da3f4c28b998a0e24359545d",
            "value": 77
          }
        },
        "2d5e1277d6d9485fa458de8d9591c7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2dd4df77a60246d9911e01458611f31e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e8536677b3d469fb430d4b1f78be004": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1588d89b9040dd8cb1dd9ef64b53d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30fda049dfc341adb4758cd70dd13c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cb03d51a5ee427699a655225ef00fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1419ebdd11ad459d8457a7a403f911ef",
            "placeholder": "​",
            "style": "IPY_MODEL_2f1588d89b9040dd8cb1dd9ef64b53d5",
            "value": "Loading hidden state batches: 100%"
          }
        },
        "3d03644918284653b3dae16568d91157": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "427f6367733848b98d2f7cd71bd9207e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4897088a258a490a81b62c43b3f9822b",
            "placeholder": "​",
            "style": "IPY_MODEL_fa9a0c457d774dcf824e7e566624e06b",
            "value": " 98/98 [01:31&lt;00:00,  1.15it/s]"
          }
        },
        "446d9c7a37a942e2b58597b8570dd09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3cb03d51a5ee427699a655225ef00fa0",
              "IPY_MODEL_7274f09a8f784b97ab326eb05e57d589",
              "IPY_MODEL_d6efaae7539a418c98931e2ea57f04e0"
            ],
            "layout": "IPY_MODEL_bef340b858054d16ae7cc19b419fa96e"
          }
        },
        "4897088a258a490a81b62c43b3f9822b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b09c76daa7c4051bdeb489e3b2a3ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e79b7f4e5c8457ab5585136ef7f370f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7478d977c8de4ac9a1ce10a329bb718f",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6e7186153ce4c6f90c32ee7308a0964",
            "value": 98
          }
        },
        "506dff407b2c4c85a1d45e98a3375878": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5352f2ec00204538873634d0028c9e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ae5b1158ba24d8180741fb75e55f10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be5dd67a17f5484ba227a40de0724b5f",
            "placeholder": "​",
            "style": "IPY_MODEL_5352f2ec00204538873634d0028c9e40",
            "value": "100%"
          }
        },
        "6672f81a6f2143ff8b720f7baf095e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8144ebf2d7474238aeb4c3d3e9acf831",
            "placeholder": "​",
            "style": "IPY_MODEL_7a2c28d304c34be1a49af630d75be4cb",
            "value": "Loading hidden state batches: 100%"
          }
        },
        "67e0c30580a34fa189e417b4b99f8b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee3a02a6f40640989eadde32979b8c55",
            "max": 394,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d5e1277d6d9485fa458de8d9591c7b8",
            "value": 394
          }
        },
        "6997c082ed4d453287a9ba5bc738ec21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6672f81a6f2143ff8b720f7baf095e72",
              "IPY_MODEL_2d0eee1f3b0c4c9ea7acd838110b3af3",
              "IPY_MODEL_8634adaf3d244e19a5e6c8c8062c1dd8"
            ],
            "layout": "IPY_MODEL_2dd4df77a60246d9911e01458611f31e"
          }
        },
        "6d08a68af6e4469e967c14c1950866f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71a2817fdf954b46973eacd20f702ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "725539b6439343ddb2ba0785560bd571": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9add8dd026f49a19f874527851a68b5",
            "placeholder": "​",
            "style": "IPY_MODEL_30fda049dfc341adb4758cd70dd13c4f",
            "value": " 20/20 [01:52&lt;00:00,  4.36s/it]"
          }
        },
        "7274f09a8f784b97ab326eb05e57d589": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23d8a46406404615939dc08a271acdb1",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_932c789225e342c8976e37ff1be24118",
            "value": 20
          }
        },
        "7478d977c8de4ac9a1ce10a329bb718f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77b0ddec8b8748fd8c49df83609bb1ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7990c8e071084241809c543684a0af21",
            "placeholder": "​",
            "style": "IPY_MODEL_b6546da356c249a9977f8dd40d766f43",
            "value": "Processing batches: 100%"
          }
        },
        "7990c8e071084241809c543684a0af21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a2c28d304c34be1a49af630d75be4cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8144ebf2d7474238aeb4c3d3e9acf831": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84ff62e3589048d7ab5c6f53b2b1685b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "8634adaf3d244e19a5e6c8c8062c1dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1622398cd29a43608dcad4e38acf8ee4",
            "placeholder": "​",
            "style": "IPY_MODEL_4b09c76daa7c4051bdeb489e3b2a3ab7",
            "value": " 77/77 [15:33&lt;00:00, 12.34s/it]"
          }
        },
        "87494cf6e61d435f9fb78ce6aa8372f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77b0ddec8b8748fd8c49df83609bb1ce",
              "IPY_MODEL_bafcfb5170d548f5abe39d5ecaf108e3",
              "IPY_MODEL_725539b6439343ddb2ba0785560bd571"
            ],
            "layout": "IPY_MODEL_84ff62e3589048d7ab5c6f53b2b1685b"
          }
        },
        "919f5881244649aaa284c3853433a2f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "932c789225e342c8976e37ff1be24118": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94653bf3562b4c7fb1c0dd3d884e20ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_506dff407b2c4c85a1d45e98a3375878",
            "placeholder": "​",
            "style": "IPY_MODEL_f6aa6069303a4b1893ac089a89be24d4",
            "value": " 394/394 [06:15&lt;00:00,  1.09it/s]"
          }
        },
        "ae230456060c4e07a557d71827ad11da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc033c0b573c4c6b91d8634b263d99b0",
            "placeholder": "​",
            "style": "IPY_MODEL_71a2817fdf954b46973eacd20f702ef4",
            "value": "100%"
          }
        },
        "b6546da356c249a9977f8dd40d766f43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bafcfb5170d548f5abe39d5ecaf108e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d03644918284653b3dae16568d91157",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_919f5881244649aaa284c3853433a2f2",
            "value": 20
          }
        },
        "bc033c0b573c4c6b91d8634b263d99b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be5dd67a17f5484ba227a40de0724b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef340b858054d16ae7cc19b419fa96e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e7186153ce4c6f90c32ee7308a0964": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d360f5f3e6b64d8a8c4bd26102b61403": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6efaae7539a418c98931e2ea57f04e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c72258392f540a3bd7873e11a056e41",
            "placeholder": "​",
            "style": "IPY_MODEL_6d08a68af6e4469e967c14c1950866f2",
            "value": " 20/20 [00:34&lt;00:00,  1.34s/it]"
          }
        },
        "dccbb9b9da3f4c28b998a0e24359545d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de39776067424779a8506b8ef4a41882": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ae5b1158ba24d8180741fb75e55f10c",
              "IPY_MODEL_67e0c30580a34fa189e417b4b99f8b75",
              "IPY_MODEL_94653bf3562b4c7fb1c0dd3d884e20ab"
            ],
            "layout": "IPY_MODEL_d360f5f3e6b64d8a8c4bd26102b61403"
          }
        },
        "e8217b37ecc0489a85cfd63adaea902a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae230456060c4e07a557d71827ad11da",
              "IPY_MODEL_4e79b7f4e5c8457ab5585136ef7f370f",
              "IPY_MODEL_427f6367733848b98d2f7cd71bd9207e"
            ],
            "layout": "IPY_MODEL_03785cd2135945348d9bfb000eead73e"
          }
        },
        "e9add8dd026f49a19f874527851a68b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee3a02a6f40640989eadde32979b8c55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6aa6069303a4b1893ac089a89be24d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa9a0c457d774dcf824e7e566624e06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}