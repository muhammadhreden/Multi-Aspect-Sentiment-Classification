{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Access to my drive"
      ],
      "metadata": {
        "id": "NhyOiUY5ytCk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LPW_24Shv-32"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "LjJfTFQmy5he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import csv\n",
        "import math\n",
        "import torch\n",
        "import keras\n",
        "import os\n",
        "import gc\n",
        "import tensorflow as tf\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch import nn\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from keras.models import Sequential , Model\n",
        "from keras.layers import LSTM, Bidirectional, Dense, Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from scipy.special import softmax\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Embedding, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D\n",
        "import os\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping,Callback,CSVLogger\n",
        "import gc\n",
        "class GarbageCollectorCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Reshape, Conv1D, GlobalAveragePooling1D, Dense"
      ],
      "metadata": {
        "id": "t_nzrbL3wrUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data"
      ],
      "metadata": {
        "id": "Hy4mEyZ80HIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_from_pickle(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "try:\n",
        "    with open('/content/drive/MyDrive/ASC/dataset/final_train_bert.pkl', \"rb\") as f:\n",
        "        X_final_train_bert = np.load(f, allow_pickle=True)\n",
        "except IOError:  # Handle potential errors during loading\n",
        "    X_final_train_bert = []\n",
        "\n",
        "# Load y_train\n",
        "y_train = load_from_pickle('/content/drive/MyDrive/ASC/dataset/y_train.pkl')"
      ],
      "metadata": {
        "id": "TdpAGMudxy1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform data"
      ],
      "metadata": {
        "id": "bKiRjtLO0MNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "basic = [i for i in range(3)]\n",
        "enc = OneHotEncoder()\n",
        "enc.fit(np.array(basic).reshape(-1,1))\n",
        "y_enc = enc.transform(y_train[:39400].values.reshape(-1, 1))\n",
        "y_enc = [y_enc[j].toarray() for j in range(39400)]"
      ],
      "metadata": {
        "id": "nGUoJx3HyJDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_enc = np.array(y_enc).reshape((39400, 3))"
      ],
      "metadata": {
        "id": "U31i7hCMyJyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "rHxMkRy50Ugm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(data, labels, batch_size):\n",
        "  while True:\n",
        "    indices = np.random.permutation(len(data))\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "      batch_indices = indices[i:i + batch_size]\n",
        "      yield data[batch_indices], labels[batch_indices]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(4, input_shape=(393816,), activation='linear'),\n",
        "])\n",
        "\n",
        "for _ in range(15):\n",
        "    model.add(Dense(units=64, activation='relu'))\n",
        "\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "\n",
        "\n",
        "model.compile( loss=loss_function, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "checkpoint_filepath = '/content/drive/MyDrive/ASC/results/models/exp5/'\n",
        "if not os.path.exists(checkpoint_filepath):\n",
        "  os.mkdir(checkpoint_filepath)\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_filepath, 'z_model_weights_epoch{epoch:02d}_val_acc{val_accuracy:.4f}.h5'),\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "history_logger_1 = CSVLogger(os.path.join(checkpoint_filepath,'history1.csv'), separator=\",\", append=True)\n",
        "checkpoint_1 = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
        "es_1 = EarlyStopping(monitor='val_loss', patience=5)\n",
        "callbacks_list_1 = [GarbageCollectorCallback(),model_checkpoint_callback, history_logger_1]\n",
        "\n",
        "validation_split = 0.1  # You can adjust this based on your needs\n",
        "num_validation_samples = int(validation_split * len(X_final_train_bert))\n",
        "\n",
        "X_train = X_final_train_bert[:-num_validation_samples]\n",
        "y_train = y_enc[:-num_validation_samples]\n",
        "X_validation = X_final_train_bert[-num_validation_samples:]\n",
        "y_validation = y_enc[-num_validation_samples:]\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "history = model.fit_generator(\n",
        "    generator=generator(X_train, y_train, batch_size),\n",
        "    epochs=1000,\n",
        "    steps_per_epoch=len(X_train) // batch_size,\n",
        "    validation_data=generator(X_validation, y_validation, batch_size),  # Use generator for validation data as well\n",
        "    validation_steps=len(X_validation) // batch_size,\n",
        "    callbacks=callbacks_list_1\n",
        ")"
      ],
      "metadata": {
        "id": "cRtc8o2qyof_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}